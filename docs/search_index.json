[["index.html", "Ecological Analysis Chapter 1 INTRODUCTION 1.1 Data Distributions 1.2 Hands-on Exercises", " Ecological Analysis Basim Alsaedi 2024-11-16 Chapter 1 INTRODUCTION The distinction between data sample and the underlying data population is always ignored. For instance the mean concentration of a chemical contaminant in a soil sample of a contaminated site. Here, the data sample is the soil sample that was taken to the laboratory to find out the mean concentration of the contaminant while the data population is the entire data population consisting of all possible soil measurements of the chemical contaminant at the site. We need to define some terms to get this clear. Data Population: is the entire set of individuals, items or observations of interest in a study. For instance all the soil samples from the contaminant site. Data Sample: is the subset of the population selected for analysis. For example, measuring the height of randomly selected trees in the forest. Data Distribution: describes how data points are spread of arranged. Data Sample vs Data Population A deep understanding of the difference between data sample and data population is important in inferential statistics which seeks to generalize the results of data analyses performed on the data samples. A data population consists of all possible observations or data points concerning the characteristic of interest. For instance, when finding out the mean height of all the oak trees in a forest, the data population will be every height of every single oak tree in the forest. Contrary, data sample is usually a limited subset of observations drawn for the entire population. In this case we can have a data sample by randomly selecting 100 oak trees and measuring their heights. Try it! Lets simulate data of oak trees with R # Seed for reproducibility set.seed(123) # A population of 12500 oak trees population_heights &lt;- rnorm(12500, mean = 20, sd=5) head(population_heights) ## [1] 17.19762 18.84911 27.79354 20.35254 20.64644 28.57532 We have simulated the heights of 12500 oak trees in a forest. The variance population_heights represent the data population for the oak tree heights. Lets randomly select 86 trees from the entire data population of oak tree heights # Randomly select 86 trees sample_heights &lt;- sample(population_heights, size=86, replace = FALSE) tail(sample_heights) ## [1] 25.88792 29.03996 29.00965 37.10547 17.76631 19.81021 The variable sample_heights represents data sample for the oak tree heights. If we make this experiment more interesting, we find the mean and standard deviation for the data population and the data sample for the oak tree heights. Lets do it! # Mean and standard deviation of the entire population population_stats &lt;- c( Mean = mean(population_heights), SD = sd(population_heights) ) population_stats ## Mean SD ## 19.995609 4.996295 The entire oak tree data population has a mean height of approximately 20 meters and a standard deviation of approx 5 meters. Lets see how it compares with the data sample of the oak tree heights. # Mean and standard deviation of the sample sample_stats &lt;- c( Mean = mean(sample_heights), SD = sd(sample_heights) ) sample_stats ## Mean SD ## 21.094995 5.507379 There is a minor difference between the data sample and the entire data population of the oak tree heights. As you can see the selected sample of oak trees has an average height of 21 meters and a standard deviation of 5.5 meters. Practical Exercise Run the code below to generate the data of soil samples collected in farm with livestock. The main objective was to find the pH of the soil. Use the code below to simulate the collection of the data. # Set seed for reproducibility set.seed(76) # Simulate the data collection farm_population_pH &lt;- rnorm(n = 5342, mean=3.61, sd=0.8) head(farm_population_pH) ## [1] 3.864037 3.471632 3.544874 3.192810 3.239502 3.724600 You are required to work on the problems below; What is the Mean and Standard Deviation of the pH of the entire farm population soil samples. Select 65 random samples from the entire population. Calculate the Mean and Standard Deviation of the sample. How does the Mean and Standard deviation differ from the entire population? Solution Run the code below to generate the data of soil samples collected in farm with livestock. The main objective was to find the pH of the soil. Use the code below to simulate the collection of the data. # Set seed for reproducibility set.seed(76) # Simulate the data collection farm_population_pH &lt;- rnorm(n = 5342, mean=3.61, sd=0.8) head(farm_population_pH) ## [1] 3.864037 3.471632 3.544874 3.192810 3.239502 3.724600 You are required to work on the problems below; What is the Mean and Standard Deviation of the pH of the entire farm population soil samples. population_stats &lt;- c( Mean = mean(farm_population_pH), standard_deviation = sd(farm_population_pH) ) population_stats # show the results ## Mean standard_deviation ## 3.6218315 0.7934042 Select 65 random samples from the entire population. farm_sample_pH &lt;- sample(farm_population_pH, size = 65, replace = FALSE) head(farm_sample_pH) ## [1] 3.478034 4.268278 4.084203 4.044815 2.613637 3.575463 Calculate the Mean and Standard Deviation of the sample. sample_stats &lt;- c( Mean = mean(farm_sample_pH), standard_deviation = (sd(farm_sample_pH)) ) sample_stats # show the results ## Mean standard_deviation ## 3.7237092 0.7554283 How does the Mean and Standard deviation differ from the entire population? The sample mean is slightly higher than the whole population mean while the standard deviation of the whole populations is almost equal to the data sample’s standard deviation ________________________________________________________________________________ 1.1 Data Distributions Data Distribution describes how data values in data population are spread across the range of possible values. For instance, in our case the height of oak trees in the forest might be distributed normally, meaning that most trees cluster around the average height. Understanding data distribution is crucial in ecological analysis since it helps in; identifying patterns ( for instance seasonal changes in bird populations). choosing appropriate statistical tests. modelling ecological phenomena accurately. When dealing with continuous data values like height of oak trees, soil acidity or rainfall, Probability Distribution Functions is used to find the likelihood of different outcomes in a population. There are two types of probability distribution functions, namely; Cumulative distribution function, Empirical Cumulative distribution function. Below is the distribution of oak tree heights represented in a density plot. library(ggplot2) # Create a data frame from the data oak_df &lt;- data.frame(Height = population_heights) # Plot the density plot ggplot(oak_df, aes(x = Height)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of tree heights&quot;, x = &quot;Tree Height(m)&quot;, y = &quot;Density&quot; ) + theme_minimal() Above is the distribution of oak tree heights, lets find its probability using the two types of distribution functions. Cumulative Distribution Function This distribution function leans toward that a probability of a random variable is less than or equal to a certain value. ## Add code Empirical Cumulative Distribution Function ———————-Add more info—————– 1.1.1 Types of Distribution In statistical tests, it is assumed that the data sample represents the underlying data population. Similarly, the distribution of the data sample is assumed to be similar to the one of the data population. These are what characterizes the data distribution; whether the data is; discrete or continuous symmetrical or asymmetrical bounded by lower and/or upper limits or unbounded. Lets discuss different types of data distributions. 1.1.1.1 Normal distribution The normal distribution is a continuous symmetrical distribution in the real number domain (i.e., with the set of possible values ranging between -∞ and ∞) whose probability density function plots as a smooth bell-shaped curve, and whose cumulative distribution is S-shaped. Try it! To demonstrate this, lets generate 100 random data values with a mean of 5 and a standard deviation of 3. set.seed(1) data &lt;- rnorm(100, mean=5, sd=3) df &lt;- data.frame(data) # Plot the data ggplot(df, aes(x = data)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of data sample&quot;, x = &quot;Sample Value&quot;, y = &quot;Density&quot; ) + theme_minimal() The data distribution below represents a near bell-shaped curve. A normal distribution with a mean of zero and variance of 1 (or standard deviation of 1, since standard deviation is the square root of variance and square root of 1 is 1) is referred to as a standard normal distribution A standard normal distribution has many applications in statistics such as computation of cumulative probabilities and critical values for hypothesis tests. Try it! Lets generate a random data set that is standardized. set.seed(100) data &lt;- rnorm(100, mean=0, sd=1) df &lt;- data.frame(data) # Plot the data ggplot(df, aes(x = data)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of data sample&quot;, x = &quot;Sample Value&quot;, y = &quot;Density&quot; ) + theme_minimal() Any data sample that follows a normal distribution can be standardized to a standard normal distribution by subtracting its mean from each individual data value and dividing the result by its standard deviation. The resultant values can be referred to as standard score, normal score, normal quantile or z-value) Here is the formula for calculation the z-value (standardized score); \\[z = {{x - \\overline x}\\over{\\sigma}}\\] Where; \\(x\\) is the actual value \\(\\overline x\\) is the data sample mean \\(\\sigma\\) is the standard deviation Try it ——————-will generate a data distribution with a known standard deviation and mean then standardize it. Will then plot both samples - raw data and the standardized one—————————- Example: CUMULATIVE AND EXCEEDANCE PROBABILITIES FOR A NORMAL DISTRIBUTION Groundwater Manganese Concentrations in mg/L ______________________Expand on this____________________________________ modify the code below to show up the distribution x = seq(0, 0.4, length=100) y = dnorm(x, 0.52, 0.18) polygon(c(0, x, 0.4), c(0, y, 0), col = &quot;gray&quot;) # Add a shaded area x = seq(1, 0.9, length=25) y = dnorm(x, 0.52, 0.18) polygon(c(1, x, 0.9), c(0, y, 0), col = &quot;gray&quot;) Goodness-of-Fit (GOF) Tests for the Normal Distribution In parametric statistical tests, it assumed that the data is normally distributed or can be normalized by data transformation such as log transform. The parameters, such as mean and standard deviation, in parametric tests must be specified. Verifying the data normality is crucial before conducting the tests. If it fails the test of normality other types of tests(non parametric) are considered. The GOF tests are used to access the normality of the data sample which requires 8 to 10 randomly picked data values. The tests are; Normal probability plot - also known as normal quantile plot. Coefficient of Variation(CV) Coefficient of Skewness Shapiro-Wilk(SW) and Shapiro-Francia(SF) procedures Filiben’s probability plot correlation coefficient (PPCC) Shapiro-Wilk multiple group normality The CV test is the most commonly used method. It is computed simply by dividing the standard deviation by the mean. If the resultant value is greater than 1 then the data fails the normality test, otherwise it passes. \\[CV = {\\sigma\\over{\\overline x}}\\] Other GOF tests that are used in testing for normal and other distributions are Anderson–Darling (AD) test and the Lilliefors–Kolmogorov–Smirnov test The Coefficient of Skewness stated above provides a more direct measure of skewness where skewness of the magnitude of greater than 0.5 is considered moderate to the substantial degree of skewness. ——————————Add an example———————— Central Limit Theorem ____________________simplify it with an example _________________________ 1.1.1.2 Lognormal, Gamma, and Other Continuous Distributions Before we dive into Gamma distribution, we need to to get the definition of exponential distribution. Exponential distribution is the probability of the waiting time between events in a Poisson Process. Gamma Distribution Exponential infers the probability until the first event happens while Gamma distribution gives us the probability of the waiting time until the \\(n^{th}\\) event. Gamma distribution Logistic Distribution Other Continuous Distributions 1.1.1.3 Distributions Used in Inferential Statistics Student’s t Distribution Chi-Square Distribution F Distribution 1.1.1.4 Dicrete Distributions Binomial Distribution Poisson Distribution 1.2 Hands-on Exercises Solution ________________________________________________________________________________ "],["hypothesis-testing.html", "Chapter 2 HYPOTHESIS TESTING 2.1 Introduction 2.2 Parametric tests 2.3 Nonparametric test 2.4 One-Way ANOVA 2.5 Hands-on Exercise", " Chapter 2 HYPOTHESIS TESTING 2.1 Introduction Hypothesis testing is a standard statistical procedure for deciding between two competing alternatives or scenarios, based on evidence presented by a data sample collected for the purpose of conducting the test. Most descriptive statistics such as mean, median and standard deviation are usually computed from data samples which may vary with the whole population. For instance, we want to find the concentration of hexavalent chromium, a human carcinogen, in soil. We will collect random 20 samples. The average concentration of the carcinogen from the collected samples will at most vary with the average concentration in the entire land, that’s where hypothesis testing comes in. Basic terminology and procedure for hypothesis testing There is a need to understand the basic terms before diving deep into the process of hypothesis testing. If we have historical data, the presence of a trend must be established and below are the assumptions that should be made prior to conducting hypothesis testing; The data is assumed to belong to a specified distribution(typically normal distribution). The data has a stable or stationary trend(free from of temporal or spatial trend). There are no unusual observations or outliers. The hypothesis testing that is based on the above is described as parametric hypothesis testing. Otherwise, if the above assumptions are not met, the test is known as non-parametric hypothesis testing. Null hypothesis (\\(H_0\\)): Is a statement of the presumed default or baseline condition. In this case the null hypothesis is that there is no difference between the carcinogenic concentration is below the allowed limit. Alternate hypothesis (\\(H_1\\) or \\(H_A\\)): is the inverse or the opposite of the null hypothesis. Alternate hypothesis contradicts the statement of the presumed default. In this case, the alternate hypothesis states that the carcinogenic level is above the allowed limit. The evidence produced by the sample after conducting hypothesis testing can either refute or support the null hypothesis. The one-tailed (One-Sided) Hypothesis test checks for an effect in only one direction. In our case, we test if the carcinogen levels exceed the allowed limit(right side). Alternatively, it can be checked of it is below the allowed limits (left-sided). Upper-Tailed Hypothesis Test is the same as the right one-tailed hypothesis test that checks if the value of interest exceed the null hypothesis value Lower-Tailed Hypothesis Test is the left one-tailed hypthesis test where it checks if the value of interest is below the null hypothesis value. In our case the allowed limit for carcinogens in soil. Two-Tailed Hypothesis Test checks for the effect in both directions. In our we can change to find if the carcinogen levels are below or above the allowed limit. T-Value is the test statistic calculated from the sample data to determine how far the sample mean is from the hypothesized mean, in terms of standard error. Critical T-Value is the threshold value for the t-distribution that corresponds to the significance level. If the t-value exceeds the critical t-value, the null hypothesis is rejected. Significant level(\\(\\alpha\\)) is the probability threshold for rejecting the null hypothesis, usually set to 0.05(5%). In most cases if the probability of observing the test statistic (p-value) is less than 0.005, the null hypothesis is rejected. Degrees of Freedom(df) are the number of independent values in the data sample that are free to vary when calculating a statistic. In most cases df is calculated as \\[df = n - 1\\] where \\(n\\) represents the sample size(count). In our cases where we have 20 samples therefore \\(df = 20 -1\\) and the degree of freedom will be 19 P-Value indicates the probability of obtaining a test statistic at least as extreme as the observed one, assuming the null hypothesis is true. Below are the steps taken in hypothesis testing; Formulate the null and alternate hypothesis. Assume the null hypothesis is true and compute a test statistic that follows a known distribution. Calculate a test statistic value based on the data sample which follows a known distribution and determine the critical value for chosen distribution at a given significance level. Compare the test statistic to the critical value to assess the plausibility of the null hypothesis. For symettrical distributions(e.g t, normal), critical values are located on; upper-tailed test: positive side of the distribution lower-tailed test: negative side of the distribution two-tailed test: both sides, with equal magnitude Calculate the p-value to find how the probability of obtaining a test statistic more extreme than the observed value. Try it! To get the steps above clear, lets work on an example. This is an hypothetical problem where a regulatory body sets that the acceptable phosphorous concentration in a lake at 1.5mg/L. 30 sample measurements were collected at different locations in the lake. The objective was to determine if the phosphorous levels are significantly higher than the allowed limit. Lets simulate the data. set.seed(123) # for reproducibility phosphorous_levels &lt;- rnorm(n = 30, mean = 1.7, sd = 0.2) Formulate the null hypothesis Null Hypothesis(\\(H_0\\)): \\(\\mu\\) &lt; 1.5 - phosphorous levels are within the acceptable levels. Alternate Hypothesis (\\(H_1\\)): \\(\\mu\\) &gt; 1.5 - phosphorous levels exceed the acceptable limit. This is a right-sided(one-sided) test since we are only interested in detecting an increase in phosphorous levels We use a one-sample ttest since the population data standard deviation is unknown, and the sample size is small(n=30). The formula for t-statistic is calculated as; \\[t = {{\\overline{x} - \\mu_0}\\over{s/\\sqrt{n}}}\\] Where: \\(\\overline{x}\\) is the sample mean. \\(\\mu_0\\) is the population mean under null hypothesis (1.5mg/L) \\(s\\) is the sample standard deviation. \\(n\\) is the sample size. Perform the one-sample t-test # One sample t-test t_test_result &lt;- t.test(phosphorous_levels, mu = 1.5, alternative = &quot;greater&quot;) # Display the results print(t_test_result) ## ## One Sample t-test ## ## data: phosphorous_levels ## t = 5.3201, df = 29, p-value = 5.21e-06 ## alternative hypothesis: true mean is greater than 1.5 ## 95 percent confidence interval: ## 1.629713 Inf ## sample estimates: ## mean of x ## 1.690579 Determine the critical value For the right-tailed test at a 5% significance levels(\\(alpha\\)=0.05), the critical value is the t-value that corresponds to the top 5% of the t-distribution. The qt() function will be used to calculate this; \\[t_{critical} = qt(1-\\alpha, df = n - 1)\\] critical_t &lt;- qt(0.95, df = length(phosphorous_levels) - 1) critical_t ## [1] 1.699127 The t-statistic is greater than the critical t therefore the null hypothesis is rejected. Lets confirm this by also computing the p_value Calculate the null hypothesis p_value &lt;- t_test_result$p.value p_value ## [1] 5.210072e-06 The null hypothesis is rejected as the p_value is less than 0.05. Therefore, we make conclusion that the phosphorous levels in the lake exceed the acceptable levels. 2.2 Parametric tests As described before parametric tests rely on the assumptions that the data follows a known distribution (commonly the normal distribution) and it should be free of major outliers. In case where the test requires the data to follow a normal distribution, if the sample size is small it must follow a normal distribution while for larger sample size (e.g n≥30) the Central Limit Theorem allows the researchers to approximate normality even of the data is not perfectly normal 2.2.1 Parametric single-sample test This type of test is used to determine whether the mean of a single sample is significantly different from a known or hypothesized population mean. It assumes that the sample data comes from a population that follows a normal distribution (or has a large enough sample size to approximate normality). The main objective of this test is to test whether the sample mean differs significantly from a reference or target value, such as an industry standard, a population average, or a theoretical expectation in hypothesis testing These are the steps taken for One-Sample T-test; State the Hypotheses: Null Hypothesis(\\(H_0\\)): The sample mean equals the population mean (\\(\\mu = \\mu_0\\)) Alternate Hypothesis(\\(H_A\\)): The sample mean is different from the population mean(\\(\\mu \\neq \\mu_0\\)) Compute the test statistic The formula for test statistic is \\[t = {{\\overline x - \\mu_0}\\over{s/\\sqrt n}}\\] Where: \\(\\overline x\\): is the sample mean \\(\\mu_0\\): is the hypothesized population mean \\(s\\): is the sample standard deviation \\(n\\): is the sample size Determine the degree of freedom as \\(df = n- 1\\) Compare \\(t\\) to critical value or use p-value 2.2.2 Parametric two-sample test 2.3 Nonparametric test 2.3.1 Nonparametric two-sample paired sign test and paired Wilcoxon Signed Rank Test 2.4 One-Way ANOVA 2.4.1 Parametric One-Way ANOVA 2.4.2 Nonparametric One-Way ANOVA (Kruskal-Wallis Test) 2.5 Hands-on Exercise Solution ________________________________________________________________________________ "],["test-for-autocorrelation-and-temporal-effects.html", "Chapter 3 TEST FOR AUTOCORRELATION AND TEMPORAL EFFECTS 3.1 Test for Autocorrelation Using the Sample Autocorrelation Function", " Chapter 3 TEST FOR AUTOCORRELATION AND TEMPORAL EFFECTS Environment Data is not always stable or stationary over time or space and is frequently subjected to sustained or cyclical change to the affecting factors. These concepts will help us understand the temporal effects better; Temporal Trends: refer to changes in a variable or phenomenon over time. Consider monitoring soil nitrogen levels in agricultural field over several years. There might be temporal rise of nitrogen immediately after application of nitrogen-based fertilizer and return back to almost the average state after time. Although there might be a general increase in soil nitrogen levels, but they won’t match the immediate spikes after fertilizer application. Autocorrelation or serial correlation: is the correlation between data values of the same variable. Positive autocorrelation means that values close in time/space tend to be similar while negative autocorrelation means that they tend to differ. Monotonic trend: this is a trend that is in one direction, either constantly upward or downwards. A good example are the levels of carbon dioxide(CO2) concentrations in the atmosphere, they have been consistently rising due to industrial activities and deforestation. Cyclic or Seasonal trend refer to patterns that repeat at regular intervals such as daily ,hourly or annually. These trends are usually affected by natural cycles, human activities or climatic conditions. For instance, these trends are evident in phosphorous levels in water bodies that might increase during rainy seasons due agricultural runoff and decline during dry seasons. When testing for autocorrelation and temporal effects the data samples are assumed to have values that are independent or uncorrelated, and identically or similarly distributed. The data are said to be autocorrelated or serially correlated when this assumption is violated. These are the tests that are used to check for autocorrelation: Sample Autocorrelation Function(ACF): used for a single time series where data is normally distributed. Rank von Neumann ratio test used when the data samples evidence of nonnormality. The complete block design ANOVA or Friedman test is used for multiple different data samples for instance testing the concentration of ground water contaminant from multiple monitoring wells at a site. 3.1 Test for Autocorrelation Using the Sample Autocorrelation Function Before testing for autocorrelation using the ACF method, the data sample should be; normally distributed. stationary(that is, not trending). free of outliers. containing atleast 10 to 12 observations. … And here is how ACf is computed. Arrange the data in lagged data pairs (\\(x_{i}, x_{i + k}\\)), for \\(i\\) = 1, 2, ….(\\(n-k\\)), where; \\(n\\) is the size of the data samples(number of data values therein). \\(k\\) is the lag. i.e number of sampling events dates separating one data value in the pair from the second value. For instance a lag of 1 shows that the two value pairs were collected in consecutive time intervals(e.g days) while lag shows that the data value pairs were collected every each time interval Calculate the sample Autocorrelation coefficient \\[r_k = {{\\sum^{n-k}_{i=1}(x_i-x^!)(x_{i+k}- x^!)}\\over{\\sum^n_{i=1}(x_i - x^!)^2}}\\] where; \\(n\\) is the data sample size. \\(k\\) is the lag \\(x^!\\) is the data sample mean \\(x_{i}, x_{i + k}\\) are the components of the data pairs formed based on the lag. If \\(k\\) = 1, the sample autocorrelation coefﬁcient, \\(r_1\\) is referred to as the ﬁrst order sample autocorrelation coefﬁcient. If \\(k\\) = 2, \\(r_2\\) is the second order coefﬁcient, and so on. Lets breakdown the possible results of autocorrelation; \\(r_0\\) = 1: Autocorrelation at lag 0 is always one because a value is perfectly correlated with itself. Random Data: if the data is random, most autocorrelation coefficients will be close to zero, and will decrease further as the lag increases. Autocorrelated Data: If there’s autocorrelation, some coefficients(\\(r_k\\)) will be significantly larger than zero, but their strength decreases with higher lags. Trend in data: If there is a trend, coefficients wont diminish, showing persistent correlation. As a summary, the normally distributed data, \\(r_k\\) should be close to zero if there is no correlation. At a 95% confidence level, autocorrelation is insignificant if no \\(r_k\\) exceeds the threshold \\(2\\over{\\sqrt{n}}\\). There result of each autocorrelation are therefore plotted in a correlogram and the confidence limits are show as the horizontal lines. Try it! "],["correlation-covariance-geostatistics.html", "Chapter 4 CORRELATION, COVARIANCE, GEOSTATISTICS 4.1 INTRODUCTION 4.2 CORRELATION AND COVARIANCE 4.3 GEOSTATISTICS", " Chapter 4 CORRELATION, COVARIANCE, GEOSTATISTICS 4.1 INTRODUCTION In this topic, we will focus on Bivariate correlation and multivariate correlation, that is, the correlation between two or multiple variables. Lets define correlation once again! Correlation is the degree if association of variables. Correlation coefficient: is the quantifier that indicates the association of variables whether whether one variable is increasing/decreasing and the other one is increasing/decreasing or if there is no association at all. The correlation coefficient always ranges from -1 to 1 where the closer it gets to +1 means a stronger positive correlation(both variables increases/decreases at a concert) while it gets closer to -1 means a stronger negative correlation where one variable increases and the other variable decreases. A correlation close to zero from any direction(\\(+_{ve}\\) or \\(-_{ve}\\)) shows that there is little to no correlation between variables. Multiple correlation is where a group of variables (usually referred to as the predictor or X variables) to jointly relate to another variable (usually referred to as the response or Y variable), Partial correlation is the correlation between any pair of variables, given the presence of the remaining variables. There are two methods to compute the correlation coefficient; Pearson’s correlation coefficient. - satisfies linearity, normality and other parametric assumptions test. Spearman’s correlation coefficient. - free from linearity, normality etc. conditions. Kendall’s correlation coefficient. - free from linearity, normality etc conditions. 4.2 CORRELATION AND COVARIANCE ____________differentiate between the two___________________ Define Spurious correlation 4.2.1 Pearson’s Correlation Coefficient \\[r = {1\\over{n - 1}}{\\sum^n_{i=1}[{{x_i - \\overline{x}}\\over{s_x}}][{{y_i - \\overline{y}}\\over{s_y}}]}\\] 4.2.2 Spearman’s Correlaton Coeefficient 4.3 GEOSTATISTICS "],["linear-regression-analysis.html", "Chapter 5 LINEAR REGRESSION ANALYSIS 5.1 Simple Linear Regression Analysis 5.2 Data transformation versus generalized linear model 5.3 Multiple Linear Regression (MLR) model 5.4 Practical Exercise", " Chapter 5 LINEAR REGRESSION ANALYSIS What is Regression OLS 5.1 Simple Linear Regression Analysis Definition 5.1.1 Empirical model-building-regression analysis 5.1.2 Interpreting the regression results 5.1.3 R-squared and Adjusted R-Squared 5.2 Data transformation versus generalized linear model 5.3 Multiple Linear Regression (MLR) model 5.3.1 Assumptions of Multiple Linear Regression 5.4 Practical Exercise "],["conventional-and-probabilistic-risk-assessment.html", "Chapter 6 CONVENTIONAL AND PROBABILISTIC RISK ASSESSMENT 6.1 Introduction 6.2 Conventional or Point Risk Estimation 6.3 Probabilistic Risk Assessment Using Monte Carlo Simulation 6.4 Practical Exercise", " Chapter 6 CONVENTIONAL AND PROBABILISTIC RISK ASSESSMENT 6.1 Introduction Exposure to chemical is currently inevitable to the current society. Scientists and environmentalists aim not for zero exposure but minimize the release of the chemical contaminants to the environment and limit their potential adverse health or ecological effects. Maximum Contaminant Levels(MCLs) - is the concentration of a chemical contaminant that the environmental protection authorities believe would not cause substantial adverse health effects to the public. Assumptions based on exposure characteristics are made before deciding on a effective MCL. The input factors are then combined with the estimated toxicity or potency of the chemical, to back-calculate an allowable concentration or MCL for the chemical, which is believed would not cause substantial adverse health effects. These are some of the assumptions; The daily rate of water consumption. An individual body weight. Duration of residency at one location Expected human lifespan. These assumptions are the basic of back-calculations that is used to establish the allowable contaminant concentrations. Also, forward risk assessment can be performed to quantify the actual health risks posed by exposure to environmental contaminants. There are two methods used to estimate the potential risk of a contaminant concentration; Conventional or point risk estimation. Probabilistic risk assessment. 6.2 Conventional or Point Risk Estimation Point risk estimation uses a single variable to calculate the potential risk of a chemical contaminant for instance an adult individual body weight of 80 kilos, 1 to 2 liters of water as the daily ingestion rate for an individual, human lifespan of 70 years etc. Based on the above variable values the potential risk of cancer from a contaminant may be 10 in a million. The exposure of contaminants to humans have main three pathways; ingestion : consumption of medium containing the contaminant such as water or soil. dermal contact : absorption of the contaminant through skin contact. inhalation : breathing air containing the contaminant. The exposure might be either; carcinogenic(causing cancer), noncarncinogenic (causing other illness that is not cancer such skin rashes, suffocation, irritation) or both. Here, the potential health risk is estimated by first computing an average daily intake of the contaminant, and then integrating it with the contaminant health factor to quantify the risk. According to USEPA, 1989, the basic equation for computing daily intake or dose the ingestion or oral pathway is as follows; \\[I = {{C * IR * EF * ED}\\over{BW * AT}}\\] Where; \\(I\\): the chronic average daily intake in milligrams per kilogram of bodyweight of the contaminant per day. \\(C\\): the chemical concentration(eg. mg/kg for soil or mg/l for water) \\(IR\\): the ingestion rate(e.g 50mg/day for soil or 2liters/day for water). \\(EF\\): the exposure frequency(days per year that the exposure occurs) \\(ED\\): the exposure duration in years. \\(BW\\): body weight in kilograms. \\(AT\\): averaging time in days(which is equal to ED365 for noncarcinogens and 70365 for carcinogens, where 70 years is the assumed average human lifespan and there are 365 days in the year) Also, according to USEPA, 2004, this is the basic dermally absorbed intake or dose through the soil dermal contact pathway; \\[I = {{Cs * 10^{-6}SA*AF*EV*ABS*EF*ED}\\over{BW*AT}}\\] and this is the equation to calculate the daily absorbed intake or dose through water dermal contact pathway; \\[I = {{Cw * 10^{-3}SA*PC*ET*EF*ED}\\over{BW*AT}}\\] Where: \\(Cs\\) : is the soil concentration of the contaminant, that is, EPC, usually reported in mg/kg but the conversion factor of \\(10^{-6}\\) is applied to convert the units to kg/kg. \\(Cw\\) : is the water concentration of the contaminant, that is, EPC, usually reported in mg/l but the conversion factor of \\(10^{-3}\\) is applied to convert the units to mg/cm3 to maintain consistency of units. \\(SA\\): the exposed skin surface area (cm2). \\(AF\\): s the soil-to-skin adherence factor in milligrams per square centimeters per soil contact event (i.e.,mg/cm2 event) \\(EV\\): is the number of contact events per day, usually assumed as one event per day (i.e., 1/day). \\(PC\\): the dermal permeability constant (cm/h), which estimates the rate of transport of the contaminant across the skin into the body. \\(ABS\\): the dermal absorption fraction for the contaminant (unitless) Try it! ———Add Examples——————- One of the examples is the inhalation risk assessment (example 21.1) 6.3 Probabilistic Risk Assessment Using Monte Carlo Simulation While the method discussed above, point risk estimation, produces a single estimate of the health risk, probabilistic risk assessment produces probabilities or likelihoods of specified risks. This is due to variability and uncertanities regarding contaminant concentrations and other assumptions like daily water consumption, individual bodyweight, genetics and exposure to the contaminant. This leads to result in overestimation or underestimation of the actual exposure risk for some members. Probability risk assessment produces a full probability distribution of the risks indicating the probability or likelihood for each specified risk by using exposure characteristics and contaminant concentrations. Similarly, probability distributions are speciﬁed for other exposure characteristics such as water or soil ingestion rates, inhalation or breathing rates, and skin surface area, if the distributions are known. Monte Carlo simulation is then used to produce hundreds or thousands of various combinations of values from each of the probability distributions speciﬁed for the exposure characteristics and contaminant concentrations, and a point risk computed for each combination of inputs, thereby generating a probability distribution of risks Try it! ———Add Examples——————- One of the examples is the inhalation risk assessment (example 21.2). Will search examples from online 6.4 Practical Exercise Solution ________________________________________________________________________________ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
