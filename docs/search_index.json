[["index.html", "Ecological Analysis Chapter 1 INTRODUCTION 1.1 Data Distributions 1.2 Hands-on Exercises", " Ecological Analysis Basim Alsaedi 2024-11-16 Chapter 1 INTRODUCTION The distinction between data sample and the underlying data population is always ignored. For instance the mean concentration of a chemical contaminant in a soil sample of a contaminated site. Here, the data sample is the soil sample that was taken to the laboratory to find out the mean concentration of the contaminant while the data population is the entire data population consisting of all possible soil measurements of the chemical contaminant at the site. We need to define some terms to get this clear. Data Population: is the entire set of individuals, items or observations of interest in a study. For instance all the soil samples from the contaminant site. Data Sample: is the subset of the population selected for analysis. For example, measuring the height of randomly selected trees in the forest. Data Distribution: describes how data points are spread of arranged. Data Sample vs Data Population A deep understanding of the difference between data sample and data population is important in inferential statistics which seeks to generalize the results of data analyses performed on the data samples. A data population consists of all possible observations or data points concerning the characteristic of interest. For instance, when finding out the mean height of all the oak trees in a forest, the data population will be every height of every single oak tree in the forest. Contrary, data sample is usually a limited subset of observations drawn for the entire population. In this case we can have a data sample by randomly selecting 100 oak trees and measuring their heights. Try it! Lets simulate data of oak trees with R # Seed for reproducibility set.seed(123) # A population of 12500 oak trees population_heights &lt;- rnorm(12500, mean = 20, sd=5) head(population_heights) ## [1] 17.19762 18.84911 27.79354 20.35254 20.64644 28.57532 We have simulated the heights of 12500 oak trees in a forest. The variance population_heights represent the data population for the oak tree heights. Lets randomly select 86 trees from the entire data population of oak tree heights # Randomly select 86 trees sample_heights &lt;- sample(population_heights, size=86, replace = FALSE) tail(sample_heights) ## [1] 25.88792 29.03996 29.00965 37.10547 17.76631 19.81021 The variable sample_heights represents data sample for the oak tree heights. If we make this experiment more interesting, we find the mean and standard deviation for the data population and the data sample for the oak tree heights. Lets do it! # Mean and standard deviation of the entire population population_stats &lt;- c( Mean = mean(population_heights), SD = sd(population_heights) ) population_stats ## Mean SD ## 19.995609 4.996295 The entire oak tree data population has a mean height of approximately 20 meters and a standard deviation of approx 5 meters. Lets see how it compares with the data sample of the oak tree heights. # Mean and standard deviation of the sample sample_stats &lt;- c( Mean = mean(sample_heights), SD = sd(sample_heights) ) sample_stats ## Mean SD ## 21.094995 5.507379 There is a minor difference between the data sample and the entire data population of the oak tree heights. As you can see the selected sample of oak trees has an average height of 21 meters and a standard deviation of 5.5 meters. Practical Exercise Run the code below to generate the data of soil samples collected in farm with livestock. The main objective was to find the pH of the soil. Use the code below to simulate the collection of the data. # Set seed for reproducibility set.seed(76) # Simulate the data collection farm_population_pH &lt;- rnorm(n = 5342, mean=3.61, sd=0.8) head(farm_population_pH) ## [1] 3.864037 3.471632 3.544874 3.192810 3.239502 3.724600 You are required to work on the problems below; What is the Mean and Standard Deviation of the pH of the entire farm population soil samples. Select 65 random samples from the entire population. Calculate the Mean and Standard Deviation of the sample. How does the Mean and Standard deviation differ from the entire population? Solution Run the code below to generate the data of soil samples collected in farm with livestock. The main objective was to find the pH of the soil. Use the code below to simulate the collection of the data. # Set seed for reproducibility set.seed(76) # Simulate the data collection farm_population_pH &lt;- rnorm(n = 5342, mean=3.61, sd=0.8) head(farm_population_pH) ## [1] 3.864037 3.471632 3.544874 3.192810 3.239502 3.724600 You are required to work on the problems below; What is the Mean and Standard Deviation of the pH of the entire farm population soil samples. population_stats &lt;- c( Mean = mean(farm_population_pH), standard_deviation = sd(farm_population_pH) ) population_stats # show the results ## Mean standard_deviation ## 3.6218315 0.7934042 Select 65 random samples from the entire population. farm_sample_pH &lt;- sample(farm_population_pH, size = 65, replace = FALSE) head(farm_sample_pH) ## [1] 3.478034 4.268278 4.084203 4.044815 2.613637 3.575463 Calculate the Mean and Standard Deviation of the sample. sample_stats &lt;- c( Mean = mean(farm_sample_pH), standard_deviation = (sd(farm_sample_pH)) ) sample_stats # show the results ## Mean standard_deviation ## 3.7237092 0.7554283 How does the Mean and Standard deviation differ from the entire population? The sample mean is slightly higher than the whole population mean while the standard deviation of the whole populations is almost equal to the data sample’s standard deviation ________________________________________________________________________________ 1.1 Data Distributions Data Distribution describes how data values in data population are spread across the range of possible values. For instance, in our case the height of oak trees in the forest might be distributed normally, meaning that most trees cluster around the average height. Understanding data distribution is crucial in ecological analysis since it helps in; identifying patterns ( for instance seasonal changes in bird populations). choosing appropriate statistical tests. modelling ecological phenomena accurately. When dealing with continuous data values like height of oak trees, soil acidity or rainfall, Probability Distribution Functions is used to find the likelihood of different outcomes in a population. There are two types of probability distribution functions, namely; Cumulative distribution function, Empirical Cumulative distribution function. Below is the distribution of oak tree heights represented in a density plot. library(ggplot2) # Create a data frame from the data oak_df &lt;- data.frame(Height = population_heights) # Plot the density plot ggplot(oak_df, aes(x = Height)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of tree heights&quot;, x = &quot;Tree Height(m)&quot;, y = &quot;Density&quot; ) + theme_minimal() Above is the distribution of oak tree heights, lets find its probability using the two types of distribution functions. Cumulative Distribution Function This distribution function leans toward that a probability of a random variable is less than or equal to a certain value. We can randomly select a sample of heights from a population using the following code: sample_heights &lt;- sample(population_heights, size=86, replace = FALSE) tail(sample_heights) ## [1] 16.77457 21.71678 21.76080 36.45259 25.02581 14.43727 For example, if we analyze heights, the CDF could help answer: “What percentage of people have a height less than or equal to 170 cm?” The CDF of a normal distribution can be visualized using: x &lt;- seq(min(population_heights), max(population_heights), length = 100) cdf_values &lt;- pnorm(x, mean = mean(population_heights), sd = sd(population_heights)) plot(x, cdf_values, type=&quot;l&quot;, col=&quot;blue&quot;, lwd=2, xlab=&quot;Height&quot;, ylab=&quot;Cumulative Probability&quot;, main=&quot;Cumulative Distribution Function (CDF)&quot;) Empirical Cumulative Distribution Function The Empirical Cumulative Distribution Function (ECDF) is a way to estimate the CDF based on the actual observed data (your sample). Instead of assuming a theoretical distribution, it calculates the proportion of values that are less than or equal to each observed value in the sample. Lets create an ECDF plot in R using the sample heights # calculate CDF CDF &lt;- ecdf(sample_heights) # draw the cdf plot plot(CDF, main=&quot;Empirical Cumulative Distribution Function (ECDF)&quot;, xlab=&quot;Height&quot;, ylab=&quot;Cumulative Probability&quot;, col=&quot;red&quot;) As you can see, the ECDF graph shows a step-like increase whenever a new height is encountered and the curve provides a data-driven approximation of how heights are distributed in the sample. Here are the main differences between CDF(Cumulative Distribution Function) and ECDF(Empirical Cumulative Distribution Function); CDF is based on the entire population while ECDF is based on the observed sample data. The shape of CDF curve is smooth while the ECDF has a step-like plot. CDF is used to make model assumptions and probability estimates while ECDF is used to make data driven insights and explanatory analysis. Practical Exercise Solution ________________________________________________________________________________ 1.1.1 Types of Distribution In statistical tests, it is assumed that the data sample represents the underlying data population. Similarly, the distribution of the data sample is assumed to be similar to the one of the data population. These are what characterizes the data distribution; whether the data is; discrete or continuous symmetrical or asymmetrical bounded by lower and/or upper limits or unbounded. Lets discuss different types of data distributions. 1.1.1.1 Normal distribution The normal distribution is a continuous symmetrical distribution in the real number domain (i.e., with the set of possible values ranging between -∞ and ∞) whose probability density function plots as a smooth bell-shaped curve, and whose cumulative distribution is S-shaped. Try it! To demonstrate this, lets generate 100 random data values with a mean of 5 and a standard deviation of 3. set.seed(1) data &lt;- rnorm(100, mean=5, sd=3) df &lt;- data.frame(data) # Plot the data ggplot(df, aes(x = data)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of data sample&quot;, x = &quot;Sample Value&quot;, y = &quot;Density&quot; ) + theme_minimal() The data distribution below represents a near bell-shaped curve. A normal distribution with a mean of zero and variance of 1 (or standard deviation of 1, since standard deviation is the square root of variance and square root of 1 is 1) is referred to as a standard normal distribution A standard normal distribution has many applications in statistics such as computation of cumulative probabilities and critical values for hypothesis tests. Try it! Lets generate a random data set that is standardized. set.seed(100) data &lt;- rnorm(100, mean=0, sd=1) df &lt;- data.frame(data) # Plot the data ggplot(df, aes(x = data)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of data sample&quot;, x = &quot;Sample Value&quot;, y = &quot;Density&quot; ) + theme_minimal() Any data sample that follows a normal distribution can be standardized to a standard normal distribution by subtracting its mean from each individual data value and dividing the result by its standard deviation. The resultant values can be referred to as standard score, normal score, normal quantile or z-value) Here is the formula for calculation the z-value (standardized score); \\[z = {{x - \\overline x}\\over{\\sigma}}\\] Where; \\(x\\) is the actual value \\(\\overline x\\) is the data sample mean \\(\\sigma\\) is the standard deviation Try it ——————-will generate a data distribution with a known standard deviation and mean then standardize it. Will then plot both samples - raw data and the standardized one—————————- Example: CUMULATIVE AND EXCEEDANCE PROBABILITIES FOR A NORMAL DISTRIBUTION Groundwater Manganese Concentrations in mg/L ______________________Expand on this____________________________________ modify the code below to show up the distribution x = seq(0, 0.4, length=100) y = dnorm(x, 0.52, 0.18) polygon(c(0, x, 0.4), c(0, y, 0), col = &quot;gray&quot;) # Add a shaded area x = seq(1, 0.9, length=25) y = dnorm(x, 0.52, 0.18) polygon(c(1, x, 0.9), c(0, y, 0), col = &quot;gray&quot;) Goodness-of-Fit (GOF) Tests for the Normal Distribution In parametric statistical tests, it assumed that the data is normally distributed or can be normalized by data transformation such as log transform. The parameters, such as mean and standard deviation, in parametric tests must be specified. Verifying the data normality is crucial before conducting the tests. If it fails the test of normality other types of tests(non parametric) are considered. The GOF tests are used to access the normality of the data sample which requires 8 to 10 randomly picked data values. The tests are; Normal probability plot - also known as normal quantile plot. Coefficient of Variation(CV) Coefficient of Skewness Shapiro-Wilk(SW) and Shapiro-Francia(SF) procedures Filiben’s probability plot correlation coefficient (PPCC) Shapiro-Wilk multiple group normality The CV test is the most commonly used method. It is computed simply by dividing the standard deviation by the mean. If the resultant value is greater than 1 then the data fails the normality test, otherwise it passes. \\[CV = {\\sigma\\over{\\overline x}}\\] Other GOF tests that are used in testing for normal and other distributions are Anderson–Darling (AD) test and the Lilliefors–Kolmogorov–Smirnov test The Coefficient of Skewness stated above provides a more direct measure of skewness where skewness of the magnitude of greater than 0.5 is considered moderate to the substantial degree of skewness. ——————————Add an example———————— Central Limit Theorem ____________________simplify it with an example _________________________ 1.1.1.2 Lognormal, Gamma, and Other Continuous Distributions Before we dive into Gamma distribution, we need to to get the definition of exponential distribution. Exponential distribution is the probability of the waiting time between events in a Poisson Process. Imagine you’re analyzing environmental data, such as soil contamination levels or water flow rates. Often, these values are not normally distributed because they can’t be negative and tend to have a few extremely high values. This is where the lognormal distribution comes in—it describes data whose logarithms follow a normal distribution. For example, if you take the natural logarithm of highly skewed mercury concentration values, their distribution becomes nearly normal, making statistical analysis easier. However, when back-transforming results, the arithmetic mean can be underestimated, leading to what’s called transformation bias—but the median, often equal to the geometric mean, provides a useful estimate. Aside from lognormal, environmental studies frequently rely on other distributions such as gamma, logistic, and uniform, each useful in different scenarios. The gamma distribution, for instance, is often applied to model waiting times or rainfall amounts, offering flexibility through its shape and scale parameters. Understanding these distributions allows us to apply the right statistical methods, improve predictions, and make sense of complex real-world data patterns effectively. Lets dive deep into each type of the distribution mentioned Gamma Distribution Exponential infers the probability until the first event happens while Gamma distribution gives us the probability of the waiting time until the \\(n^{th}\\) event. The gamma distribution is bounded by zero on the left (no negative values) and can stretch infinitely to the right, making it suitable for data that exhibit long right tails. It is a powerful tool for modeling real-world data that is positively skewed, such as rainfall amounts, insurance claims, and even environmental contaminant levels. When dealing with skewed data, both the lognormal and gamma distributions are commonly used. However, the gamma distribution has an advantage: it avoids the pitfalls of transformation bias, which can arise when converting data back from logarithmic scales to the original scale. The gamma distribution is controlled by two key parameters: Shape parameter (k) – Determines the shape of the distribution. A higher k makes the distribution look more like a normal distribution. Scale parameter (θ) – Controls the spread or range of the distribution. Want to see how these parameters influence the distribution? Let’s generate some random data and visualize it! # Load necessary library library(ggplot2) # Generate random data from a gamma distribution set.seed(42) # Ensures reproducibility gamma_data &lt;- rgamma(1000, shape = 5, scale = 2) # Visualize the data distribution ggplot(data.frame(gamma_data), aes(x = gamma_data)) + geom_histogram(binwidth = 2, fill = &quot;skyblue&quot;, color = &quot;black&quot;, alpha = 0.7) + ggtitle(&quot;Histogram of Gamma-Distributed Data&quot;) + xlab(&quot;Values&quot;) + ylab(&quot;Frequency&quot;) + theme_minimal() Try it Try tweaking the shape and scale values in the code above to see how they affect the distribution. Increase the shape parameter (k) to see how the distribution approaches normality. Reduce the scale parameter (θ) to create a sharper peak. Want to check if your data follows a gamma distribution? You can perform Goodness-of-Fit (GOF) tests using the fitdistrplus package in R. The package is installed by: install.packages(&quot;fitdistrplus&quot;) library(fitdistrplus) ## Loading required package: MASS ## Loading required package: survival # Fit a gamma distribution to the data fit &lt;- fitdist(gamma_data, &quot;gamma&quot;) # Print summary of the fit summary(fit) ## Fitting of the distribution &#39; gamma &#39; by maximum likelihood ## Parameters : ## estimate Std. Error ## shape 4.851288 0.20991639 ## rate 0.491825 0.02242213 ## Loglikelihood: -2845.914 AIC: 5695.827 BIC: 5705.643 ## Correlation matrix: ## shape rate ## shape 1.0000000 0.9491175 ## rate 0.9491175 1.0000000 The gamma distribution is an excellent choice for modeling skewed data, offering flexibility and reliable statistical methods without the transformation bias of lognormal distributions. By experimenting with different parameters and visualizing the results, you can better understand its applications in environmental science, finance, and beyond! Practical Exercise Solution ________________________________________________________________________________ Logistic Distribution The logistic distribution, like the normal distribution, is symmetric and unbounded. However, it differs in having fatter tails, meaning it contains more extreme values on both sides. This makes it useful in various fields, such as: Logistic regression, where it models binary outcomes (e.g., success/failure). Ecological modeling, where population growth follows an S-shaped logistic curve. Machine learning, for classification problems. The image below shows the slight difference between the logistic and normal distribution Here are the key characteristics of lognormal distribution; Symmetry: Centered around a location parameter (mean). Scale Parameter: Determines the spread, similar to the standard deviation in normal distribution. Fat Tails: Greater occurrence of extreme values compared to the normal distribution. The cumulative distribution function (CDF) of the logistic distribution resembles the logistic function: \\[{P(t)}={{e^t}\\over{1 + e^t}}\\] In logistic regression, the probability of success is expressed as: \\[{\\pi(x)}={{e^{(b_0 +b_1x)}}\\over{1+e^{(b_0+b_1x)}}}\\] The logit function, which serves as the response variable in logistic regression, defined as; \\[{log({{\\pi(x)}\\over{1-\\pi(x)}})}={b_0 + b_1x}\\] Try it Let’s generate some logistic-distributed data and compare it with the normal distribution to understand their differences. The below one is a logistic distribution # Load necessary library library(ggplot2) # Generate random data from the logistic distribution set.seed(42) # Ensures reproducibility logistic_data &lt;- rlogis(1000, location = 0, scale = 1) # Visualize the data distribution ggplot(data.frame(logistic_data), aes(x = logistic_data)) + geom_histogram(binwidth = 0.5, fill = &quot;lightcoral&quot;, color = &quot;black&quot;, alpha = 0.7) + ggtitle(&quot;Histogram of Logistic-Distributed Data&quot;) + xlab(&quot;Values&quot;) + ylab(&quot;Frequency&quot;) + theme_minimal() Now let’s compare their probability density functions (PDFs). # Define x-axis range x &lt;- seq(-5, 5, length.out = 300) # Compute density for normal and logistic distributions normal_density &lt;- dnorm(x, mean = 0, sd = 1) logistic_density &lt;- dlogis(x, location = 0, scale = 1) # Plot the distributions plot(x, normal_density, type = &quot;l&quot;, lwd = 2, col = &quot;blue&quot;, ylab = &quot;Density&quot;, xlab = &quot;x&quot;, main = &quot;Normal vs Logistic Distribution&quot;) lines(x, logistic_density, lwd = 2, col = &quot;red&quot;, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Normal Distribution&quot;, &quot;Logistic Distribution&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lty = c(1, 2), lwd = 2) You can try experiment with different location and scale parameters in the above code to see how the logistic distribution changes. Location is changes to shift the distribution left and right while the scale is increased to make the distribution spread out more. Want to see logistic regression in action? Here’s a quick example of modeling a binary outcome. # Simulating data set.seed(42) x &lt;- rnorm(100) y &lt;- rbinom(100, 1, plogis(0.5 + 1.2 * x)) # Logistic function # Fit logistic regression logistic_model &lt;- glm(y ~ x, family = binomial) # Summary of model summary(logistic_model) ## ## Call: ## glm(formula = y ~ x, family = binomial) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2892 0.2357 1.227 0.22 ## x 1.3348 0.3044 4.385 1.16e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 136.66 on 99 degrees of freedom ## Residual deviance: 107.25 on 98 degrees of freedom ## AIC: 111.25 ## ## Number of Fisher Scoring iterations: 4 The logistic distribution is a powerful tool for modeling data with skewed distributions and binary outcomes. Understanding its properties, generating data, and visualizing it interactively can provide valuable insights, especially in fields like statistics, machine learning, and population modeling. Practical Exercise Solution ________________________________________________________________________________ Other Continuous Distributions Other types of continuous distributions include; Uniform distribution: is appropriate when the only available data consists of the minimum value and maximum value . All values within this range have an equal probability of occurring, meaning the probability density function (PDF) is constant at within the interval , and zero elsewhere. Triangular distribution: is suitable when the minimum value , maximum value , and most likely value are known. This distribution assumes that values increase linearly from to the peak , and then decrease linearly to. install.packages(&quot;triangle&quot;) library(triangle) Extreme Value distributions: Extreme value distributions model the smallest or largest values in a dataset. A common example is analyzing annual peak streamflow rates at a monitoring station to estimate flood magnitudes expected within a specific period (e.g., 50-year or 100-year floods). One commonly used extreme value distribution is the Log-Pearson Type III distribution, which is right-skewed and bounded on the left by zero (since negative values are not possible). This distribution is widely used in flood frequency analysis and is related to the gamma distribution. For analyzing low flow values, distributions such as the Weibull and Gumbel distributions are commonly employed. These distributions are particularly useful for estimating probabilities of extreme low flow conditions over time. Try it! Practical Exercise! Solution ________________________________________________________________________________ 1.1.1.3 Distributions Used in Inferential Statistics When the sample size is large (typically ≥30) and the data are not highly skewed (skewness coefficient ≤ 0.5 or 1), the central limit theorem allows for approximate normality, enabling extrapolations from sample statistics to population parameters. However, for smaller samples—common in environmental data—or when the population standard deviation is unknown, exact sampling theory is preferred, leading to the use of distributions like Student’s t, chi-square, and F distributions. These distributions are essential for statistical inference, including confidence interval estimation, hypothesis testing for population means, and significance tests for regression coefficients. 1.1.1.3.1 Student’s t Distribution This type of distribution is essential for inference when the population standard deviation is unknown. The t-distribution is used for hypothesis testing, confidence intervals, and regression analysis, among other applications. Given a random sample \\(x_1,x2,...,x_n\\) from a normal distribution with an unknown mean and standard deviation , the t-statistic is given by: \\[t = {{x-\\overline X}\\over{s/\\sqrt n}}\\] Where: \\(x\\) is the new observation \\(\\overline X\\) is the sample mean \\(s\\) is the sample standard deviation \\(n\\) is the sample size It can be used in ecology to; Compare species populations – if the two populations have the same mean Assessing biodiversity changes – like before and fater habitat restoration differs significantly Analyzing environmental variables – like poullution levels between two forests areas differs. Try it! We will investigate difference in tree heights in a forested area. Generationg and plotting t-distribution # Load necessary library library(ggplot2) # Define x values t_values &lt;- seq(-4, 4, length=100) # Compute density for different degrees of freedom df2 &lt;- dt(t_values, df=2) df12 &lt;- dt(t_values, df=12) norm &lt;- dnorm(t_values) # Standard normal data &lt;- data.frame(x=t_values, StdNormal=norm, df2=df2, df12=df12) # Plot the distributions ggplot(data, aes(x=x)) + geom_line(aes(y=StdNormal, linetype=&quot;Std Normal&quot;)) + geom_line(aes(y=df2, linetype=&quot;t, df = 2&quot;)) + geom_line(aes(y=df12, linetype=&quot;t, df = 12&quot;)) + labs(y=&quot;Probability Density&quot;, x=&quot;t values&quot;) + scale_linetype_manual(&quot;&quot;, values=c(&quot;solid&quot;, &quot;dashed&quot;, &quot;dotdash&quot;)) + theme_minimal() Perform two sample t-test # Sample data (species richness in two forest areas) area_A &lt;- c(23, 25, 27, 30, 28, 26, 29, 31) area_B &lt;- c(19, 20, 22, 24, 21, 23, 22, 25) # Perform two-sample t-test t.test(area_A, area_B, var.equal=TRUE) ## ## Two Sample t-test ## ## data: area_A and area_B ## t = 4.558, df = 14, p-value = 0.0004468 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.845765 7.904235 ## sample estimates: ## mean of x mean of y ## 27.375 22.000 Lets now visualize the critical regions in the t-distributions # Define x values x_vals &lt;- seq(-4, 4, length=200) y_vals &lt;- dt(x_vals, df=10) # Critical region (alpha = 0.05, two-tailed) critical_val &lt;- qt(0.975, df=10) # Plot plot(x_vals, y_vals, type=&quot;l&quot;, ylab=&quot;Probability Density&quot;, xlab=&quot;t values&quot;, main=&quot;t-Distribution with Critical Regions&quot;) abline(v=c(-critical_val, critical_val), col=&quot;red&quot;, lwd=2, lty=2) This will help you with the interpretation Compares the t-distribution with different degrees of freedom to the normal distribution. The t-distribution has heavier tails for small degrees of freedom. One-Sample t-Test: Tests if the mean tree height significantly differs from 14m. Two-Sample t-Test: Tests if species richness significantly differs between two areas. Critical Region Visualization: Shows rejection regions for hypothesis testing. Practical Exercise Solution ________________________________________________________________________________ 1.1.1.3.2 Chi-Square Distribution The Chi-square(\\(x^2\\)) distribution is widely used in ecological research for analyzing categorical data and testing statistical hypotheses. Many ecological studies involve count data, presence-absence data, or species distribution models, where the Chi-square test provides insights into species interactions, habitat preferences, and environmental associations. This type of chi-square distribution arises when we sum the squares of independent standard normal variables. In ecology, chi-square distribution may be used to analyze species-habitat relationships and species interactions. Try it! We will analyze the presence and absence of species A in a habitat in a contingency table. Null Hypothesis - Species A’s occurrence is independent of habitat type Alternative Hypothesis - Species A prefers a particular habitat # Create contingency table species_habitat &lt;- matrix(c(30, 20, 15, 35), nrow=2, byrow=TRUE) colnames(species_habitat) &lt;- c(&quot;Present&quot;, &quot;Absent&quot;) rownames(species_habitat) &lt;- c(&quot;Forest&quot;, &quot;Grassland&quot;) # Perform Chi-square test chisq.test(species_habitat) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: species_habitat ## X-squared = 7.9192, df = 1, p-value = 0.004891 The p-value is way below 0.05 therefore we reject the null hypothesis. We have enough evidence to conclude that Species A prefers a particular habitat. Lets repeat the analysis with observed and expected bird counts on different wetland sites. We will use goodness-of-fit test Null hypothesis - Birds are evenly distributed. Alternate hypothesis - Birds are not evenly distributed. # Observed and expected bird counts observed &lt;- c(45, 55, 60, 40) expected &lt;- c(50, 50, 50, 50) # Perform Chi-square goodness-of-fit test chisq.test(observed, p = rep(1/4, 4)) # Equal probability across sites ## ## Chi-squared test for given probabilities ## ## data: observed ## X-squared = 5, df = 3, p-value = 0.1718 p-value is above 0.05 therefore we conclude that the birds are evenly distributed. The chi-square distribution is used to construct confidence intervals for variance estimates such as error variance in ecological regression models. The F-distributions, which is the ration of two chi-square distributions is used in ANOVA tests for comparing species abundance across multiple environmental conditions. Lets perform an example to demonstrate this par(mfrow = c(1,2)) # Chi-square distribution for different degrees of freedom curve(dchisq(x, df=2), from=0, to=10, ylab=&quot;Probability density&quot;, main=&quot;Chi-square Distributions in Ecology&quot;, lwd=1, lty=1) curve(dchisq(x, df=5), from=0, to=10, add=TRUE, lty=2) curve(dchisq(x, df=10), from=0, to=10, add=TRUE, lty=3) legend(&quot;topright&quot;, legend=c(&quot;df = 2&quot;, &quot;df = 5&quot;, &quot;df = 10&quot;), lty=1:3, bty=&quot;n&quot;) # F-distribution for different df values curve(df(x, df1=1, df2=10), from=0, to=5, ylab=&quot;Probability density&quot;, main=&quot;F Distributions in Ecology&quot;, lwd=1, lty=1) curve(df(x, df1=6, df2=60), from=0, to=5, add=TRUE, lty=2) curve(df(x, df1=10, df2=100), from=0, to=5, add=TRUE, lty=3) legend(&quot;topright&quot;, legend=c(&quot;df1 = 1, df2 = 10&quot;, &quot;df1 = 6, df2 = 60&quot;, &quot;df1 = 10, df2 = 100&quot;), lty=1:3, bty=&quot;n&quot;) 1.1.1.3.3 F Distribution The F-distribution, named after the British statistician Ronald Fisher, is commonly used in ecological studies for testing hypotheses involving variance and regression models. It is the distribution of the F-statistic, which is computed as the ratio of two chi-square distributed variables, each normalized by their respective degrees of freedom. Mathematically, the F-statistic is expressed as: \\[F = {{X_{k1}^2/k1}\\over{X_{k2}^2/k2}}\\] where: \\(X_{k1}^2/k1\\) is a chi-square distributed variable with \\(k1\\) degrees of freedom, \\(X_{k1}^2/k1\\) is anothe chi-square distributed variable with \\(k2\\) degress of freedom Try it! Lets visualize F-distribution in Ecology par(mfrow = c(1,1)) # F-distributions for different degrees of freedom curve(df(x, df1=1, df2=10), from=0, to=5, ylab=&quot;Probability Density&quot;, main=&quot;F Distribution in Ecology&quot;, lwd=1, lty=1) curve(df(x, df1=6, df2=60), from=0, to=5, add=TRUE, lty=2) curve(df(x, df1=10, df2=100), from=0, to=5, add=TRUE, lty=3) legend(&quot;topright&quot;, legend=c(&quot;df1 = 1, df2 = 10&quot;, &quot;df1 = 6, df2 = 60&quot;, &quot;df1 = 10, df2 = 100&quot;), lty=1:3, bty=&quot;n&quot;) As df1 and df2 increase, the F-distribution becomes less-skewed and more bell-shaped. 1.1.1.4 Dicrete Distributions A random variable is considered discrete if it takes only whole-number values, rather than fractions (for example, a class cannot have 10.5 students). Discrete data include both count data and categorical or qualitative data, such as whether an organism is present or absent or whether a value exceeds a control limit. Two commonly used distributions for modeling discrete data are the binomial distribution, which applies to categorical data, and the Poisson distribution, which is used for count data. 1.1.1.4.1 Binomial Distribution The binomial distribution models the number of successes in a fixed number of independent trials, where each trial has only two possible outcomes: success (1) or failure (0). Imagine we are testing 20 groundwater monitoring wells for a contaminant. Each well either detects the contaminant (success) or does not detect it (failure). For instance; Probability of detecting the contaminant (success): p = 0.4 Probability of not detecting the contaminant (failure): q = 1 - p = 0.6 The probability of getting exactly successes in n trials is given by: \\[{P(X=k)}={\\begin{pmatrix} n \\\\ k \\end{pmatrix}p^k(1-p)^{n-k}}\\] Where: \\({\\begin{pmatrix} n \\\\ k \\end{pmatrix}}={{n!}\\over{k!(n-k)!}}\\) is the binomial coefficient. \\(p\\) is the probability of success. \\((1-p)\\) is the probability of failure. \\(k\\) is the number of successes. For a Binomial(n, p) distributions: Mean(Expected Value): \\[E(X) = np\\] This tells us the average of successes over many trials. Variance \\[\\sigma^2 = np(1-p)\\] Standard deviation \\[\\sigma = \\sqrt{np(1-p)}\\] In this case, for our groundwater monitoring example Expected Value: \\(E(X) = 20 X 0.4 = 8\\) Standard Deviation \\[\\sigma = \\sqrt{20 x 0.4 X 0.6} = \\sqrt {4.8} = 2.19\\] Lets apply R for this example of groundwater contamination. R provides four main functions to work with the binomial distribution: dbinom(x, size, prob): Computes \\(P(X=x\\), the probability of exactly x successes. pbinom(x, size, prob): Computes \\(P(\\geq x)\\), the cumulative probability. qbinom(p, size, prob): Computes the quantile for a given probability. rbinom(n, size, prob): Generates random binomial values (simulating real-world experiments) Try it! Lets simulate binomial outcomes set.seed(123) # For reproducibility rbinom(10, size = 20, prob = 0.4) ## [1] 7 10 7 11 11 4 8 11 8 8 This simulates 10 trials where we sample 20 wells each time, and each well has a 40% chance of detecting the contaminant. Lets try the probability of detecting the contaminant in exactly 9 wells dbinom(9, size = 20, prob = 0.4) ## [1] 0.1597385 This means there’s a 15.97% chance that exactly 9 out of 20 wells will detect the contaminant. Now lets try the Probability of Detecting the Contaminant in 9 or Fewer Wells to find the cumulative probability of detecting the contaminant in at most 9 wells. pbinom(9, size = 20, prob = 0.4) ## [1] 0.7553372 Lets plot it par(lend = 2) # Adjust bar ends plot(0:15, dbinom(0:15, 20, 0.4), type = &quot;h&quot;, lwd = 6, xlab = &quot;# of Wells with Contaminant&quot;, ylab = &quot;Probability&quot;, main = &quot;Binomial Distribution (n=20, p=0.4)&quot;) From the graph; The most probable outcome is finding the contaminant in 8 wells. Extreme values (0 or 15 wells detecting contaminants) are very unlikely. What if? When we increase n (e.g., testing 100 wells instead of 20), the distribution becomes smoother and begins to resemble a normal distribution. plot(0:40, dbinom(0:40, 100, 0.4), type = &quot;h&quot;, lwd = 6, xlab = &quot;# of Wells with Contaminant&quot;, ylab = &quot;Probability&quot;, main = &quot;Binomial Distribution (n=100, p=0.4)&quot;) The graph is left skewed when N was adjusted 1.1.1.4.2 Poisson Distribution Suppose a count of the wood turtle, a protected species under the New Jersey Endangered Species Statute, is conducted in a specific geographic region. What would be a suitable data distribution model for the results obtained? Since counts are discrete rather than continuous (e.g., there can be 5 or 12 turtles, but not 7.4 turtles), continuous distribution models such as the normal, lognormal, and gamma distributions would not be appropriate. Additionally, because we are interested in the number of turtles rather than their presence or absence, the binomial distribution—though discrete—would also not be suitable, as it is designed for binary data (i.e., yes/no types of data). The Poisson distribution, named after Siméon Poisson, is a discrete probability distribution used to model the number of events occurring within a fixed space or time interval, given an estimated average rate of occurrence (denoted by λ). If our focus were on the presence or absence of turtles in a given area rather than their actual count, the binomial distribution would be the appropriate choice. The probability mass function (PMF) for the Poisson distribution is given by: \\[P(X=k) = {{e^{-\\lambda}\\lambda^k}\\over{k!}}\\] Where: \\(P(X=k)\\) is the probability that the Poisson-distributed random variable \\(X\\) takes a value \\(k\\) (where \\(k\\) can be 0, 1, 2, etc) \\(\\lambda\\) is the mean or expected number of occurrences. \\(e\\) is the base of the natural logarithm (apporximately 2.71828) \\(k!\\)(k factorial) is computed as \\(k \\times (k-1) \\times(k-2) \\times...\\times 1\\) For the Poisson distribution, the mean (expected value) is \\(\\lambda\\), and the variance is also equal to \\(\\lambda\\), meaning the distribution’s spread is directly tied to the mean. Consider a scenario where the expected number of turtles per hectare is 5. The probability of finding exactly 10 turtles per hectare can be computed in R as: dpois(10, 5) ## [1] 0.01813279 This yields approximately 0.0181 (or 1.81%). Conversely, the probability of finding 10 or fewer turtles can be obtained using: ppois(10, 5) ## [1] 0.9863047 which results in 0.9863 (or 98.63%). Thus, the probability of finding more than 10 turtles is: \\[1 - 0.9863 = 0.0137\\] Lets visualize this; The following R script generates Poisson probability mass function plots for different values of \\(\\lambda\\) (2, 5, and 20): par(mfrow = c(1, 3)) par(lend = 2) plot(0:10, dpois(0:10, 2), type = &#39;h&#39;, lwd = 6, xlab = &#39;Count or # of Occurrences (k)&#39;, ylab = &#39;Probability (P(X = k))&#39;) mtext(side = 3, line = 0.5, expression(lambda == 2)) plot(0:15, dpois(0:15, 5), type = &#39;h&#39;, lwd = 4, xlab = &#39;Count or # of Occurrences (k)&#39;, ylab = &#39;Probability (P(X = k))&#39;) mtext(side = 3, line = 0.5, expression(lambda == 5)) plot(0:40, dpois(0:40, 20), type = &#39;h&#39;, lwd = 2, xlab = &#39;Count or # of Occurrences (k)&#39;, ylab = &#39;Probability (P(X = k))&#39;) mtext(side = 3, line = 0.5, expression(lambda == 20)) As \\(\\lambda\\) increases, the Poisson distribution becomes more symmetric and begins to resemble the normal distribution. The Poisson distribution can also serve as an approximation for the binomial distribution when the number of trials (\\(n\\)) is large and the probability of success (\\(p\\)) is small, such that \\(\\lambda = np\\). Typically, the Poisson approximation to the binomial is valid when \\(n \\geq 20\\) and \\(p\\leq 0.05\\), or when \\(n \\geq 100\\) and \\(np \\geq 10\\). A common application of the Poisson distribution in environmental science is Poisson regression, where the response variable consists of event counts assumed to follow a Poisson distribution, while predictor variables influence these counts. For instance, turtle population density per hectare might be modeled as a function of habitat type (e.g., forested wetlands, upland forests, agricultural fields) and proximity to roadways or urban areas. A key issue in Poisson regression is overdispersion—when the observed data variance significantly exceeds the mean. Overdispersion can lead to poor model fit and necessitate alternative models, such as the negative binomial distribution. In summary, the Poisson distribution provides a useful model for count data, particularly when events occur independently over a fixed interval of time or space. 1.2 Hands-on Exercises Solution ________________________________________________________________________________ "],["hypothesis-testing.html", "Chapter 2 HYPOTHESIS TESTING 2.1 Introduction 2.2 Parametric tests 2.3 Nonparametric test 2.4 One-Way ANOVA 2.5 Hands-on Exercise", " Chapter 2 HYPOTHESIS TESTING 2.1 Introduction Hypothesis testing is a standard statistical procedure for deciding between two competing alternatives or scenarios, based on evidence presented by a data sample collected for the purpose of conducting the test. Most descriptive statistics such as mean, median and standard deviation are usually computed from data samples which may vary with the whole population. For instance, we want to find the concentration of hexavalent chromium, a human carcinogen, in soil. We will collect random 20 samples. The average concentration of the carcinogen from the collected samples will at most vary with the average concentration in the entire land, that’s where hypothesis testing comes in. Basic terminology and procedure for hypothesis testing There is a need to understand the basic terms before diving deep into the process of hypothesis testing. If we have historical data, the presence of a trend must be established and below are the assumptions that should be made prior to conducting hypothesis testing; The data is assumed to belong to a specified distribution(typically normal distribution). The data has a stable or stationary trend(free from of temporal or spatial trend). There are no unusual observations or outliers. The hypothesis testing that is based on the above is described as parametric hypothesis testing. Otherwise, if the above assumptions are not met, the test is known as non-parametric hypothesis testing. Null hypothesis (\\(H_0\\)): Is a statement of the presumed default or baseline condition. In this case the null hypothesis is that there is no difference between the carcinogenic concentration is below the allowed limit. Alternate hypothesis (\\(H_1\\) or \\(H_A\\)): is the inverse or the opposite of the null hypothesis. Alternate hypothesis contradicts the statement of the presumed default. In this case, the alternate hypothesis states that the carcinogenic level is above the allowed limit. The evidence produced by the sample after conducting hypothesis testing can either refute or support the null hypothesis. The one-tailed (One-Sided) Hypothesis test checks for an effect in only one direction. In our case, we test if the carcinogen levels exceed the allowed limit(right side). Alternatively, it can be checked of it is below the allowed limits (left-sided). Upper-Tailed Hypothesis Test is the same as the right one-tailed hypothesis test that checks if the value of interest exceed the null hypothesis value Lower-Tailed Hypothesis Test is the left one-tailed hypthesis test where it checks if the value of interest is below the null hypothesis value. In our case the allowed limit for carcinogens in soil. Two-Tailed Hypothesis Test checks for the effect in both directions. In our we can change to find if the carcinogen levels are below or above the allowed limit. T-Value is the test statistic calculated from the sample data to determine how far the sample mean is from the hypothesized mean, in terms of standard error. Critical T-Value is the threshold value for the t-distribution that corresponds to the significance level. If the t-value exceeds the critical t-value, the null hypothesis is rejected. Significant level(\\(\\alpha\\)) is the probability threshold for rejecting the null hypothesis, usually set to 0.05(5%). In most cases if the probability of observing the test statistic (p-value) is less than 0.005, the null hypothesis is rejected. Degrees of Freedom(df) are the number of independent values in the data sample that are free to vary when calculating a statistic. In most cases df is calculated as \\[df = n - 1\\] where \\(n\\) represents the sample size(count). In our cases where we have 20 samples therefore \\(df = 20 -1\\) and the degree of freedom will be 19 P-Value indicates the probability of obtaining a test statistic at least as extreme as the observed one, assuming the null hypothesis is true. Below are the steps taken in hypothesis testing; Formulate the null and alternate hypothesis. Assume the null hypothesis is true and compute a test statistic that follows a known distribution. Calculate a test statistic value based on the data sample which follows a known distribution and determine the critical value for chosen distribution at a given significance level. Compare the test statistic to the critical value to assess the plausibility of the null hypothesis. For symettrical distributions(e.g t, normal), critical values are located on; upper-tailed test: positive side of the distribution lower-tailed test: negative side of the distribution two-tailed test: both sides, with equal magnitude Calculate the p-value to find how the probability of obtaining a test statistic more extreme than the observed value. Try it! To get the steps above clear, lets work on an example. This is an hypothetical problem where a regulatory body sets that the acceptable phosphorous concentration in a lake at 1.5mg/L. 30 sample measurements were collected at different locations in the lake. The objective was to determine if the phosphorous levels are significantly higher than the allowed limit. Lets simulate the data. set.seed(123) # for reproducibility phosphorous_levels &lt;- rnorm(n = 30, mean = 1.7, sd = 0.2) Formulate the null hypothesis Null Hypothesis(\\(H_0\\)): \\(\\mu\\) &lt; 1.5 - phosphorous levels are within the acceptable levels. Alternate Hypothesis (\\(H_1\\)): \\(\\mu\\) &gt; 1.5 - phosphorous levels exceed the acceptable limit. This is a right-sided(one-sided) test since we are only interested in detecting an increase in phosphorous levels We use a one-sample ttest since the population data standard deviation is unknown, and the sample size is small(n=30). The formula for t-statistic is calculated as; \\[t = {{\\overline{x} - \\mu_0}\\over{s/\\sqrt{n}}}\\] Where: \\(\\overline{x}\\) is the sample mean. \\(\\mu_0\\) is the population mean under null hypothesis (1.5mg/L) \\(s\\) is the sample standard deviation. \\(n\\) is the sample size. Perform the one-sample t-test # One sample t-test t_test_result &lt;- t.test(phosphorous_levels, mu = 1.5, alternative = &quot;greater&quot;) # Display the results print(t_test_result) ## ## One Sample t-test ## ## data: phosphorous_levels ## t = 5.3201, df = 29, p-value = 5.21e-06 ## alternative hypothesis: true mean is greater than 1.5 ## 95 percent confidence interval: ## 1.629713 Inf ## sample estimates: ## mean of x ## 1.690579 Determine the critical value For the right-tailed test at a 5% significance levels(\\(alpha\\)=0.05), the critical value is the t-value that corresponds to the top 5% of the t-distribution. The qt() function will be used to calculate this; \\[t_{critical} = qt(1-\\alpha, df = n - 1)\\] critical_t &lt;- qt(0.95, df = length(phosphorous_levels) - 1) critical_t ## [1] 1.699127 The t-statistic is greater than the critical t therefore the null hypothesis is rejected. Lets confirm this by also computing the p_value Calculate the null hypothesis p_value &lt;- t_test_result$p.value p_value ## [1] 5.210072e-06 The null hypothesis is rejected as the p_value is less than 0.05. Therefore, we make conclusion that the phosphorous levels in the lake exceed the acceptable levels. 2.2 Parametric tests As described before parametric tests rely on the assumptions that the data follows a known distribution (commonly the normal distribution) and it should be free of major outliers. In case where the test requires the data to follow a normal distribution, if the sample size is small it must follow a normal distribution while for larger sample size (e.g n≥30) the Central Limit Theorem allows the researchers to approximate normality even of the data is not perfectly normal 2.2.1 Parametric single-sample test This type of test is used to determine whether the mean of a single sample is significantly different from a known or hypothesized population mean. It assumes that the sample data comes from a population that follows a normal distribution (or has a large enough sample size to approximate normality). The main objective of this test is to test whether the sample mean differs significantly from a reference or target value, such as an industry standard, a population average, or a theoretical expectation in hypothesis testing These are the steps taken for One-Sample T-test; State the Hypotheses: Null Hypothesis(\\(H_0\\)): The sample mean equals the population mean (\\(\\mu = \\mu_0\\)) Alternate Hypothesis(\\(H_A\\)): The sample mean is different from the population mean(\\(\\mu \\neq \\mu_0\\)) Compute the test statistic The formula for test statistic is \\[t = {{\\overline x - \\mu_0}\\over{s/\\sqrt n}}\\] Where: \\(\\overline x\\): is the sample mean \\(\\mu_0\\): is the hypothesized population mean \\(s\\): is the sample standard deviation \\(n\\): is the sample size Determine the degree of freedom as \\(df = n- 1\\) Compare \\(t\\) to critical value or use p-value 2.2.2 Parametric two-sample test Lets understand what is two-sample test, before specifically diving into the parametric two sample test; Two-sample tests are essential statistical tools used to compare two different populations to determine if there are significant differences between them. These tests are widely applied in ecology to compare environmental factors such as contaminant levels, species diversity, or soil nutrient concentrations across different locations or time periods. Two-sample tests help us compare the means, medians, or other characteristics of two groups. For example, scientists might want to know if the average soil arsenic concentration in an industrial area is higher than that in a nearby natural reserve. To perform these tests, data must be collected from both groups being compared. Here are the assumptions of two-sample tests; Independence of Data Values: Data points should not influence each other. No trends in data: data should not show patterns over time for instance increasing and decreasing trends. If trends exist, adjustments such as pairing or deseasonalization should be considered. In this case, we are focusing on the parametric tests – a parametric two-sample test is carried when the data follows a normal distribution. There are two types of parametric two-sample tests; independent and paired sample tests. Independent Two-Sample Test These tests are used when the two groups being compared have no inherent connection. For example: Comparing soil arsenic levels up-gradient vs. down-gradient of an industrial facility. Measuring PCB concentrations upstream and downstream of a river. In these cases, since the samples are collected from different locations without any relationship, an independent test is appropriate. These are the steps taken when carrying out independent tests; Calculate the test statistic based on the difference between sample means. Compare the test statistic with a critical value from the t-distribution. If the test statistic is extreme or the p-value is less than the significance level (e.g., 0.05), reject the null hypothesis. Paired Two-Sample Tests Paired tests are used when the samples are related or dependent on each other. This often happens when measurements are taken from the same location at different times. Examples include: Before and After Cleanup: Measuring soil contaminant levels at a site before and after remediation. Duplicate Sample Analysis: Sending identical samples to two different laboratories to assess the consistency of results. Before carrying out paired two-sample test it is assumed that the data is paired(each observation in one group corresponds to an observation in the other group), differences between pairs should follow a normal distribution and no extreme outliers should be present. Here are the steps taken when performing paired two-sample test; Calculate the differences between paired observations. Compute the mean and standard deviation of the differences Calculate the t-statistic Compare the test statistic with the critical value or p-value If the test statistic is extreme or the p-value is less than the significance level, reject the null hypothesis. Here is the formula for calculating the t-statistic; \\[{t_0} = {{\\overline D}\\over{S_D/\\sqrt n}}\\] where; \\(\\overline D\\) and \\(S_D\\) is the mean and standard deviation of the differences between the data samples of the two populations \\(n\\) is the sample size The null hypothesis \\(H_0\\) is that the population mean difference between the two populations, $_D $ is zero Imagine an ecologist wants to determine if the cleanup of an industrial site has effectively reduced contaminant levels. By comparing pre- and post-cleanup soil samples at the same locations, they can assess the success of remediation efforts using a paired two-sample test. Similarly, comparing background levels with site concentrations using an independent two-sample test can help identify pollution sources. Try it! ————-Add an example exercise—————– Here are the important considerations when performing parametric two sample test; If the data is not normally distributed, applying a logarithmic transformation can help, but it should be done carefully to avoid misinterpretation. These tests are often conducted using statistical software such as R, Python, or Excel for ease and accuracy. Practical exercises Industrial fertilizer use can increase organic carbon levels in groundwater, potentially impacting water quality and ecosystem health. In this exercise, you will use three parametric tests to answer the following questions: Single-Sample Test: Is the average organic carbon concentration in background (control) wells significantly different from a regulatory threshold (e.g., 25 ppm)? Paired Two-Sample Test: Is there a significant difference in organic carbon concentration between cleaned water and before? Independent Two-Sample Test: Your are provided with water portability data set that can be downloaded from here Solution ________________________________________________________________________________ 2.3 Nonparametric test 2.3.1 Nonparametric One-Sample Wilcoxon Signed-Rank Test The Wilcoxon Signed-Rank (WSR) Test is a nonparametric test used to determine whether the median of a population differs from a fixed reference value. Unlike the sign test, which only considers the direction of differences, the WSR test accounts for the magnitude of deviations as well. Here are the asssumptions to considered when performing the test; The data values are independent. The underlying population is symmetrically distributed (but not necessarily normal) around the median. The number of tied values should be minimal. The test cannot handle nondetect (ND) values because it requires actual magnitudes for ranking. Just follow the procedures outline below to perform the test; Compute the deviations from the reference value. Rank the absolute values of the deviations (smallest gets rank 1, next smallest gets rank 2, etc.). Assign average ranks in case of tied values. Sum the ranks corresponding to positive deviations. Compare the test statistic with a critical value (from tables) or use a normal approximation if sample size is large (n &gt; 20). Compute a p-value and determine statistical significance. Try it! Suppose we have measured pollutant concentrations at a site and want to test whether the median concentration is different from a standard of 50 ppm. Lets formulate the null and the alternate hypothesis before carrying out the test. Null Hypothesis(\\(H_0\\)): The median of the population is equal to the reference value. Alternate Hypothesis(\\(H_A\\)): The median is different from the reference value. Now lets perform the test, where it will return the W#-statistic and p-value if: If p &lt; 0.05, we reject H₀, indicating that the median is significantly different from 50 ppm. If p &gt; 0.05, we fail to reject H₀, meaning there is no strong evidence to say the median differs from 50 ppm. # Load required package set.seed(123) data_values &lt;- c(45, 55, 60, 52, 48, 51, 53, 47, 49, 54) reference_value &lt;- 50 # Perform Wilcoxon Signed-Rank Test wilcox.test(data_values, mu = reference_value, alternative = &quot;two.sided&quot;) ## ## Wilcoxon signed rank test with continuity correction ## ## data: data_values ## V = 36, p-value = 0.4136 ## alternative hypothesis: true location is not equal to 50 Note that! The test does not work well if the data contain ND values (nondetects), since exact magnitudes are needed. Works best when the data are symmetrically distributed For large samples (n &gt; 20), the normal approximation is used instead of exact critical values. The Wilcoxon Signed-Rank Test is a useful nonparametric alternative to the one-sample t-test when normality is questionable. It provides a robust way to assess median differences while taking data magnitudes into account. Practical Exercise Acid rain is a major environmental concern, particularly in industrial areas. In our study, we are interested in understanding the effect of emission controls on rainwater acidity. Rainwater pH values are recorded before and after the intervention at the same 15 monitoring stations. Because pH measurements are continuous but the data set is relatively small and may not meet the assumptions of parametric tests, nonparametric methods offer a robust alternative. You are required to perform to use the acid_rain data set provided by the instructor to perform Nonparametric One-Sample Wilcoxon Signed-Rank Test. The objective of this project is to test whether the median rainwater pH before the intervention is different from the regulatory standard of 5.5. Solution Project Objective: Test whether the median rainwater pH before the intervention is different from the regulatory standard of 5.5. Hypotheses: Null Hypothesis: The median pH before intervention is 5.5 Alternative Hypothesis: The median pH before intervention is not 5.5. Mathematical Concept: For each observation, compute the difference: \\[d_i = pH_i - 5.5\\] Then rank the absolute differences \\(|d_i|\\) (ignoring zeros) and sum the ranks for positive differences. The Wilcoxon test statistic \\(W\\) is compared against its distribution to determine significance. # Load necessary library library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Read the dataset (adjust the URL as needed) acid_data &lt;- read.csv(&quot;data/acid_rain.csv&quot;) # View the first few rows #head(acid_data) # Nonparametric One-Sample Wilcoxon Signed-Rank Test on &#39;Before&#39; pH values # Test if the median pH before intervention is 5.5 wilcox_test_before &lt;- wilcox.test(acid_data$Before, mu = 5.5, alternative = &quot;two.sided&quot;) print(wilcox_test_before) ## ## Wilcoxon signed rank exact test ## ## data: acid_data$Before ## V = 67, p-value = 0.7197 ## alternative hypothesis: true location is not equal to 5.5 p-value is above 0.05 therefore the null hypothesis is accepted where we conclude that the median pH before the intervention is 5.5. ________________________________________________________________________________ 2.3.2 Nonparametric two-sample paired sign test and paired Wilcoxon Signed Rank Test The Wilcoxon Rank Sum Test (also known as the Mann–Whitney U test or the Wilcoxon–Mann–Whitney test) is a nonparametric procedure that tests for a difference between two population medians. The test is used to determine whether one population consistently produces larger or smaller measurements than another, assuming that the two populations have approximately equal variance. Here are the assumptions before carrying out the test; The two samples are independent. The data distributions of the two populations are similar in shape (same variance), though not necessarily the same mean or median. The test does not assume a specific distribution (e.g., normal, lognormal, gamma, etc.). The sample size for each group should preferably be at least 20, but meaningful results can still be obtained with samples as small as 10 using refined algorithms. Follow the steps below to carry out the tests; Combine the two samples into a single data set and rank the observations in ascending order. Compute the Wilcoxon test statistic as the sum of the ranks for one of the groups (usually the smaller group). If each sample size is , obtain the critical value from statistical tables. If each sample size is , compute the test statistic’s standardized form and compare it to the normal distribution. Compute the p-value from statistical tables or using functions in R such as pnorm(). If tied values exist; the average rank is assigned to all tied values. The presence of ties may require an adjustment to the test statistic calculation. If there exist Non-Detects (NDs); If all NDs share a single reporting limit, they can be treated as tied values. If multiple reporting limits exist, alternative tests such as the Gehan test are preferred. Try it! Lets test between two groups # Sample data group1 &lt;- c(3, 5, 9, 10, 15) group2 &lt;- c(2, 4, 6, 8, 12) # Perform Wilcoxon Rank Sum Test wilcox.test(group1, group2, alternative = &quot;two.sided&quot;) ## ## Wilcoxon rank sum exact test ## ## data: group1 and group2 ## W = 16, p-value = 0.5476 ## alternative hypothesis: true location shift is not equal to 0 Exact testing using wilcox.exact() install the exactRankTests install.packages(&quot;exactRankTests&quot;) Run the test library(exactRankTests) ## Package &#39;exactRankTests&#39; is no longer under development. ## Please consider using package &#39;coin&#39; instead. wilcox.exact(group1, group2, alternative = &quot;two.sided&quot;) ## ## Exact Wilcoxon rank sum test ## ## data: group1 and group2 ## W = 16, p-value = 0.5476 ## alternative hypothesis: true mu is not equal to 0 Using coin package for more complex cases. First install the package install.packages(\"coin\") Run the test library(coin) ## ## Attaching package: &#39;coin&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## alpha ## The following objects are masked from &#39;package:exactRankTests&#39;: ## ## dperm, pperm, qperm, rperm # wilcox_test(group1 ~ group2) The Wilcoxon Rank Sum Test is a useful nonparametric alternative to the two-sample t-test when normality cannot be assumed. It is robust to outliers and does not require knowledge of the underlying data distribution. R provides multiple functions for computing the test, with options for handling ties and computing exact p-values. Practical Exercise Using the “acid rain” above determine if there is a consistent direction of change in pH values (i.e., whether more stations show an increase or decrease in pH after the intervention). Use the non-parametric paired sign test. Repeat what you have done with Nonparametric Paired Wilcoxon Signed-Rank Test. However, in this case, test whether there is a statistically significant difference between the pH values before and after the intervention. _______________________________________________________________________ Solution Project Objective: Is there a consistent direction of change in pH values (i.e., whether more stations show an increase or decrease in pH after the intervention). Hypotheses Null Hypothesis: There is no difference in the number of stations with increased versus decreased pH. Alternate Hypothesis: There is a significant difference in the number of stations showing an increase compared to a decrease. Methods For each station, calculate: $$ sign(d_i) = \\[\\begin{cases} 1, &amp; \\text{if } After - Before &gt; 0 \\\\ 0, &amp; \\text{if } After - Before = 0 \\\\ -1, &amp; \\text{if } After - Before &lt; 0 \\end{cases}\\] $$ Count the number of positive differences and use a binomial test to see if it deviates from the expected 50% chance. # Compute differences between After and Before pH values acid_data &lt;- acid_data %&gt;% mutate(Diff = After - Before) # Count number of positive changes (excluding ties) positive_changes &lt;- sum(acid_data$Diff &gt; 0) total_nonzero &lt;- sum(acid_data$Diff != 0) # Perform a binomial test assuming a 50% chance of an increase sign_test &lt;- binom.test(positive_changes, total_nonzero, p = 0.5) print(sign_test) ## ## Exact binomial test ## ## data: positive_changes and total_nonzero ## number of successes = 10, number of trials = 15, p-value = 0.3018 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.3838037 0.8817589 ## sample estimates: ## probability of success ## 0.6666667 p-value is above 0.05 therefore the null hypothesis is accepted. We will conclude that there is no difference in the number of stations with increased versus decreased pH. Lets repeat the same for Nonparametric Paired Wilcoxon Signed-Rank Test Project Objective : Test whether there is a statistically significant difference between the pH values before and after the intervention. Hypotheses: Null hypothesis: The median difference between paired pH values (After - Before) is zero. Alternate hypothesis: The median difference is not zero. # Paired Wilcoxon Signed-Rank Test paired_wilcox &lt;- wilcox.test(acid_data$Before, acid_data$After, paired = TRUE, alternative = &quot;two.sided&quot;) print(paired_wilcox) ## ## Wilcoxon signed rank exact test ## ## data: acid_data$Before and acid_data$After ## V = 53, p-value = 0.7197 ## alternative hypothesis: true location shift is not equal to 0 The p-value is above -.-5 therefore there is no significant change in pH after the intervention. ________________________________________________________________________________ 2.4 One-Way ANOVA ANOVA (Analysis of Variance) is a statistical method used to compare three or more groups to determine whether their means or medians significantly differ, or if they belong to the same population. It is particularly useful in scenarios where multiple groups need to be analyzed simultaneously, reducing the risk of errors that arise from multiple two-sample tests. ANOVA can be categorized into parametric (assuming normality and equal variances) and nonparametric (distribution-free) methods. Variants include One-Way ANOVA (influenced by a single factor) and Two-Way or Multifactor ANOVA (considering multiple factors). While ANOVA helps identify overall differences, post hoc tests are often needed to determine which groups specifically differ In this analysis we will focus on One-Way ANOVA, the parametric and non-parametric one. 2.4.1 Parametric One-Way ANOVA One-Way ANOVA is a parametric statistical test used to determine whether there are significant differences among the means of three or more independent groups. It is commonly applied in various fields, including ecology, to analyze how a single factor influences a dependent variable, such as plant growth across different soil types. Here are the assumptions that must be met before conducting One-Way ANOVA Normality – Each group’s data should follow a normal distribution. This can be tested using the Shapiro-Wilk test or Kolmogorov-Smirnov test. Homogeneity of Variance – The variance across groups should be approximately equal, verified using Levene’s test or Bartlett’s test. Independence – Observations should be independent, meaning one measurement should not influence another. If these assumptions are violated, alternatives such as Welch’s ANOVA (for unequal variances) or Kruskal-Wallis test (non-parametric alternative) should be considered. This is how One-Way ANOVA works; It compares - Between-Group Variance – Measures differences among the group means. Within-Group Variance – Measures variability within each group. A high F-statistic suggests that at least one group mean differs significantly. The test’s p-value is then used to determine statistical significance (typically, if p &lt; 0.05, the null hypothesis is rejected). One-Way ANOVA is important in Ecology as it can be used to find; - Differences in plant species diversity across multiple forest regions. - Variations in water pH levels among different lakes. - The effect of pollution levels on fish populations across multiple rivers. For example, an ecologist studying wetland ecosystems might apply One-Way ANOVA to determine whether soil nitrogen levels significantly affect plant biodiversity across different wetland sites. If the test finds a significant difference, post hoc tests like Tukey’s HSD can identify which specific wetland sites differ. By using One-Way ANOVA, ecologists and researchers can make informed, data-driven decisions about conservation efforts and environmental management strategies. Try it! In our case, we have have generated the sample data for three groups; group1, group2 and group3. Formulating the null hypothesis; Null Hypothesis: The mean is the same across all gropu populations. Alternate hypothesis: At least one group population has a different mean from the rest of the group. When modelling the parametric one-way ANOVA, the F-statistic is computed as; \\[{F} = {{\\text Between-Groups Variance(MSG)}\\over{\\text Within-Group Variance(MSEE)}}\\] Where; \\(MSG = {{SSBG}\\over{k-1}}\\), the \\(SSBG\\) is the sum of squares. \\(MSE = {{SSWG}\\over{N-k}}\\), the \\(SSWG\\) is the between group sum of squares. \\(k\\) as the number of groups and \\(N\\) as the total number of observaions. # Load necessary library library(dplyr) library(ggplot2) # Generate sample data set.seed(123) group1 &lt;- rnorm(10, mean = 50, sd = 10) group2 &lt;- rnorm(10, mean = 55, sd = 10) group3 &lt;- rnorm(10, mean = 60, sd = 10) data &lt;- data.frame( values = c(group1, group2, group3), group = rep(c(&quot;Group1&quot;, &quot;Group2&quot;, &quot;Group3&quot;), each = 10) ) # Compute group means and overall mean means &lt;- data %&gt;% group_by(group) %&gt;% summarise(mean_value = mean(values)) overall_mean &lt;- mean(data$values) # Compute sum of squares SSBG &lt;- sum(10 * (means$mean_value - overall_mean)^2) # Between-group sum of squares SSWG &lt;- sum((data$values - ave(data$values, data$group, FUN = mean))^2) # Within-group sum of squares SSTotal &lt;- sum((data$values - overall_mean)^2) # Total sum of squares df_SSBG &lt;- length(unique(data$group)) - 1 df_SSWG &lt;- nrow(data) - length(unique(data$group)) df_SSTotal &lt;- nrow(data) - 1 # Compute mean squares MSG &lt;- SSBG / df_SSBG MSE &lt;- SSWG / df_SSWG # Compute F-statistic F0 &lt;- MSG / MSE # Compute critical F value alpha &lt;- 0.05 F_critical &lt;- qf(1 - alpha, df_SSBG, df_SSWG) # Compute p-value p_value &lt;- 1 - pf(F0, df_SSBG, df_SSWG) # Display results cat(&quot;Between-group sum of squares (SSBG):&quot;, SSBG, &quot;\\n&quot;) ## Between-group sum of squares (SSBG): 223.5015 cat(&quot;Within-group sum of squares (SSWG):&quot;, SSWG, &quot;\\n&quot;) ## Within-group sum of squares (SSWG): 2568.336 cat(&quot;Total sum of squares (SSTotal):&quot;, SSTotal, &quot;\\n&quot;) ## Total sum of squares (SSTotal): 2791.837 cat(&quot;F-statistic (Fo):&quot;, F0, &quot;\\n&quot;) ## F-statistic (Fo): 1.174796 cat(&quot;Critical F value (Fc):&quot;, F_critical, &quot;\\n&quot;) ## Critical F value (Fc): 3.354131 cat(&quot;p-value:&quot;, p_value, &quot;\\n&quot;) ## p-value: 0.3241775 # Perform ANOVA using built-in function anova_result &lt;- aov(values ~ group, data = data) summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 223.5 111.75 1.175 0.324 ## Residuals 27 2568.3 95.12 Practical Exercise Air quality is an important ecological and public health concern. For example, understanding whether Nitrogen Dioxide (\\(NO_2\\)) concentrations differ significantly across different states can inform regulatory decisions and resource management. In this exercise, you will use the “Air Quality Data in India” dataset, which is publicly available on Kaggle, to test the following: You are required to use the Parametric One-Way ANOVA to compare the means of the pollutant levels across different groups. Solution # Load the required libraries library(dplyr) # Load the data air_data &lt;- read.csv(&quot;data/AirQualityDataIndia.csv&quot;) # Perform one-way ANOVA anova_result &lt;- aov(no2 ~ state, data = air_data) summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## state 33 53343341 1616465 7511 &lt;2e-16 *** ## Residuals 419475 90281183 215 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 16233 observations deleted due to missingness Although there were missing values in the data set, The F-statistic was 7511 and 33 degrees of freedom. The p-value was far much less than 0.05 therefore there is enough evidence to reject the null hypothesis. We conclude that atleast one state had a different mean concentration of Nitrogen dioxide from the rest of the states ________________________________________________________________________________ 2.4.2 Nonparametric One-Way ANOVA (Kruskal-Wallis Test) When conducting a parametric one-way ANOVA, one of the key assumptions is that the residuals follow a normal distribution. If this assumption is violated, a common approach is to apply a logarithmic transformation—using either the natural logarithm (\\(ln\\)) or the base-10 logarithm (log10)—to the data. If this transformation successfully normalizes the data and satisfies the assumption of homogeneity of variances, then the ANOVA can be performed on the transformed values. However, if both the original and log-transformed data fail to meet the assumptions required for parametric ANOVA, an alternative nonparametric test, the Kruskal–Wallis test, can be used. Unlike parametric ANOVA, the Kruskal–Wallis test does not assume normality but does require that the distributions of the populations have similar shapes, particularly regarding skewness and variance. This test also assumes that the data points are independent and do not exhibit trends. If the presence of a trend affects the data, one approach is to use the Friedman test, which accounts for structured variability by analyzing data within blocks. Another option for handling seasonal or cyclical trends is deseasonalization before applying the Kruskal–Wallis test. Kruskal–Wallis Test Procedure The Kruskal–Wallis test examines differences in the medians of multiple groups rather than their means. It is performed by ranking all data values across groups and using the calculating a test statistic: This statistic follows a chi-square() distribution with degrees of freedom, where is the number of groups. If exceeds the critical value at a chosen significance level(), or if the p-value is below 0.05, the null hypothesis of equal medians is rejected. Try it! Ecological Example: Testing Differences in Soil Nutrient Levels Consider a study where researchers measure soil nitrogen levels in three different ecosystems: grassland, wetland, and forest. Since soil nutrient data often exhibit skewness, the Kruskal–Wallis test is a suitable choice. # Load necessary library library(dplyr) # Simulated soil nitrogen data (in mg/kg) set.seed(42) grassland &lt;- rnorm(10, mean = 30, sd = 5) wetland &lt;- rnorm(10, mean = 40, sd = 5) forest &lt;- rnorm(10, mean = 35, sd = 5) # Combine into a data frame data &lt;- data.frame( Nitrogen = c(grassland, wetland, forest), Ecosystem = rep(c(&quot;Grassland&quot;, &quot;Wetland&quot;, &quot;Forest&quot;), each = 10) ) # Perform Kruskal-Wallis test kruskal.test(Nitrogen ~ Ecosystem, data = data) ## ## Kruskal-Wallis rank sum test ## ## data: Nitrogen by Ecosystem ## Kruskal-Wallis chi-squared = 4.2039, df = 2, p-value = 0.1222 Since the p-value (0.1222) is greater than 0.05, we fail to reject the null hypothesis. This suggests that there is no statistically significant difference in median soil nitrogen levels among the three ecosystems. If a lower p-value had been obtained, further analysis using post hoc pairwise comparisons would be required to determine which specific ecosystems differ. Practical Exercise Using the air quality data in India from the previous exercise, perform non-parametric one-way ANOVA(Kruskal-Wallis Test) to compare the medians when data assumptions for ANOVA might not hold. Solution Formulate the hypothesis; Null hypothesis: The median Nitrogen dioxide concentration is the same across all states. Alternate hypothesis: At least one state has a median Nitrogen dioxide concentration that is different. # Perform the Kruskal-Wallis test kruskal_result &lt;- kruskal.test(no2 ~ state, data = air_data) print(kruskal_result) ## ## Kruskal-Wallis rank sum test ## ## data: no2 by state ## Kruskal-Wallis chi-squared = 166893, df = 33, p-value &lt; 2.2e-16 The p-value is far much below 0.05 therefore the null hypothesis is rejected. It is concluded that at least one state has a median Nitrogen dioxide contration that is different. ________________________________________________________________________________ 2.5 Hands-on Exercise Solution ________________________________________________________________________________ "],["test-for-autocorrelation-and-temporal-effects.html", "Chapter 3 TEST FOR AUTOCORRELATION AND TEMPORAL EFFECTS 3.1 Test for Autocorrelation Using the Sample Autocorrelation Function 3.2 An Example on Site-Wide Temporal Effects 3.3 TESTS FOR TREND", " Chapter 3 TEST FOR AUTOCORRELATION AND TEMPORAL EFFECTS Environment Data is not always stable or stationary over time or space and is frequently subjected to sustained or cyclical change to the affecting factors. These concepts will help us understand the temporal effects better; Temporal Trends: refer to changes in a variable or phenomenon over time. Consider monitoring soil nitrogen levels in agricultural field over several years. There might be temporal rise of nitrogen immediately after application of nitrogen-based fertilizer and return back to almost the average state after time. Although there might be a general increase in soil nitrogen levels, but they won’t match the immediate spikes after fertilizer application. Autocorrelation or serial correlation: is the correlation between data values of the same variable. Positive autocorrelation means that values close in time/space tend to be similar while negative autocorrelation means that they tend to differ. Monotonic trend: this is a trend that is in one direction, either constantly upward or downwards. A good example are the levels of carbon dioxide(CO2) concentrations in the atmosphere, they have been consistently rising due to industrial activities and deforestation. Cyclic or Seasonal trend refer to patterns that repeat at regular intervals such as daily ,hourly or annually. These trends are usually affected by natural cycles, human activities or climatic conditions. For instance, these trends are evident in phosphorous levels in water bodies that might increase during rainy seasons due agricultural runoff and decline during dry seasons. When testing for autocorrelation and temporal effects the data samples are assumed to have values that are independent or uncorrelated, and identically or similarly distributed. The data are said to be autocorrelated or serially correlated when this assumption is violated. These are the tests that are used to check for autocorrelation: Sample Autocorrelation Function(ACF): used for a single time series where data is normally distributed. Rank von Neumann ratio test used when the data samples evidence of nonnormality. The complete block design ANOVA or Friedman test is used for multiple different data samples for instance testing the concentration of ground water contaminant from multiple monitoring wells at a site. 3.1 Test for Autocorrelation Using the Sample Autocorrelation Function Before testing for autocorrelation using the ACF method, the data sample should be; normally distributed. stationary(that is, not trending). free of outliers. containing atleast 10 to 12 observations. … And here is how ACf is computed. Arrange the data in lagged data pairs (\\(x_{i}, x_{i + k}\\)), for \\(i\\) = 1, 2, ….(\\(n-k\\)), where; \\(n\\) is the size of the data samples(number of data values therein). \\(k\\) is the lag. i.e number of sampling events dates separating one data value in the pair from the second value. For instance a lag of 1 shows that the two value pairs were collected in consecutive time intervals(e.g days) while lag shows that the data value pairs were collected every each time interval Calculate the sample Autocorrelation coefficient \\[r_k = {{\\sum^{n-k}_{i=1}(x_i-x^!)(x_{i+k}- x^!)}\\over{\\sum^n_{i=1}(x_i - x^!)^2}}\\] where; \\(n\\) is the data sample size. \\(k\\) is the lag \\(x^!\\) is the data sample mean \\(x_{i}, x_{i + k}\\) are the components of the data pairs formed based on the lag. If \\(k\\) = 1, the sample autocorrelation coefﬁcient, \\(r_1\\) is referred to as the ﬁrst order sample autocorrelation coefﬁcient. If \\(k\\) = 2, \\(r_2\\) is the second order coefﬁcient, and so on. Lets breakdown the possible results of autocorrelation; \\(r_0\\) = 1: Autocorrelation at lag 0 is always one because a value is perfectly correlated with itself. Random Data: if the data is random, most autocorrelation coefficients will be close to zero, and will decrease further as the lag increases. Autocorrelated Data: If there’s autocorrelation, some coefficients(\\(r_k\\)) will be significantly larger than zero, but their strength decreases with higher lags. Trend in data: If there is a trend, coefficients wont diminish, showing persistent correlation. As a summary, the normally distributed data, \\(r_k\\) should be close to zero if there is no correlation. At a 95% confidence level, autocorrelation is insignificant if no \\(r_k\\) exceeds the threshold \\(2\\over{\\sqrt{n}}\\). There result of each autocorrelation are therefore plotted in a correlogram and the confidence limits are show as the horizontal lines. Try it! Climate and weather data are central to ecological research, helping us understand environmental trends and variability. In this exercise, you’ll examine whether daily temperature data from Melbourne exhibit autocorrelation over time. Autocorrelation indicates that today’s temperature might be correlated with yesterday’s, the day before, and so on. Detecting significant autocorrelation is important because it affects how we model and predict future climate patterns. You are required to retrieve the daily Temperature of Melbourne and save it as a csv file. Formulate the hypothesis; Null hypothesis: The daily minimum temperature series is random(i.e there is no significant autocorrelation) Alternate hypothesis: The daily temperature time series exhibits significant autocorrelation (i.e., past values influence future values). Perform the analysis with R # Load necessary libraries library(readr) library(ggplot2) # Load the data set melbourne_temp &lt;- read_csv(&quot;data/melboune-daily-temperature.csv&quot;) ## Warning: One or more parsing issues, call `problems()` on ## your data frame for details, e.g.: ## dat &lt;- vroom(...) ## problems(dat) ## Rows: 3651 Columns: 2 ## ── Column specification ───────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Date, Daily minimum temperatures in Melbourne, Australia, 1981-1990 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Inspect the data structure str(melbourne_temp) ## spc_tbl_ [3,651 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Date : chr [1:3651] &quot;1981-01-01&quot; &quot;1981-01-02&quot; &quot;1981-01-03&quot; &quot;1981-01-04&quot; ... ## $ Daily minimum temperatures in Melbourne, Australia, 1981-1990: chr [1:3651] &quot;20.7&quot; &quot;17.9&quot; &quot;18.8&quot; &quot;14.6&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Date = col_character(), ## .. `Daily minimum temperatures in Melbourne, Australia, 1981-1990` = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Convert the &#39;Date&#39; column to Date format. melbourne_temp$Date &lt;- as.Date(melbourne_temp$Date, format = &quot;%Y-%m-%d&quot;) # Rename the temperature column names(melbourne_temp)[names(melbourne_temp) == &#39;Daily minimum temperatures in Melbourne, Australia, 1981-1990&#39;] &lt;- &#39;Temperature&#39; # Convert to numeric melbourne_temp &lt;- transform(melbourne_temp, Temperature = as.numeric(Temperature)) ## Warning in eval(substitute(list(...)), `_data`, parent.frame()): NAs introduced ## by coercion # Remove rows with missing values melbourne_temp &lt;- na.omit(melbourne_temp) # Sort data by date if not already sorted melbourne_temp &lt;- melbourne_temp[order(melbourne_temp$Date), ] # Create a time series object (assuming daily data; adjust the &#39;frequency&#39; parameter if needed) # Here, we assume the data is daily, so frequency = 365 temp_ts &lt;- ts(melbourne_temp$Temp, frequency = 365) # Plot the time series to visualize trends plot(temp_ts, main = &quot;Daily Temperature Time Series (Melbourne)&quot;, xlab = &quot;Time&quot;, ylab = &quot;Temperature (°C)&quot;) # Compute and plot the Sample Autocorrelation Function (ACF) acf(temp_ts, main = &quot;ACF of Melbourne Daily Temperature&quot;) The results show the time series of daily temperature in Melbourne and its corresponding autocorrelation function (ACF). There is a cyclic trends in temperature over time and the ACF plot suggests strong autocorrelation at shorter lags, indicating a temporal dependency in the data. 3.2 An Example on Site-Wide Temporal Effects Analysis of Site-Wide Temporal Effects on Groundwater Manganese Concentrations Generate the data # Load necessary libraries library(dplyr) # Set seed for reproducibility set.seed(42) # Generate synthetic manganese concentration data Mn_values &lt;- c(3.5, 2.8, 4.1, 3.9, 5.2, 4.8, 6.0, 5.7, # Well 1 3.2, 2.5, 3.8, 3.7, 5.0, 4.6, 5.8, 5.5, # Well 2 3.6, 3.0, 4.2, 4.0, 5.3, 4.9, 6.1, 5.8, # Well 3 3.3, 2.6, 3.9, 3.8, 5.1, 4.7, 5.9, 5.6) # Well 4 # Create a data frame Mn2Data8 &lt;- data.frame( Mn = Mn_values, Location = rep(c(&quot;Well1&quot;, &quot;Well2&quot;, &quot;Well3&quot;, &quot;Well4&quot;), each = 8), Time = rep(paste0(&quot;Q&quot;, 1:8), times = 4) ) # Convert factors Mn2Data8$Location &lt;- as.factor(Mn2Data8$Location) Mn2Data8$Time &lt;- as.factor(Mn2Data8$Time) # Display the first few rows head(Mn2Data8) ## Mn Location Time ## 1 3.5 Well1 Q1 ## 2 2.8 Well1 Q2 ## 3 4.1 Well1 Q3 ## 4 3.9 Well1 Q4 ## 5 5.2 Well1 Q5 ## 6 4.8 Well1 Q6 Groundwater manganese concentrations have been monitored quarterly across four wells for a period of two years (eight quarters). The data set above, 2009), includes measurements from each well per quarter. The goal is to determine whether there is a site-wide temporal trend in manganese concentrations over time. To establish whether a time trend exists, we must first check for spatial variability among the four wells. If the manganese concentrations do not significantly differ across wells, we can then assess whether concentrations change over time. Here is the process that will be followed; Check for Spatial Variability (Homogeneity) by; Testing whether the means or medians of the four well datasets are statistically equal. Checking If the datasets show normality and homogeneity of variances, a two-way ANOVA without replication (Complete Block Design) is appropriate. Otherwise, the non-parametric Friedman test is used. Check for Temporal Trends by; Checking for a significant trend over time if no spatial variability is found. A two-way ANOVA is performed again, switching the roles of Time and Location where initially Time was the factor of interest and Location as a blocking factor. The ANOVA model follows the equation: \\[{Y_{ij}}={\\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}}\\] Where: \\(Y_{ij}\\) is the Manganese concentration \\(\\mu\\) is the overall mean \\(\\alpha_i\\) represents the effect of the well (location) \\(\\beta_j\\) represents the effect of time (quarter) \\(\\epsilon_{ij}\\) is the error term The hypothesis tests: For spatial variability: \\(H_0:\\mu_1=\\mu_2=\\mu_3=\\mu_4\\) (no significant difference between wells) For temporal trend \\(H_0:\\mu_{Q1}=\\mu_{Q2}=...=\\mu_{Q8}\\)(no significant difference across quarters) Check for variability # Step 1: Check for Spatial Variability Mn2.anova1 &lt;- aov(Mn ~ Location + Time, data = Mn2Data8) summary(Mn2.anova1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Location 3 0.57 0.189 125.5 1.44e-13 *** ## Time 7 35.63 5.091 3387.1 &lt; 2e-16 *** ## Residuals 21 0.03 0.002 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Results; Location: p_value(denoted as Pr(&gt;F) on the table above) is less than 0.05 - therefore there is a significant spatial variability Time: p_value is less than 0.05 - there is a siginfant temporal trend. If there was no spatial variability we could have proceed to test the time trend. However, lets just proceed to show you how it is done. Check the code below # Step 2: Check for Temporal Effects Mn2.anova2 &lt;- aov(Mn ~ Time + Location, data = Mn2Data8) summary(Mn2.anova2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Time 7 35.63 5.091 3387.1 &lt; 2e-16 *** ## Location 3 0.57 0.189 125.5 1.44e-13 *** ## Residuals 21 0.03 0.002 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 P-value is far much less than 0.05. The results confirm that manganese concentrations vary significantly over time. 3.3 TESTS FOR TREND When working with data that has a time component (i.e., data collected at different points in time), a time series plot is a useful first step in exploring whether the values are randomly distributed or if a trend or pattern exists. If the plot suggests a trend, the next step is to determine whether this trend is statistically significant or just a random occurrence. To assess this, we can use parametric or nonparametric methods, depending on whether the data meets certain assumptions such as normality, absence of outliers, and constant variance. Parametric methods assume that the data follows a specific distribution (usually normal). These methods rely on parameters like mean and variance. A common parametric approach for trend detection is simple linear regression. Nonparametric methods do not require the data to follow any particular distribution, making them more flexible. Two widely used nonparametric trend tests are: The Mann-Kendall test, which determines whether a trend exists but does not measure its magnitude. The Theil-Sen method, which not only detects trends but also estimates the slope (rate of change) of the trend line. In some cases, external factors like temperature or streamflow rate can influence the trend results. If these factors are not accounted for, they may lead to incorrect conclusions. Therefore, it is important to adjust for such variables before performing trend analysis. 3.3.1 Parametric Test for Trends - Simple Linear Regression Simple linear regression is a commonly used parametric method for detecting trends over time. It involves plotting data against time and fitting a best-fit regression line to model the relationship. The equation for this regression line is: \\[Y_t = \\beta_0 + \\beta_1t + \\epsilon_t\\] Where: \\(Y_t\\) represents the observed data value at time \\(t\\)(e.g., pollutant concentration, species population, or temperature). \\(\\beta_0\\) is the intercept (initial value at \\(t=0\\)) \\(\\beta_1\\) is the slope coefficient, indicating the rate of change over time. \\(\\epsilon_t\\) represents random errors. A statistically significant positive slope(\\(\\beta_1&gt;0\\)) suggests an increasing trend, while a negative slope(\\(\\beta_1&lt;0\\)) indicates a declining trend. To determine if the trend is significant, we set up the following hypotheses: Null hypothesis(\\(H_0\\)): There is no trend; the true slope is zero (\\(\\beta_1=0\\)). Alternative Hypothesis (\\(H_a\\)): A trend exists; the slope is not zero (\\(\\beta_1≠0\\)). The t-test is used to check the significance of \\(\\beta_1\\): \\[{t} = {{\\hat\\beta_1}\\over{SE(\\hat\\beta_1)}}\\] Where \\(\\hat\\beta_1\\) is the setimated slope and \\(SE(\\hat\\beta_1)\\) is its standard error. The p-value associated with this test determines whether we reject \\(H_0\\) at a given significance level \\(\\alpha\\) (typically 0.05) Here are the assumptions must be satisfied for valid results; Linearity: The trend should be approximately linear. Independence : Observations should be independent over time. Normality of Residuals: The residuals (differences between actual and predicted values) should follow a normal distribution. Homoscedacity: Variance of residuals should be constant over time. Sufficient Data Points: At least 8–10 observations are recommended. Try it!: Detecting Trends in River Nitrate Levels In ecological studies, trend analysis is often applied to environmental parameters such as nitrate concentration in rivers over time. The dataset below simulates nitrate levels (mg/L) measured monthly over several years. # Load necessary libraries library(ggplot2) library(lmtest) ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric # Simulated ecological data: Nitrate levels in a river over time set.seed(42) time &lt;- 1:120 # 10 years of monthly data nitrate_levels &lt;- 2 + 0.01 * time + rnorm(120, mean = 0, sd = 0.2) # Simulated increasing trend # Create a data frame data &lt;- data.frame(time = time, nitrate = nitrate_levels) # Fit linear regression model model &lt;- lm(nitrate ~ time, data = data) # Summary of the regression summary(model) ## ## Call: ## lm(formula = nitrate ~ time, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.60476 -0.12832 0.01333 0.12895 0.54269 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0145997 0.0382374 52.69 &lt;2e-16 *** ## time 0.0098567 0.0005485 17.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2081 on 118 degrees of freedom ## Multiple R-squared: 0.7324, Adjusted R-squared: 0.7301 ## F-statistic: 323 on 1 and 118 DF, p-value: &lt; 2.2e-16 # Extract p-value and slope slope &lt;- coef(model)[2] p_value &lt;- summary(model)$coefficients[2,4] # Decision based on p-value alpha &lt;- 0.05 if (p_value &lt; alpha) { print(paste(&quot;Significant trend detected with slope:&quot;, round(slope, 4))) } else { print(&quot;No significant trend detected.&quot;) } ## [1] &quot;Significant trend detected with slope: 0.0099&quot; # Plot the trend with regression line ggplot(data, aes(x = time, y = nitrate)) + geom_point(color = &quot;blue&quot;, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(title = &quot;Trend Analysis of Nitrate Levels in River&quot;, x = &quot;Time (Months)&quot;, y = &quot;Nitrate Concentration (mg/L)&quot;) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Here is how we interpret the regression output: Intercept\\((\\beta_0)\\) = 2.0146 (baseline concentration) Slope\\(\\beta_1\\) = 0.0099 (indicates an increasing trend in nitrate levels). p-value&lt;0.001 (significant trend detected). R-squared = 0.7324, meaning 73.2% of the variability in nitrate concentration is explained by time. This suggests that nitrate levels are increasing over time, which could indicate environmental pollution or agricultural runoff. Practical Exercise Using the Melbourne daily temperature(Used in Sample Autocorrelation Coeefficient Exercise), determine whether there is a statistically significant trend in temperature over time. Solution Formulate the hypotheses; Null hypothesis: There is no significant trend in temperature over time. Alternate hypothesis: There is a significant increasing or decreasing trend in temperature over time. Here is the simple linear regression model; \\[Temperatur = \\beta_0 + \\beta_1.Time + \\epsilon\\] where \\(\\beta_1\\) is the slope coefficient. # data set - melbourne_temp(reuse the variable name from the previous exercise) # Calculate time difference since the beginning melbourne_temp$Time &lt;- as.numeric(as.Date(melbourne_temp$Date) - min(as.Date(melbourne_temp$Date))) # Convert Date to numerical # Visualize the time series ggplot(melbourne_temp, aes(x = Date, y = Temperature)) + geom_line() + labs(title = &quot;Daily Temperature Time Series (Melbourne)&quot;, y = &quot;Temperature (°C)&quot;, x = &quot;Date&quot;) # Parametric Test for Trend: Linear Regression model &lt;- lm(Temperature ~ Time, data = melbourne_temp) summary(model) # Get regression output ## ## Call: ## lm(formula = Temperature ~ Time, data = melbourne_temp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.1518 -2.8964 -0.1497 2.7674 15.1686 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.112e+01 1.345e-01 82.628 &lt;2e-16 *** ## Time 3.904e-05 6.380e-05 0.612 0.541 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.062 on 3645 degrees of freedom ## Multiple R-squared: 0.0001028, Adjusted R-squared: -0.0001716 ## F-statistic: 0.3746 on 1 and 3645 DF, p-value: 0.5406 # Plot regression line ggplot(melbourne_temp, aes(x = Time, y = Temperature)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + labs(title = &quot;Linear Trend in Melbourne&#39;s Temperature&quot;, x = &quot;Time&quot;, y = &quot;Temperature (°C)&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The p-value(statistical significance) is 0.5406 which above 0.05, therefore the null hypothesis(There is no significant trend in temperature over time) is accepted. Furthermore the slope coefficient(\\(\\beta_1\\)) is \\(3.904 \\times 10^{-5}\\)(which is close to zero) suggests that temperature does not change significantly over time. We can conclude that there is no change in temperature over time. ________________________________________________________________________________ 3.3.2 Nonparametric Test for Trends 3.3.2.1 Mann–Kendall Test and Seasonal Mann–Kendall Test In ecological statistics, detecting trends in environmental data is crucial for understanding changes over time. However, traditional parametric tests such as linear regression require data to meet assumptions like normality and homoscedasticity, which are often violated in ecological datasets. Nonparametric tests like the Mann-Kendall (MK) test and Seasonal Mann-Kendall (SMK) test offer robust alternatives that do not assume any specific data distribution. 3.3.2.1.1 Mann-Kendall Trend Test The Mann-Kendall test is a rank-based nonparametric test used to detect a monotonic trend in time-series data, whether linear or nonlinear. Given a data set $ X = {x_1, x_2, …, x_n} $, the Mann-Kendall test examines all possible pairs $ x_i, x_j $ where $ i &lt; j $, and assigns a score based on their relative magnitudes: \\[S = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\text{sgn}(x_j - x_i)\\] where the sign function is defined as: \\[\\text{sgn}(x_j - x_i) = \\begin{cases} +1, &amp; \\text{if } x_j &gt; x_i \\\\ 0, &amp; \\text{if } x_j = x_i \\\\ -1, &amp; \\text{if } x_j &lt; x_i \\end{cases}\\] A positive \\(S\\) indicates an increasing trend, while a negative \\(S\\) suggests a decreasing trend. For large samples (\\(n&gt;10\\)), the test statistic \\(S\\) follows an approximately normal distribution with mean 0 and variance: \\[{Var(S)} = {{n(n-1)(2n-5)- \\sum_t t(t-1)(2t+5)}\\over{18}}\\] Where t represents the number of tied ranks in the data set. The standardized test statistic (Z-score) is given by: \\[{Z} = \\begin{cases} {{s-1}\\over{Var(S)}}, \\text if \\space S&gt;0\\\\ 0, \\text if \\space S = 0\\\\ {{s-1}\\over{Var(S)}}, \\text if \\space S&lt;0 \\end{cases}\\] Lets formulate the hypothesis; Null hypothesis(\\(H_0\\)): No trend in the data. Alternate Hypothesis(\\(H_1\\)): Presence of a monotonic trend Decision Rule: Reject $H_0 if |Z|&gt;Z_{a/2} $ where \\(Z_{a/2}\\) is the critical value from the normal distribution for a given significance level(e.g, 1.96 for \\(\\alpha = 0.05\\)) Try it: Analysis for the monthly average temperature A package trend is needed for this practical and can be installed by; install.packages(&quot;trend&quot;) Run the analysis library(trend) # Example dataset: Monthly average temperature data &lt;- c(12.4, 12.8, 13.1, 12.9, 13.5, 13.7, 13.9, 14.0, 14.1, 14.5, 14.8, 15.0) time &lt;- seq(2000, 2011) # Perform Mann-Kendall Test mk_result &lt;- mk.test(data) print(mk_result) ## ## Mann-Kendall trend test ## ## data: data ## z = 4.3201, n = 12, p-value = 1.56e-05 ## alternative hypothesis: true S is not equal to 0 ## sample estimates: ## S varS tau ## 64.000000 212.666667 0.969697 3.3.2.1.2 Seasonal Mann-Kendall (SMK) Test Environmental data often exhibit seasonality, meaning the trend might be masked by cyclic variations. The Seasonal Mann-Kendall test adjusts for seasonality by performing the Mann-Kendall test on subsets of data corresponding to different seasons. Here is how to perform it; Partition data into seasonal subsets (e.g, monthly data grouped by month across years) Apply the Mann-Kendall test to each seasonal subset/. Sum the S-values from each season to get an overall trend statistic Try it! Monthly pollutant concentration across five years A package Kendall is required for this analysis and can be installed by; install.packages(&quot;Kendall&quot;) Run the analysis # Load the library library(Kendall) # Example dataset: Monthly pollutant concentrations across 5 years set.seed(123) dates &lt;- seq(as.Date(&quot;2010-01-01&quot;), as.Date(&quot;2014-12-01&quot;), by=&quot;month&quot;) data_values &lt;- rnorm(length(dates), mean=50, sd=5) seasonal_data &lt;- data.frame(Date=dates, Value=data_values) # Perform Seasonal Mann-Kendall Test #smk_result &lt;- SeasonalMannKendall(seasonal_data$Value ~ #as.POSIXlt(seasonal_data$Date)$mon + #as.POSIXlt(seasonal_data$Date)$year) #print(smk_result) Practical Exercise Using the Melbourne daily temperature data set, perform the following non-parametric tests to find if there is a significant trend in temperature over time; Mann-Kendall Test Seasonal Mann-Kendall Test Solution Mann-Kendall Test # Load the required library library(Kendall) # Perform the test mk_test &lt;- MannKendall(melbourne_temp$Temperature) print(mk_test) ## tau = 0.0121, 2-sided pvalue =0.27545 \\(\\tau\\) = 0.0121 is small thus suggesting a slight increasing trend, though very weak. A value closer to -ve or +ve 1 would indicate a strong trend. The p-value is 0.27545(which is above 0.05) therefore we fail to reject the null hypothesis. This means there is no significant trend detected in the Melboourne. Lets try working on the Seasonall Mann-Kendall Test. Seasonal Mann-Kendall Test # Install the package if not already installed # install.packages(&quot;trend&quot;) # Load the package library(trend) # Convert data to time series format melbourne_ts &lt;- ts(melbourne_temp$Temperature, frequency = 365) # Perform Seasonal Mann-Kendall Test smk_test &lt;- smk.test(melbourne_ts) print(smk_test) ## ## Seasonal Mann-Kendall trend test (Hirsch-Slack test) ## ## data: melbourne_ts ## z = 2.8042, p-value = 0.005044 ## alternative hypothesis: true S is not equal to 0 ## sample estimates: ## S varS ## 598.00 45324.67 Alternatively, you can use the Kendall package # install.packages(&quot;Kendall&quot;) library(Kendall) # Convert time into a time series object melbourne_ts &lt;- ts(melbourne_temp$Temperature, frequency = 365) # Perform Seasonal Mann-Kendall Test smk_test &lt;- SeasonalMannKendall(melbourne_ts) print(smk_test) ## tau = 0.0367, 2-sided pvalue =0.0049713 After taking a closer looking using the Seasonal Mann-Kendall test we find that; The test statistic (z) = 2.802 – a positive value that suggests an increasing trend in temperature. P-value of less than 0.05 – Meaning that we reject the null hypothesis. S (Mann-Kendall Statistic) = 598 – A positive S value further confirms an upward trend. It can be conclude that since p-value is low (p &lt; 0.05), we reject the null hypothesis and conclude that there is a significant increasing trend in Melbourne’s temperature over time. ________________________________________________________________________________ 3.3.2.2 Theil–Sen Trend Test When studying climate change, pollution levels, or species populations, scientists often need to determine whether a trend exists over time. The Theil-Sen estimator is a robust, nonparametric method for estimating trends, making it perfect for ecological data, where outliers and missing values (non-detects) are common! Theil-sen is preferred since; Robust to Outliers – Unlike least-squares regression, Theil-Sen takes the median of all slopes, making it resistant to extreme values. No Normality Assumptions – Works with non-normally distributed data (great for environmental datasets!). Handles Missing Data – Can be used with censored (non-detect) values using the cenken() function in R. try it! Analyzing Water Quality Trends In this exercise, we will analyze nitrate concentrations in a river over time to detect pollution trends. We will simulated data. Lets break it down step by step # Install required packages if not already installed #install.packages(&quot;NADA&quot;) # For dealing with non-detects #install.packages(&quot;tidyverse&quot;) # For data manipulation &amp; visualization # Load libraries library(NADA) ## ## Attaching package: &#39;NADA&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## cor library(tidyverse) ## ── Attaching core tidyverse packages ──────────── ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ lubridate 1.9.4 ✔ tibble 3.2.1 ## ✔ purrr 1.0.2 ✔ tidyr 1.3.1 ## ── Conflicts ─────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::select() masks MASS::select() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors Simulate Nitrate Concentration Data Imagine we are monitoring nitrate levels (mg/L) in a river over 10 years (2010-2019). Some measurements were below the detection limit (ND = non-detects). # Create a dataset of nitrate levels over time data &lt;- tibble( Year = 2010:2019, Nitrate = c(2.1, 2.5, 2.3, 2.7, 2.8, 3.0, 3.5, 3.8, NA, 4.2), # Some missing values Detection_Limit = c(NA, NA, NA, NA, NA, NA, NA, 0.5, 0.5, NA) # Limit for ND values ) # View data print(data) ## # A tibble: 10 × 3 ## Year Nitrate Detection_Limit ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 2.1 NA ## 2 2011 2.5 NA ## 3 2012 2.3 NA ## 4 2013 2.7 NA ## 5 2014 2.8 NA ## 6 2015 3 NA ## 7 2016 3.5 NA ## 8 2017 3.8 0.5 ## 9 2018 NA 0.5 ## 10 2019 4.2 NA Some values are missing (NA), and some are below the detection limit (ND). Perform the Theil-Sen Estimation To compute the trend, we use cenken(), which handles both detected and non-detect values. # Replace non-detects (NDs) with detection limits data$Nitrate[is.na(data$Nitrate)] &lt;- data$Detection_Limit[is.na(data$Nitrate)] # Apply Theil-Sen estimator #result &lt;- cenken(Year, Nitrate, data = data) # Print result #print(result) Lets visualize the trend # Extract slope and intercept slope &lt;- result$coefficients[2] intercept &lt;- result$coefficients[1] # Create trend line equation data &lt;- data %&gt;% mutate(Predicted_Nitrate = intercept + slope * Year) # Plot ggplot(data, aes(x = Year, y = Nitrate)) + geom_point(color = &quot;blue&quot;, size = 3) + # Actual data points geom_line(aes(y = Predicted_Nitrate), color = &quot;red&quot;, size = 1) + # Theil-Sen trend labs(title = &quot;Trend of Nitrate Levels in River (2010-2019)&quot;, subtitle = paste(&quot;Slope:&quot;, round(slope, 3), &quot;mg/L per year&quot;), x = &quot;Year&quot;, y = &quot;Nitrate Concentration (mg/L)&quot;) + theme_minimal() Practical Exercise Finally, lets repeat the analysis(Melbourne daily temperature performed in the exercises earlier). Thjis time you will be required to perform the Theil-Sen Slope Estimator. Solution # Perform the Theil-Sen Slope Estimator theil_sen &lt;- sens.slope(melbourne_temp$Temperature) print(theil_sen) ## ## Sen&#39;s slope ## ## data: melbourne_temp$Temperature ## z = 1.0906, n = 3647, p-value = 0.2754 ## alternative hypothesis: true z is not equal to 0 ## 95 percent confidence interval: ## -4.861449e-05 2.023472e-04 ## sample estimates: ## Sen&#39;s slope ## 6.736275e-05 The Theil-sen slope estimate = \\(6.736 \\times 10^{-5}\\) indicates a very small positive value that suggests a slight increasing trend. Contrary to the Seasonal Mann-Kendall test, the p-value = 0.2754(above 0.05) indicating that we fail to reject the null hypothesis and there is trend is not statistically significant. Taking a closer look, the Seasonal Mann-Kendall found a significant trend while Theil-Sen test failed to find, this discrepancy might be due to; Seasonal effects influencing the Mann-Kendall test but having less impact in the Theil-Sen slope. High variance in temperature data, reducing the power of the Theil-Sen test. ________________________________________________________________________________ "],["correlation-covariance-geostatistics.html", "Chapter 4 CORRELATION, COVARIANCE, GEOSTATISTICS 4.1 INTRODUCTION 4.2 CORRELATION AND COVARIANCE 4.3 COVARIANCE 4.4 GEOSTATISTICS", " Chapter 4 CORRELATION, COVARIANCE, GEOSTATISTICS 4.1 INTRODUCTION In this topic, we will focus on Bivariate correlation and multivariate correlation, that is, the correlation between two or multiple variables. Lets define correlation once again! Correlation is the degree if association of variables. Correlation coefficient: is the quantifier that indicates the association of variables whether whether one variable is increasing/decreasing and the other one is increasing/decreasing or if there is no association at all. The correlation coefficient always ranges from -1 to 1 where the closer it gets to +1 means a stronger positive correlation(both variables increases/decreases at a concert) while it gets closer to -1 means a stronger negative correlation where one variable increases and the other variable decreases. A correlation close to zero from any direction(\\(+_{ve}\\) or \\(-_{ve}\\)) shows that there is little to no correlation between variables. Multiple correlation is where a group of variables (usually referred to as the predictor or X variables) to jointly relate to another variable (usually referred to as the response or Y variable), Partial correlation is the correlation between any pair of variables, given the presence of the remaining variables. There are several methods to compute the correlation coefficient; Pearson’s correlation coefficient. - satisfies linearity, normality and other parametric assumptions test. Spearman’s correlation coefficient. - free from linearity, normality etc. conditions. Kendall’s correlation coefficient. - free from linearity, normality etc conditions. 4.2 CORRELATION AND COVARIANCE Remember, Correlation is the degree of association between two variables - and the strength and direction of this relationship is captured by the correlation coeeficient which quantifies how the variables are related. Correlation can be linear or non-linear where the former refers to the proportional changes in variables(e.g doubling of one variables results in doubling the other) while the later refers to the relationship being not proportional(e.g doubling resulting into triple or fourfold change of the other). While correlation does not imply causation, understanding correlations can provide valuable insights. For example, a correlation between soil pH and aluminum levels in groundwater might indicate that lower pH levels increase aluminum dissolution, even though pH itself is not the direct cause of aluminum contamination. Covariance, on the other hand, measures the degree to which two variables change together. Unlike the correlation coefficient, covariance lacks a standardized scale, making it harder to interpret. The correlation coefficient can be derived by dividing the covariance by the product of the standard deviations of the two variables. Application of correlation Correlation forms the foundation for many analyses, including regression, where relationships are used to predict one variable based on another. However, spurious correlations (associations without logical explanation) should be critically evaluated to avoid misleading conclusions. For instance, a correlation between the number of graduating students and basketball game wins is likely coincidental rather than meaningful. In Summary, correlation provides a normalized measure of association, covariance gives an absolute value of joint variability. Together, these metrics are crucial for understanding relationships between variables and guiding further analyses. Lets dive deep into exploring the correlation coefficient mentioned in the introduction. 4.2.1 Pearson’s Correlation Coefficient Pearson’s correlation coefficient, often referred to as Pearson’s r or the product-moment correlation, measures the strength and direction of a linear relationship between two variables. It works best for linear and monotonic relationships, where the association is consistently positive or negative. A high pearson correlation will have the data points when plotted on a scatter plot to align in a straight line and have all points easily predicted/approximated by a straight line(“regression line”). Pearson correlation is computed when the test or data satisfies the parametric assumptions, such that; Be from normally distributed populations. Have no significant outliers. Show consistent variance (homoscedasticity). This type of correlation is sensitive to non-linear data or data containing missing values(e.g non-detects). Under such cases alternatives like Spearman or Kendall coefficients are recommended. The Pearsons correlation coeefficient is: \\[r = {1\\over{n - 1}}{\\sum^n_{i=1}[{{x_i - \\overline{x}}\\over{s_x}}][{{y_i - \\overline{y}}\\over{s_y}}]}\\] Where; \\(x_i\\): the independent variable data value \\(\\overline x\\): the mean of independent data values \\(y_i\\): the dependent variable data value \\(\\overline y\\): the mean of the dependent data values \\(s_x\\): The standard deviation of the independent variable \\(s_y\\): the standard deviation of the dependent variable ALternatively, it can be calculated as; \\[r = {{\\sum(x_i - \\overline x)(y_i- \\overline y)}\\over{\\sqrt{\\sum{(x_i - \\overline x)^2(y_i - \\overline y)^2}}}}\\] Where; \\(x_i\\) and \\(y_i\\): the data values for the two variables \\(\\overline x\\) and \\(\\overline y\\): the mean values of the variables. If r’s magnitude is; 0.95 and above, this indicates a strong correlation. between 0.75 and below 0.95 , indicates a moderate correlation. below 0.5, therefore there is a weak correlation. Significance Testing for Pearson’s Correlation Coefficient When working with real-world ecological data like the release of acidity of agricultural farms near zero grazing structures, significance testing helps determine whether the observed correlation between two variables in a sample reflects a true correlation in the population or is simply due to chance. Here is a step by step process; Forumalate the null hypothesis Null hypothesis (\\(H_0\\)): There is no correlation between the two variables in the population (r = 0) Alternate Hypothesis(\\(H_a\\)): There is a strong correlation Compute the test statistic Test statistic (\\(t_r\\)) can be computed by; \\[t_r = {{r\\sqrt{n-2}}\\over{\\sqrt{1-r^2}}}\\] Where; \\(r\\): Sample correlation coefficient \\(n\\): Number of paired data points This formula adjusts \\(r\\) for the sample size and expresses how extreme the observed correlation is. Compute the degree of freedom The degree of freedom is calculated by; \\[df = n -2 \\] Determine the critical value, pvalue and make your decision —————————-**remember to put this into practice————————————————– Try it! Lets have a practice on an ecological problem. we’ll utilize the Forest Health and Ecological Diversity dataset, which offers a comprehensive collection of ecological and environmental measurements focused on tree characteristics and site conditions. This data set is publicly available on Kaggle. Download it from here. First lets perform a pearson correlation to measure the linear relationship between two continuous variables. In this case we will focus on the tree height(Tree_Height) versus Diameter at breast height(DBH). # Load the required libraries library(ggplot2) # Load the required data set forest_data &lt;- read.csv(&quot;data/forest_health/forest_health_data.csv&quot;) # Compute Pearson’s correlation coefficient pearson_corr &lt;- cor(forest_data$Tree_Height, forest_data$DBH, method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) print(pearson_corr) ## [1] -0.01355993 # Visualize correlation ggplot(forest_data, aes(x = DBH, y = Tree_Height)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;blue&quot;) + labs(title = &quot;Scatter Plot of Tree Height vs. DBH&quot;, x = &quot;DBH&quot;, y = &quot;Tree Height&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The Pearson’s correlation coefficient is nearly 0 and the scatter plot along with the regression line shows no sign of linear relationship. This shows that there is little to no relationship between Diameter at Breast height and height of the tree itself in the forest Practical Exercise Solution ________________________________________________________________________________ 4.2.2 Spearman’s and Kendall’s Correlaton Coeefficients In ecological studies, researchers often analyze relationships between variables that may not follow normal distributions or linear trends. Nonparametric correlation methods like Spearman’s Rho (\\(\\rho\\)) and Kendall’s Tau (\\(\\tau\\)) are particularly useful in such scenarios. These methods allow for the exploration of monotonic relationships—either linear or nonlinear—without relying on strict parametric assumptions. 4.2.2.1 Spearman’s Rho(\\(\\rho\\)) Spearman’s Rho is commonly used in ecology to measure how two variables, such as species abundance and habitat quality, vary together in a monotonic fashion. Key Features and Calculations Data Ranking: Rank the values of one variable (e.g., habitat quality) from smallest to largest. Repeat the ranking process for the second variable (e.g., species abundance). Rank Alignment: Rearrange the ranks of the second variable to match the order of the first variable’s values. These ranks replace the original data values in the formula used for Pearson’s correlation coefficient. This approach ensures that outliers, which can significantly distort parametric methods, have minimal impact. The test of significance for Spearman’s Rho follows the same process as that for Pearson’s coefficient, using the ranked data instead of raw values. While Spearman’s method is robust and flexible, it does not utilize the full magnitude of the data values, making it less precise than Pearson’s method when the assumptions for Pearson’s coefficient are satisfied 4.2.2.2 Kendall’s Tau (\\(\\tau\\)) Kendall’s Tau offers another rank-based approach to measure monotonic relationships and is particularly relevant for ecological data with small sample sizes or irregular distributions. Key Features and Calculations Here is how Kendall’s Tau is calculated; Sorting and Matching: Sort one variable (e.g., elevation) in ascending order while maintaining the alignment of its corresponding variable. Pairwise Comparisons: Compare successive pairs of data to identify whether the relationship is concordant (positive), discordant (negative), or tied. Concordant pairs are assigned a \\(a + 1\\), discordant pairs a \\(a - 1\\), and tied pairs are ignored. Coefficient Calculation: Compute \\(\\tau\\) as the difference between concordant and discordant pairs divided by the total number of possible pairs. Significance Testing For small data sets (e.g, \\(n &lt; 10\\)), compare \\(S\\) (i.e concordant and discordant pairs) to critical values. For larger data sets use a z-statistic with adjustments for ties to assess significance. Kendall’s \\(\\tau\\) could evaluate the relationship between water quality indicators(e.g dissolved oxygen) and fish diversity in a rover system. Advantages of Nonparametric Methods in Ecology Robust to Outliers: Nonparametric methods are well-suited for ecological data, which often include extreme values (e.g., rare species occurrences). No Distribution Assumptions: These methods are ideal for analyzing relationships in data sets that do not conform to normal distributions. Handles Ties: Midrank adjustments make these methods practical for ecological studies, where tied observations are common (e.g., identical temperature readings across sites). How does these non parametric tests compare with the Pearson Correlation in Ecology? Are best when the relationship between variables is monotonic but not linear (e.g., species richness and disturbance level). Pearson’s Correlation assumes normality and linearity, which may not hold for ecological data, such as seasonal variations in bird populations. Try it! Still on the forest and ecological diversity data we used in the example above we will now calculate the spearman’s and Kendall’s correlation correlation between tree height and and its elevation. Remember that Spearman’s and Kendall’s correlations measure monotonic relationships and are more robust to non-normal data. # Compute Spearman’s correlation spearman_corr &lt;- cor(forest_data$Tree_Height, forest_data$Elevation, method = &quot;spearman&quot;, use = &quot;complete.obs&quot;) print(spearman_corr) ## [1] -0.02439385 # Compute Kendall’s correlation kendall_corr &lt;- cor(forest_data$Tree_Height, forest_data$Elevation, method = &quot;kendall&quot;, use = &quot;complete.obs&quot;) print(kendall_corr) ## [1] -0.01692893 Both the Spearmans(\\(\\rho\\)) and Kendall’s(\\(\\tau\\)) correlations are very low indicating a weak monotonic relationship between the tree height and its elevation within the forest. Note: Pearson’s correlation shows linear relationships, while Spearman’s and Kendall’s are better for ranked data Finally, lets visualize a joint relationship(correlation, particularly Pearson’s) between all variables # Load the required libraries library(dplyr) library(ggcorrplot) # Compute correlation matrix cor_matrix &lt;- cor(forest_data %&gt;% select_if(is.numeric), use = &quot;complete.obs&quot;) # Plot heatmap ggcorrplot(cor_matrix, method = &quot;circle&quot;, lab = TRUE, colors = c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;)) From the chart, an increase in blue shows a stronger positive linear relationship between the matching variables while increase in red shows a stronger negative linear relationship between the matching variables. White dots shows little to no relationship. In the correlation plot(heatmap-like) it is evident that the variables have little linear relationship among each other(low correlation) Practical Exercise Solution ________________________________________________________________________________ 4.3 COVARIANCE —- Remember to add content on Covariance —————- Try it! Still using the forest_data used in the examples above. We will find the covariance between the trees’ diameter at breast height (DBH) and the Soil Nuitrients which is affected by the available phosphorous in the Soil(Soil_AP). Remember that Covariance measures how two variables vary together but is scale-dependent. # Compute covariance covariance_value &lt;- cov(forest_data$DBH, forest_data$Soil_AP, use = &quot;complete.obs&quot;) print(covariance_value) ## [1] 0.3007336 The covariance is positive which indicates that increase in available phosphorous, Soil_AP(Soil Nutrients) has an influence in the Diameter at breast height, DBH. Practical Exercise Solution ________________________________________________________________________________ 4.4 GEOSTATISTICS Geostatistics is a specialized branch of statistics focused on analyzing and estimating spatially correlated data. It is widely used in fields such as ecology, geology, and environmental science to predict values at unsampled locations based on the spatial relationships observed in sampled data. A geostatistical analysis typically unfolds in two main steps: variogram modeling and kriging. Together, these steps help estimate unknown values and assess the precision of those estimates, enabling informed decision-making in spatial studies. Lets introduce each of them will break down later with live examples 4.4.1 Variogram Modelling The first step in geostatistical analysis involves developing a model to describe the spatial relationships between sampled and unsampled locations. This model is known as a variogram or semivariogram. Here are the some of key features of the Variogram: The variogram quantifies how measurements at nearby locations are more similar (or correlated) than those farther apart.. It incorporates assumptions about the spatial structure of the data, considering known site characteristics and potential spatial trends. A varigram can be used n a study of soil nutrient distribution across a forest, the variogram might reveal that nutrient concentrations are strongly correlated within a radius of 50 meters but weaken beyond that distance. ———————Expand on this———————-(content from the book) 4.4.2 Kriging Once the variogram is established, the second step—kriging—is performed to estimate unknown measurements at unsampled locations. This is how kriging works: Measurements at surrounding sampled locations contribute to the estimate at the unsampled point. Weights are determined based on the variogram model, the location being estimated, and the distance of nearby sampled points. Kriging provides not only an estimate of the unknown value but also an estimate of precision, known as the kriging variance. The precision depends on the variability in the surrounding data. … and this how the accuracy and precision of kriging can be improved; Increasing the number of sampled locations enhances the accuracy and precision of kriging estimates. Kriging outputs are often used to create data contours or isopleths, which provide a visual representation of spatial variation across the study area. Kriging might be used to predict plant species richness across an unsampled landscape, leveraging data from sampled plots to generate a continuous map of biodiversity. —————remember to expand on this—————— "],["linear-regression-analysis.html", "Chapter 5 LINEAR REGRESSION ANALYSIS 5.1 Simple Linear Regression Analysis 5.2 Data transformation versus generalized linear model 5.3 Multiple Linear Regression (MLR) model 5.4 Hands-on Exercise", " Chapter 5 LINEAR REGRESSION ANALYSIS Regression is the finding of a relation between the input(predictor/independent) variable/s and the continuous output(target/dependent) variable. Now Linear Regression is finding the linear relationship between dependent variable and one or more independent features by fitting a linear equation to the observed data. If there is one independent variable therefore the regression in known as simple linear regression while if multiple variables are involved it is known as multiple linear regression. 5.1 Simple Linear Regression Analysis Simple linear regression seeks to explore and model the relationship between two quantitative variables. Here’s what it helps you do: Understand the strength of the relationship: For example, it can reveal how strongly rainfall affects soil erosion. If the relationship is strong, changes in rainfall levels are likely to lead to significant changes in soil erosion. Make predictions: It allows you to estimate the dependent variable’s value for a given value of the independent variable. For instance, if you know how much it rained, you can predict how much soil erosion occurred. The beauty of simple linear regression lies in its simplicity—you’re working with just two variables. One acts as the input, and the other as the output. It’s like finding a straight-line path that best connects the data points and helps explain their relationship. For instance, imagine plotting rainfall (in mm) on the x-axis and soil erosion (in kg) on the y-axis. A regression line (a straight line) is fitted to the data, showing the trend. If the line slopes upwards, it means more rain leads to more soil erosion—a positive relationship. If it slopes downward, it’s a negative relationship. So, simple linear regression isn’t just math—it’s a way to uncover insights and make informed predictions from real-world data! 5.1.1 Empirical model-building-regression analysis Simple linear regression is a parametric test, which means it relies on certain assumptions about your data to ensure accurate and reliable results. Let’s break down these assumptions: Key Assumptions of Simple Linear Regression Homogeneity of variance (Homoscedasticity): The prediction errors (residuals) should remain roughly the same across all values of the independent variable. In simpler terms, the “spread” of data points around the regression line should stay consistent, not get wider or narrower. Independence of observations: Each data point in your dataset should be collected independently. For example, there shouldn’t be hidden relationships or patterns among your observations, like duplicate measurements or a biased sampling method. Normality: The data (especially the residuals) should follow a normal distribution. This is important for making valid inferences from your model. Linearity: The relationship between the independent and dependent variables should be linear. That means the best-fitting line through your data points should be straight, not curved or clustered into groups. What if these assumptions aren’t met? If your data violate assumptions like homoscedasticity or normality, don’t worry—you still have options! For instance, you can use a nonparametric test like the Spearman rank test, which doesn’t rely on strict parametric assumptions. By ensuring your data meets these conditions, you can confidently use simple linear regression to model and predict relationships between variables! The formula for simple linear regression looks like this: \\[y = \\alpha + \\beta X + \\epsilon\\] where; \\(y\\): the predicted value of the dependent variable. \\(X\\): the independent variable believed to be influencing the value of \\(y\\) \\(\\alpha\\): the intercept, or the starting point. It tells you the predicted value of \\(y\\) when \\(X\\) (the independent variable) is 0. \\(\\beta\\): the regression coeefficient, which measures the expected change in \\(y\\) for each one unit increase in \\(X\\). This is the slope/gradient of the line. \\(\\epsilon\\): the error term, representing how much actual data points deviate from the predicted line. This is how linear regression works: Linear regression fits a line of best fit through your data points. It finds the value of \\(\\beta\\) (the slope) that minimizes the total error (the difference between the observed and predicted values of \\(y\\). In simple terms, the regression line represents the most accurate straight-line relationship between your independent variable (\\(X\\)) and your dependent variable (\\(y\\)). Try it! Lets make this practical, you will be required to download the plant height data from here. Remember: The goal in linear regression is obtain the best estimates for the model coefficients(\\(\\alpha\\) and \\(\\beta\\)). We will break down the process in steps; Load the data set df &lt;- read.csv(&quot;data/Plant_height.csv&quot;) The goal of this analysis is to find the linear association between the plant height and the temperature, particularly if plant height is dependent on the temperature. Define the independent and dependent variables: temperature(temp) is the independent variable while plant height(loght) is the dependent variable lm(loght ~ temp, data = df) ## ## Call: ## lm(formula = loght ~ temp, data = df) ## ## Coefficients: ## (Intercept) temp ## -0.22566 0.04241 5.1.2 Interpreting the regression results To get a detailed breakdown of the regression results, such as the coefficient values, \\(R^2\\), test statistics, p-values, and confidence intervals, you need to save the output of the lm function to an object. Then, use the summary() function on that object to extract and review the analysis details. Note: loght is log(plant height). model &lt;- lm(loght ~ temp, data = df) summary(model) ## ## Call: ## lm(formula = loght ~ temp, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.97903 -0.42804 -0.00918 0.43200 1.79893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.225665 0.103776 -2.175 0.031 * ## temp 0.042414 0.005593 7.583 1.87e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6848 on 176 degrees of freedom ## Multiple R-squared: 0.2463, Adjusted R-squared: 0.242 ## F-statistic: 57.5 on 1 and 176 DF, p-value: 1.868e-12 The regression output provides the slope and intercept of the regression line. Based on this analysis, the regression equation for predicting the (log-transformed) plant height as a function of temperature is: \\[loght = −0.22566+0.04241*temp + \\epsilon\\] and this is the breakdown of the analysis; Coefficients The intercept (-0.22566) represents the predicted value of log(plant height) when the temperature is zero. The slope (0.04241) indicates that for every one-unit increase in temperature, the log(plant height) increases by approximately 0.04241. Significance of Coefficients The t-statistics and p-values test whether the coefficients are significantly different from zero. Here, the p-value for temperature (&lt; 0.005) suggests a statistically significant relationship between temperature and plant height. The intercept’s p-value (0.031) is less important since we typically focus on the slope’s significance. Overall Model Significance The F-statistic (57.5) and its associated p-value (&lt;0.001) confirm that the overall model is statistically significant. With a single predictor, the p-value for the F-statistic is identical to the p-value for the slope coefficient. 5.1.3 R-squared and Adjusted R-Squared R-squared, or the coefficient of determination, measures the proportion of the variance in the dependent variable (\\(y\\)) that is explained by the independent variable(s) (\\(x\\)) in a regression model. It is a goodness-of-fit statistic. On the other hand, Adjusted R-squared modifies R-squared to account for the number of predictors in the model, providing a more accurate measure of model performance, especially for multiple regression. Like our model above shows that: The \\(R^2\\) value (0.2463) shows that approximately 24.6% of the variation in plant height is explained by temperature. The adjusted \\(R^2\\) (0.242) slightly adjusts for the model’s complexity and is more relevant when comparing models Practical Exercise You will be provided the creek_basin_data.csv file by your instructor. The data is about the effect of the amount of rainfall on the creek run off. The objective is to investigate for a linear relationship between the amount of rainfall and the creek run off. You are required to; Build a simple linear regression model Evaluate R-squared and Adjusted r-squared to find if the the linear model is effective. Finally, visualize the model Solution Build a simple linear regression model # Load the data creek_data &lt;- read.csv(&quot;data/creek_basin_data.csv&quot;) # Fit linear regression model model &lt;- lm(Runoff_cm ~ Rainfall_cm, data = creek_data) # Display model summary summary(model) ## ## Call: ## lm(formula = Runoff_cm ~ Rainfall_cm, data = creek_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4759 -1.2265 -0.0395 1.1927 4.4345 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01801 0.51541 0.035 0.972 ## Rainfall_cm 0.69281 0.02735 25.335 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.939 on 98 degrees of freedom ## Multiple R-squared: 0.8675, Adjusted R-squared: 0.8662 ## F-statistic: 641.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 The regression line equation inform of \\(y = \\alpha + \\beta x + \\epsilon\\) goes by; \\[\\text Runoff = 0.01801 + 0.69281 \\times Rainfall + \\epsilon\\] This means that a 0cm rain would yield to approximately 0.01801cm runoff The p-value for rainfall is \\(2 \\times 10^{-16}\\), meaning that the rainfall is a significant predictor for ruin off Evaluate R-squared and Adjusted r-squared to find if the the linear model is effective. # Extract R-squared and Adjusted R-squared to present it clearly r_squared &lt;- summary(model)$r.squared adjusted_r_squared &lt;- summary(model)$adj.r.squared cat(&quot;R-Squared:&quot;, r_squared, &quot;\\n&quot;) ## R-Squared: 0.8675463 cat(&quot;Adjusted R-Squared:&quot;, adjusted_r_squared, &quot;\\n&quot;) ## Adjusted R-Squared: 0.8661947 The Multiple r-squared = 08675, that is approximately 87% of the variability in runoff is explained by rainfall. Since it is a simple linear regression problem and we only have one predictor, R² and Adjusted R² are very close. Finally, visualize the model # Load the required library library(ggplot2) # Scatter plot with regression line ggplot(creek_data, aes(x = Rainfall_cm, y = Runoff_cm)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;blue&quot;) + labs(title = &quot;Rainfall vs Runoff&quot;, x = &quot;Rainfall (cm)&quot;, y = &quot;Runoff (cm)&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ________________________________________________________________________________ 5.2 Data transformation versus generalized linear model 5.2.1 Data Transformation Approach (Log Transformation) In this sub-course, we will consider a study where we examine how pollution levels affect fish abundance in a river system. We assume: Low pollution results in higher fish abundance. High pollution results in lower fish abundance. Fish abundance is a count variable (discrete, non-negative). Ecological data is often skewed (many low values, few high values). Log transformation stabilizes variance and makes the distribution closer to normal: \\[Y&#39;= log(Y + 1)\\] where; \\(Y&#39;\\) is the transformed fish abundance \\(Y\\) is the original fish abundance (adding 1 prevents log(0)). A linear regression model can be used after transformation: \\[log(Y+1) = \\beta_0 + \\beta_1X + \\epsilon\\] as described. However transformation posses with several limitations such as; Interpretation is difficult because the response variable is no longer on the original scale. Transformation may not fully address skewness in highly non-normal data. Not ideal for count data, which follow discrete probability distributions. 5.2.2 Generalized Linear Model(GLM) Instead of transforming the data, we can use a Poisson regression model, which assumes that fish abundance follows a Poisson distribution: \\[Y \\text ~ Poisson(\\lambda)\\] where \\(Y\\) is the data value count(count of observed fish) and \\(\\lambda\\) is the expected abundance(fish abundance in our case) GLMs use a log-link function to model the expected count directly: \\[log(\\lambda) = \\beta_0 + \\beta_1X\\] Solving for \\(\\lambda\\); \\[\\lambda = e^{\\beta_o + \\beta_1X}\\] Keep in mind that; \\(e^{\\beta_o}\\) is the expected abundance in low pollution areas \\(e^{\\beta_1X}\\) is the multiplicative effect of pollution. GLM(Poisson Regression model) posses various advantages where it; Maintains original scale of the response variable (counts). is more appropriate for count data, which are often non-normal. Exponential interpretation: \\(e^{\\beta_1}\\) gives the factor by which abundance changes due to pollution. Try it! Lets simulate the fish population in respect to pollution levels using R. Step 1: Generate the data # Load necessary libraries library(ggplot2) library(dplyr) # Set seed for reproducibility set.seed(123) # Simulate pollution levels (Low = 0, High = 1) n &lt;- 100 # Number of observations pollution &lt;- sample(0:1, n, replace = TRUE) # Generate fish abundance (count data) fish_abundance &lt;- rpois(n, lambda = ifelse(pollution == 0, 15, 5)) # Lower in polluted areas # Combine into a data frame data &lt;- data.frame(Pollution = factor(pollution, levels = c(0, 1), labels = c(&quot;Low&quot;, &quot;High&quot;)), Fish_Abundance = fish_abundance) # Check its distribution hist(data$Fish_Abundance) # Summary of simulated data summary(data) ## Pollution Fish_Abundance ## Low :57 Min. : 2.00 ## High:43 1st Qu.: 5.00 ## Median :11.50 ## Mean :10.26 ## 3rd Qu.:14.25 ## Max. :22.00 Apply log transformation since the data is right-skewed # Log transform fish abundance (add 1 to avoid log(0)) data$Log_Abundance &lt;- log(data$Fish_Abundance + 1) # Boxplot to visualize transformed vs. untransformed data ggplot(data, aes(x = Pollution, y = Fish_Abundance)) + geom_boxplot() + labs(title = &quot;Raw Fish Abundance vs. Pollution Level&quot;) + theme_minimal() ggplot(data, aes(x = Pollution, y = Log_Abundance)) + geom_boxplot(fill = &quot;lightblue&quot;) + labs(title = &quot;Log-Transformed Fish Abundance vs. Pollution Level&quot;) + theme_minimal() The log transformation helps stabilize variance and make the distribution more normal. However, using transformed data in regression may not always be statistically appropriate for count data. Instead of transforming, we use a Poisson (GLM), which is designed for count data. # Fit Poisson GLM glm_model &lt;- glm(Fish_Abundance ~ Pollution, family = poisson, data = data) # Model summary summary(glm_model) ## ## Call: ## glm(formula = Fish_Abundance ~ Pollution, family = poisson, data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.66014 0.03503 75.94 &lt;2e-16 *** ## PollutionHigh -1.06948 0.07724 -13.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 289.134 on 99 degrees of freedom ## Residual deviance: 59.428 on 98 degrees of freedom ## AIC: 465.3 ## ## Number of Fisher Scoring iterations: 4 The Poisson model directly models count data without transformation. The coefficient for “High Pollution” = -1.06948 shows how pollution reduces fish abundance in a statistically valid way. A negative correlation where increase in pollution results in decrease in fish abundance and vice versa. Practical Exercise Using the creek_basin_data.csv, Log transform the data and fit with a generalized linear model. If the the transformation improves the R-squared value(from the simple linear regression model), then its is more appropriate(Shows the residuals may not be normally distributed) Solution Lets log transform the data # Apply log transformation to both variables creek_data$log_Rainfall &lt;- log(creek_data$Rainfall_cm) creek_data$log_Runoff &lt;- log(creek_data$Runoff_cm) # Fit linear model with log-transformed data log_model &lt;- lm(log_Runoff ~ log_Rainfall, data = creek_data) summary(log_model) ## ## Call: ## lm(formula = log_Runoff ~ log_Rainfall, data = creek_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.42140 -0.09909 0.01952 0.10424 0.61348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.52368 0.14773 -3.545 0.000604 *** ## log_Rainfall 1.04825 0.05274 19.877 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2447 on 98 degrees of freedom ## Multiple R-squared: 0.8013, Adjusted R-squared: 0.7992 ## F-statistic: 395.1 on 1 and 98 DF, p-value: &lt; 2.2e-16 The model above captures the nonlinear relationship of the rainfall amount the run off, however it is a lower accuracy than the simple linear model. Accuracy reduced by 0.05 which is 5%. Now we fit a generalized Linear model # Fit GLM (Gaussian with identity link function) glm_model &lt;- glm(Runoff_cm ~ Rainfall_cm, family = gaussian(link = &quot;identity&quot;), data = creek_data) summary(glm_model) ## ## Call: ## glm(formula = Runoff_cm ~ Rainfall_cm, family = gaussian(link = &quot;identity&quot;), ## data = creek_data) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01801 0.51541 0.035 0.972 ## Rainfall_cm 0.69281 0.02735 25.335 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 3.758101) ## ## Null deviance: 2780.55 on 99 degrees of freedom ## Residual deviance: 368.29 on 98 degrees of freedom ## AIC: 420.16 ## ## Number of Fisher Scoring iterations: 2 The GLM results are almost identical to the original linear model and the AIC = 420.16 confirms a relatively good model fit, though the log-transformed model had a stronger R². If the residuals in the original linear model are not normal, the GLM can handle that better. ________________________________________________________________________________ 5.3 Multiple Linear Regression (MLR) model Unlike simple linear regression, multiple linear regression describes the linear relationship between two or more independent variables with one target(dependent) variable. The objective of multiple linear regression is to;- Find the strength of the relationship between two or more independent variables with the target variables. Find the value of the target variable at a certain value of the independent variable. When working on a multiple linear regression it is assumed that; the variance is homogeneous such that the prediction error does not change significantly across the predictor(independent) variables. It is also assumed, observations were independent and there was no hidden relationships among the variables when collecting the data. Additionally, the collected data follows a normal distribution and the independent variables have a linear relationship(linearity) with the dependent variable, therefore, the line of best fit through the data points is straight and not curved. Multiple linear regression is modeled by;- where;- \\(y\\) is the predicted value of the target variable. \\(\\beta_0\\) is the y-intercept. Value of y when all other parameters are zero. \\(\\beta_1X_1\\): \\(\\beta_1\\) is the regression coefficient of the first independent variable while \\(X_1\\) is the independent variable value. \\(\\cdots\\) do the same for however the number of independent variables are present. \\(\\beta_nX_n\\): the regression coefficient of the last independent variable. \\(\\epsilon\\) is the model error(variation not explained by the independent variables) The Multiple linear regression model calculates three things to find the best fit line for each independent variable;- The regression coefficient \\(\\beta_iX_i\\) that will minimize the overall error rate(model error). The associated p-value. If the relationship between the independent variable is statistically significant. The t-statistic of the model. T-statistic is the ratio of the difference in a number’s estimated value from its assumed value to its standard error. ——add an example —————————— ——Add Assumptions of Multiple Linear Regression———————- Practical exercise Using the airquality data set, fit a multiple linear regression model whereby the Solar radiation(Solar.R), Ozone and Wind are the independent variables while the Temperature (Temp) is the dependent variable. Interpret and analyze the results 5.4 Hands-on Exercise "],["conventional-and-probabilistic-risk-assessment.html", "Chapter 6 CONVENTIONAL AND PROBABILISTIC RISK ASSESSMENT 6.1 Introduction 6.2 Conventional or Point Risk Estimation 6.3 Probabilistic Risk Assessment Using Monte Carlo Simulation 6.4 Practical Exercise", " Chapter 6 CONVENTIONAL AND PROBABILISTIC RISK ASSESSMENT 6.1 Introduction Exposure to chemical is currently inevitable to the current society. Scientists and environmentalists aim not for zero exposure but minimize the release of the chemical contaminants to the environment and limit their potential adverse health or ecological effects. Maximum Contaminant Levels(MCLs) - is the concentration of a chemical contaminant that the environmental protection authorities believe would not cause substantial adverse health effects to the public. Assumptions based on exposure characteristics are made before deciding on a effective MCL. The input factors are then combined with the estimated toxicity or potency of the chemical, to back-calculate an allowable concentration or MCL for the chemical, which is believed would not cause substantial adverse health effects. These are some of the assumptions; The daily rate of water consumption. An individual body weight. Duration of residency at one location Expected human lifespan. These assumptions are the basic of back-calculations that is used to establish the allowable contaminant concentrations. Also, forward risk assessment can be performed to quantify the actual health risks posed by exposure to environmental contaminants. There are two methods used to estimate the potential risk of a contaminant concentration; Conventional or point risk estimation. Probabilistic risk assessment. 6.2 Conventional or Point Risk Estimation Point risk estimation uses a single variable to calculate the potential risk of a chemical contaminant for instance an adult individual body weight of 80 kilos, 1 to 2 liters of water as the daily ingestion rate for an individual, human lifespan of 70 years etc. Based on the above variable values the potential risk of cancer from a contaminant may be 10 in a million. The exposure of contaminants to humans have main three pathways; ingestion : consumption of medium containing the contaminant such as water or soil. dermal contact : absorption of the contaminant through skin contact. inhalation : breathing air containing the contaminant. The exposure might be either; carcinogenic(causing cancer), noncarncinogenic (causing other illness that is not cancer such skin rashes, suffocation, irritation) or both. Here, the potential health risk is estimated by first computing an average daily intake of the contaminant, and then integrating it with the contaminant health factor to quantify the risk. According to USEPA, 1989, the basic equation for computing daily intake or dose the ingestion or oral pathway is as follows; \\[I = {{C * IR * EF * ED}\\over{BW * AT}}\\] Where; \\(I\\): the chronic average daily intake in milligrams per kilogram of bodyweight of the contaminant per day. \\(C\\): the chemical concentration(eg. mg/kg for soil or mg/l for water) \\(IR\\): the ingestion rate(e.g 50mg/day for soil or 2liters/day for water). \\(EF\\): the exposure frequency(days per year that the exposure occurs) \\(ED\\): the exposure duration in years. \\(BW\\): body weight in kilograms. \\(AT\\): averaging time in days(which is equal to ED365 for noncarcinogens and 70365 for carcinogens, where 70 years is the assumed average human lifespan and there are 365 days in the year) Also, according to USEPA, 2004, this is the basic dermally absorbed intake or dose through the soil dermal contact pathway; \\[I = {{Cs * 10^{-6}SA*AF*EV*ABS*EF*ED}\\over{BW*AT}}\\] and this is the equation to calculate the daily absorbed intake or dose through water dermal contact pathway; \\[I = {{Cw * 10^{-3}SA*PC*ET*EF*ED}\\over{BW*AT}}\\] Where: \\(Cs\\) : is the soil concentration of the contaminant, that is, EPC, usually reported in mg/kg but the conversion factor of \\(10^{-6}\\) is applied to convert the units to kg/kg. \\(Cw\\) : is the water concentration of the contaminant, that is, EPC, usually reported in mg/l but the conversion factor of \\(10^{-3}\\) is applied to convert the units to mg/cm3 to maintain consistency of units. \\(SA\\): the exposed skin surface area (cm2). \\(AF\\): s the soil-to-skin adherence factor in milligrams per square centimeters per soil contact event (i.e.,mg/cm2 event) \\(EV\\): is the number of contact events per day, usually assumed as one event per day (i.e., 1/day). \\(PC\\): the dermal permeability constant (cm/h), which estimates the rate of transport of the contaminant across the skin into the body. \\(ABS\\): the dermal absorption fraction for the contaminant (unitless) Try it! We will access the potential health risks associated with consuming water from various sources, utilizing both Conventional (Point) Risk Estimation. The dataset we’ll use is the Water Quality and Potability dataset from Kaggle, which contains water quality measurements and assessments related to potability. The data can be downloaded from here. We will; Calculate the Average Daily Intake (ADI) of a specific contaminant (e.g., Trihalomethanes) using standard exposure assumptions. Estimate the potential health risk based on the computed ADI. Before we solve the above, it assumed that; Body Weight(BW):70kg Water Ingestion Rate (IR): 2 liters/day Exposure Frequency (EF): 350 days/year Exposure Duration (ED): 30 years Averaging Time(AT): For non-carcinogens: \\(AT = ED \\times 365\\) For carcinogens: \\(AT = 70 \\times 365\\) and the formula for Average Daily Intake(ADI): \\[{ADI} = {{C \\times IR \\times EF \\times ED}\\over{BW \\times AT}}\\] # Load the data df &lt;-read.csv(&quot;data/water_potability.csv&quot;) # Focus on the Trihalomethanes column trihalo_data &lt;- df$Trihalomethanes # Define constants BW &lt;- 70 # kg IR &lt;- 2 # liters/day EF &lt;- 350 # days/year ED &lt;- 30 # years AT_noncarcinogen &lt;- ED * 365 # days AT_carcinogen &lt;- 70 * 365 # days # Compute average concentration of Trihalomethanes C_avg &lt;- mean(trihalo_data, na.rm = TRUE) # mg/L # Compute ADI ADI_noncarcinogen &lt;- (C_avg * IR * EF * ED) / (BW * AT_noncarcinogen) ADI_carcinogen &lt;- (C_avg * IR * EF * ED) / (BW * AT_carcinogen) # Display results cat(sprintf(&quot;Average Daily Intake (ADI) for Non-Carcinogens: %.6f mg/kg/day\\n&quot;, ADI_noncarcinogen)) ## Average Daily Intake (ADI) for Non-Carcinogens: 1.819077 mg/kg/day cat(sprintf(&quot;Average Daily Intake (ADI) for Carcinogens: %.6f mg/kg/day\\n&quot;, ADI_carcinogen)) ## Average Daily Intake (ADI) for Carcinogens: 0.779604 mg/kg/day Practical Exercise One of the examples is the inhalation risk assessment (example 21.1) Solution ________________________________________________________________________________ 6.3 Probabilistic Risk Assessment Using Monte Carlo Simulation While the method discussed above, point risk estimation, produces a single estimate of the health risk, probabilistic risk assessment produces probabilities or likelihoods of specified risks. This is due to variability and uncertanities regarding contaminant concentrations and other assumptions like daily water consumption, individual bodyweight, genetics and exposure to the contaminant. This leads to result in overestimation or underestimation of the actual exposure risk for some members. Probability risk assessment produces a full probability distribution of the risks indicating the probability or likelihood for each specified risk by using exposure characteristics and contaminant concentrations. Similarly, probability distributions are speciﬁed for other exposure characteristics such as water or soil ingestion rates, inhalation or breathing rates, and skin surface area, if the distributions are known. Monte Carlo simulation is then used to produce hundreds or thousands of various combinations of values from each of the probability distributions speciﬁed for the exposure characteristics and contaminant concentrations, and a point risk computed for each combination of inputs, thereby generating a probability distribution of risks Try it! Using the water potability data set(used in the previous example), we will; Define probability distributions for exposure parameters (e.g., normal distribution for body weight, log-normal for ingestion rate). Use Monte Carlo simulation to generate a large number of possible exposure scenarios. Calculate the ADI for each scenario and develop a risk distribution. Interpret the probability distribution of risks to assess the likelihood of adverse health effects. By defining the probability distributions(SD= standard deviation); Body weight(BW): Normal distribution, mean = 70kg, SD = 10kg Water Ingestion Rate (IR): Log-normal distribution, mean = 2litres/day, SD=0.5litres Trihalomethanes Concentration (C): Will find from the data values, trihalo_data. # Load necessary library library(MASS) library(ggplot2) # Number of simulations num_simulations &lt;- 10000 # Generate random values from distributions set.seed(123) # for reproducibility BW_sim &lt;- rnorm(num_simulations, mean = 70, sd = 10) IR_sim &lt;- rlnorm(num_simulations, meanlog = log(2), sdlog = 0.5) C_sim &lt;- sample(trihalo_data, num_simulations, replace = TRUE) # Compute ADI for each simulation ADI_sim &lt;- (C_sim * IR_sim * EF * ED) / (BW_sim * AT_carcinogen) # Convert to data frame for visualization ADI_df &lt;- data.frame(ADI_sim) # Plot probability distribution of ADI values ggplot(ADI_df, aes(x = ADI_sim)) + geom_histogram(bins = 50, color = &quot;black&quot;, fill = &quot;blue&quot;, alpha = 0.7) + labs(title = &quot;Probability Distribution of ADI (Monte Carlo Simulation)&quot;, x = &quot;ADI (mg/kg/day)&quot;, y = &quot;Frequency&quot;) + theme_minimal() ## Warning: Removed 455 rows containing non-finite outside ## the scale range (`stat_bin()`). Finally, lets compute the risk probabilities. We will determine the percentage of cases where ADI(calculated in the previous example exercise) exceeds the reference dose(RfD) – will find from USEPA. # Assume reference dose (RfD) from U.S. EPA for Trihalomethanes RfD &lt;- 0.01 # mg/kg/day (example value) # Compute probability of exceeding RfD prob_exceedance &lt;- mean(ADI_sim &gt; RfD, na.rm=TRUE) * 100 # Display result cat(sprintf(&quot;Probability of Exceeding Safe Limit (RfD): %.2f%%\\n&quot;, prob_exceedance)) ## Probability of Exceeding Safe Limit (RfD): 99.99% There is a high risk of carcinogenic substances in the water which should be taken care of. 6.4 Practical Exercise Solution One of the examples is the inhalation risk assessment (example 21.2). Will search examples from online ________________________________________________________________________________ "],["datasets.html", "Chapter 7 Datasets", " Chapter 7 Datasets forest_health - https://www.kaggle.com/code/furkan09/forest-health-ecological-diversity-analysis/input africa soil property - https://www.kaggle.com/competitions/afsis-soil-properties/data California Water Quality Status - https://data.ca.gov/dataset/a7601751-8e18-4d1b-a7f1-e395ca9d3517/resource/4f9dcf98-5fb4-477b-8780-4411c083fee8/download/fig.-4-5.-sum-pyrethroid-toxic-units.csv Water Portability Data - https://www.kaggle.com/code/moosecat/water-quality-eda-cleaning-and-prediction?select=water_potability.csv Air Quality Data India - https://www.kaggle.com/datasets/shrutibhargava94/india-air-quality-data Daily Temperature of Melbourne - https://www.kaggle.com/code/samfaraday/melbourne-daily-min-temperatures-eda Forest Health and Ecological Diversity - https://www.kaggle.com/datasets/ziya07/forest-health-and-ecological-diversity Water Potability - https://www.kaggle.com/datasets/uom190346a/water-quality-and-potability "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
