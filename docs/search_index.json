[["index.html", "Ecological Analysis Chapter 1 INTRODUCTION 1.1 Data Distributions 1.2 Hands-on Exercises", " Ecological Analysis Basim Alsaedi 2024-11-16 Chapter 1 INTRODUCTION The distinction between data sample and the underlying data population is always ignored. For instance the mean concentration of a chemical contaminant in a soil sample of a contaminated site. Here, the data sample is the soil sample that was taken to the laboratory to find out the mean concentration of the contaminant while the data population is the entire data population consisting of all possible soil measurements of the chemical contaminant at the site. We need to define some terms to get this clear. Data Population: is the entire set of individuals, items or observations of interest in a study. For instance all the soil samples from the contaminant site. Data Sample: is the subset of the population selected for analysis. For example, measuring the height of randomly selected trees in the forest. Data Distribution: describes how data points are spread of arranged. Data Sample vs Data Population A deep understanding of the difference between data sample and data population is important in inferential statistics which seeks to generalize the results of data analyses performed on the data samples. A data population consists of all possible observations or data points concerning the characteristic of interest. For instance, when finding out the mean height of all the oak trees in a forest, the data population will be every height of every single oak tree in the forest. Contrary, data sample is usually a limited subset of observations drawn for the entire population. In this case we can have a data sample by randomly selecting 100 oak trees and measuring their heights. Try it! Lets simulate data of oak trees with R # Seed for reproducibility set.seed(123) # A population of 12500 oak trees population_heights &lt;- rnorm(12500, mean = 20, sd=5) head(population_heights) ## [1] 17.19762 18.84911 27.79354 20.35254 20.64644 28.57532 We have simulated the heights of 12500 oak trees in a forest. The variance population_heights represent the data population for the oak tree heights. Lets randomly select 86 trees from the entire data population of oak tree heights # Randomly select 86 trees sample_heights &lt;- sample(population_heights, size=86, replace = FALSE) tail(sample_heights) ## [1] 25.88792 29.03996 29.00965 37.10547 17.76631 19.81021 The variable sample_heights represents data sample for the oak tree heights. If we make this experiment more interesting, we find the mean and standard deviation for the data population and the data sample for the oak tree heights. Lets do it! # Mean and standard deviation of the entire population population_stats &lt;- c( Mean = mean(population_heights), SD = sd(population_heights) ) population_stats ## Mean SD ## 19.995609 4.996295 The entire oak tree data population has a mean height of approximately 20 meters and a standard deviation of approx 5 meters. Lets see how it compares with the data sample of the oak tree heights. # Mean and standard deviation of the sample sample_stats &lt;- c( Mean = mean(sample_heights), SD = sd(sample_heights) ) sample_stats ## Mean SD ## 21.094995 5.507379 There is a minor difference between the data sample and the entire data population of the oak tree heights. As you can see the selected sample of oak trees has an average height of 21 meters and a standard deviation of 5.5 meters. Practical Exercise Run the code below to generate the data of soil samples collected in farm with livestock. The main objective was to find the pH of the soil. Use the code below to simulate the collection of the data. # Set seed for reproducibility set.seed(76) # Simulate the data collection farm_population_pH &lt;- rnorm(n = 5342, mean=3.61, sd=0.8) head(farm_population_pH) ## [1] 3.864037 3.471632 3.544874 3.192810 3.239502 3.724600 You are required to work on the problems below; What is the Mean and Standard Deviation of the pH of the entire farm population soil samples. Select 65 random samples from the entire population. Calculate the Mean and Standard Deviation of the sample. How does the Mean and Standard deviation differ from the entire population? Solution Run the code below to generate the data of soil samples collected in farm with livestock. The main objective was to find the pH of the soil. Use the code below to simulate the collection of the data. # Set seed for reproducibility set.seed(76) # Simulate the data collection farm_population_pH &lt;- rnorm(n = 5342, mean=3.61, sd=0.8) head(farm_population_pH) ## [1] 3.864037 3.471632 3.544874 3.192810 3.239502 3.724600 You are required to work on the problems below; What is the Mean and Standard Deviation of the pH of the entire farm population soil samples. population_stats &lt;- c( Mean = mean(farm_population_pH), standard_deviation = sd(farm_population_pH) ) population_stats # show the results ## Mean standard_deviation ## 3.6218315 0.7934042 Select 65 random samples from the entire population. farm_sample_pH &lt;- sample(farm_population_pH, size = 65, replace = FALSE) head(farm_sample_pH) ## [1] 3.478034 4.268278 4.084203 4.044815 2.613637 3.575463 Calculate the Mean and Standard Deviation of the sample. sample_stats &lt;- c( Mean = mean(farm_sample_pH), standard_deviation = (sd(farm_sample_pH)) ) sample_stats # show the results ## Mean standard_deviation ## 3.7237092 0.7554283 How does the Mean and Standard deviation differ from the entire population? The sample mean is slightly higher than the whole population mean while the standard deviation of the whole populations is almost equal to the data sample’s standard deviation ________________________________________________________________________________ 1.1 Data Distributions Data Distribution describes how data values in data population are spread across the range of possible values. For instance, in our case the height of oak trees in the forest might be distributed normally, meaning that most trees cluster around the average height. Understanding data distribution is crucial in ecological analysis since it helps in; identifying patterns ( for instance seasonal changes in bird populations). choosing appropriate statistical tests. modelling ecological phenomena accurately. When dealing with continuous data values like height of oak trees, soil acidity or rainfall, Probability Distribution Functions is used to find the likelihood of different outcomes in a population. There are two types of probability distribution functions, namely; Cumulative distribution function, Empirical Cumulative distribution function. Below is the distribution of oak tree heights represented in a density plot. library(ggplot2) # Create a data frame from the data oak_df &lt;- data.frame(Height = population_heights) # Plot the density plot ggplot(oak_df, aes(x = Height)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of tree heights&quot;, x = &quot;Tree Height(m)&quot;, y = &quot;Density&quot; ) + theme_minimal() Above is the distribution of oak tree heights, lets find its probability using the two types of distribution functions. Cumulative Distribution Function This distribution function leans toward that a probability of a random variable is less than or equal to a certain value. We can randomly select a sample of heights from a population using the following code: sample_heights &lt;- sample(population_heights, size=86, replace = FALSE) tail(sample_heights) ## [1] 16.77457 21.71678 21.76080 36.45259 25.02581 14.43727 For example, if we analyze heights, the CDF could help answer: “What percentage of people have a height less than or equal to 170 cm?” The CDF of a normal distribution can be visualized using: x &lt;- seq(min(population_heights), max(population_heights), length = 100) cdf_values &lt;- pnorm(x, mean = mean(population_heights), sd = sd(population_heights)) plot(x, cdf_values, type=&quot;l&quot;, col=&quot;blue&quot;, lwd=2, xlab=&quot;Height&quot;, ylab=&quot;Cumulative Probability&quot;, main=&quot;Cumulative Distribution Function (CDF)&quot;) Empirical Cumulative Distribution Function The Empirical Cumulative Distribution Function (ECDF) is a way to estimate the CDF based on the actual observed data (your sample). Instead of assuming a theoretical distribution, it calculates the proportion of values that are less than or equal to each observed value in the sample. Lets create an ECDF plot in R using the sample heights # calculate CDF CDF &lt;- ecdf(sample_heights) # draw the cdf plot plot(CDF, main=&quot;Empirical Cumulative Distribution Function (ECDF)&quot;, xlab=&quot;Height&quot;, ylab=&quot;Cumulative Probability&quot;, col=&quot;red&quot;) As you can see, the ECDF graph shows a step-like increase whenever a new height is encountered and the curve provides a data-driven approximation of how heights are distributed in the sample. Here are the main differences between CDF(Cumulative Distribution Function) and ECDF(Empirical Cumulative Distribution Function); CDF is based on the entire population while ECDF is based on the observed sample data. The shape of CDF curve is smooth while the ECDF has a step-like plot. CDF is used to make model assumptions and probability estimates while ECDF is used to make data driven insights and explanatory analysis. Practical Exercise Solution ________________________________________________________________________________ 1.1.1 Types of Distribution In statistical tests, it is assumed that the data sample represents the underlying data population. Similarly, the distribution of the data sample is assumed to be similar to the one of the data population. These are what characterizes the data distribution; whether the data is; discrete or continuous symmetrical or asymmetrical bounded by lower and/or upper limits or unbounded. Lets discuss different types of data distributions. 1.1.1.1 Normal distribution The normal distribution is a continuous symmetrical distribution in the real number domain (i.e., with the set of possible values ranging between -∞ and ∞) whose probability density function plots as a smooth bell-shaped curve, and whose cumulative distribution is S-shaped. Try it! To demonstrate this, lets generate 100 random data values with a mean of 5 and a standard deviation of 3. set.seed(1) data &lt;- rnorm(100, mean=5, sd=3) df &lt;- data.frame(data) # Plot the data ggplot(df, aes(x = data)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of data sample&quot;, x = &quot;Sample Value&quot;, y = &quot;Density&quot; ) + theme_minimal() The data distribution below represents a near bell-shaped curve. A normal distribution with a mean of zero and variance of 1 (or standard deviation of 1, since standard deviation is the square root of variance and square root of 1 is 1) is referred to as a standard normal distribution A standard normal distribution has many applications in statistics such as computation of cumulative probabilities and critical values for hypothesis tests. Try it! Lets generate a random data set that is standardized. set.seed(100) data &lt;- rnorm(100, mean=0, sd=1) df &lt;- data.frame(data) # Plot the data ggplot(df, aes(x = data)) + geom_density(fill = &quot;blue&quot;, alpha=0.5) + labs( title = &quot;Distribution of data sample&quot;, x = &quot;Sample Value&quot;, y = &quot;Density&quot; ) + theme_minimal() Any data sample that follows a normal distribution can be standardized to a standard normal distribution by subtracting its mean from each individual data value and dividing the result by its standard deviation. The resultant values can be referred to as standard score, normal score, normal quantile or z-value) Here is the formula for calculation the z-value (standardized score); \\[z = {{x - \\overline x}\\over{\\sigma}}\\] Where; \\(x\\) is the actual value \\(\\overline x\\) is the data sample mean \\(\\sigma\\) is the standard deviation Try it ——————-will generate a data distribution with a known standard deviation and mean then standardize it. Will then plot both samples - raw data and the standardized one—————————- Example: CUMULATIVE AND EXCEEDANCE PROBABILITIES FOR A NORMAL DISTRIBUTION Groundwater Manganese Concentrations in mg/L ______________________Expand on this____________________________________ modify the code below to show up the distribution x = seq(0, 0.4, length=100) y = dnorm(x, 0.52, 0.18) polygon(c(0, x, 0.4), c(0, y, 0), col = &quot;gray&quot;) # Add a shaded area x = seq(1, 0.9, length=25) y = dnorm(x, 0.52, 0.18) polygon(c(1, x, 0.9), c(0, y, 0), col = &quot;gray&quot;) Goodness-of-Fit (GOF) Tests for the Normal Distribution In parametric statistical tests, it assumed that the data is normally distributed or can be normalized by data transformation such as log transform. The parameters, such as mean and standard deviation, in parametric tests must be specified. Verifying the data normality is crucial before conducting the tests. If it fails the test of normality other types of tests(non parametric) are considered. The GOF tests are used to access the normality of the data sample which requires 8 to 10 randomly picked data values. The tests are; Normal probability plot - also known as normal quantile plot. Coefficient of Variation(CV) Coefficient of Skewness Shapiro-Wilk(SW) and Shapiro-Francia(SF) procedures Filiben’s probability plot correlation coefficient (PPCC) Shapiro-Wilk multiple group normality The CV test is the most commonly used method. It is computed simply by dividing the standard deviation by the mean. If the resultant value is greater than 1 then the data fails the normality test, otherwise it passes. \\[CV = {\\sigma\\over{\\overline x}}\\] Other GOF tests that are used in testing for normal and other distributions are Anderson–Darling (AD) test and the Lilliefors–Kolmogorov–Smirnov test The Coefficient of Skewness stated above provides a more direct measure of skewness where skewness of the magnitude of greater than 0.5 is considered moderate to the substantial degree of skewness. ——————————Add an example———————— Central Limit Theorem ____________________simplify it with an example _________________________ 1.1.1.2 Lognormal, Gamma, and Other Continuous Distributions Before we dive into Gamma distribution, we need to to get the definition of exponential distribution. Exponential distribution is the probability of the waiting time between events in a Poisson Process. Gamma Distribution Exponential infers the probability until the first event happens while Gamma distribution gives us the probability of the waiting time until the \\(n^{th}\\) event. Gamma distribution Logistic Distribution Other Continuous Distributions 1.1.1.3 Distributions Used in Inferential Statistics Student’s t Distribution Chi-Square Distribution F Distribution 1.1.1.4 Dicrete Distributions Binomial Distribution Poisson Distribution 1.2 Hands-on Exercises Solution ________________________________________________________________________________ "],["hypothesis-testing.html", "Chapter 2 HYPOTHESIS TESTING 2.1 Introduction 2.2 Parametric tests 2.3 Nonparametric test 2.4 One-Way ANOVA 2.5 Hands-on Exercise", " Chapter 2 HYPOTHESIS TESTING 2.1 Introduction Hypothesis testing is a standard statistical procedure for deciding between two competing alternatives or scenarios, based on evidence presented by a data sample collected for the purpose of conducting the test. Most descriptive statistics such as mean, median and standard deviation are usually computed from data samples which may vary with the whole population. For instance, we want to find the concentration of hexavalent chromium, a human carcinogen, in soil. We will collect random 20 samples. The average concentration of the carcinogen from the collected samples will at most vary with the average concentration in the entire land, that’s where hypothesis testing comes in. Basic terminology and procedure for hypothesis testing There is a need to understand the basic terms before diving deep into the process of hypothesis testing. If we have historical data, the presence of a trend must be established and below are the assumptions that should be made prior to conducting hypothesis testing; The data is assumed to belong to a specified distribution(typically normal distribution). The data has a stable or stationary trend(free from of temporal or spatial trend). There are no unusual observations or outliers. The hypothesis testing that is based on the above is described as parametric hypothesis testing. Otherwise, if the above assumptions are not met, the test is known as non-parametric hypothesis testing. Null hypothesis (\\(H_0\\)): Is a statement of the presumed default or baseline condition. In this case the null hypothesis is that there is no difference between the carcinogenic concentration is below the allowed limit. Alternate hypothesis (\\(H_1\\) or \\(H_A\\)): is the inverse or the opposite of the null hypothesis. Alternate hypothesis contradicts the statement of the presumed default. In this case, the alternate hypothesis states that the carcinogenic level is above the allowed limit. The evidence produced by the sample after conducting hypothesis testing can either refute or support the null hypothesis. The one-tailed (One-Sided) Hypothesis test checks for an effect in only one direction. In our case, we test if the carcinogen levels exceed the allowed limit(right side). Alternatively, it can be checked of it is below the allowed limits (left-sided). Upper-Tailed Hypothesis Test is the same as the right one-tailed hypothesis test that checks if the value of interest exceed the null hypothesis value Lower-Tailed Hypothesis Test is the left one-tailed hypthesis test where it checks if the value of interest is below the null hypothesis value. In our case the allowed limit for carcinogens in soil. Two-Tailed Hypothesis Test checks for the effect in both directions. In our we can change to find if the carcinogen levels are below or above the allowed limit. T-Value is the test statistic calculated from the sample data to determine how far the sample mean is from the hypothesized mean, in terms of standard error. Critical T-Value is the threshold value for the t-distribution that corresponds to the significance level. If the t-value exceeds the critical t-value, the null hypothesis is rejected. Significant level(\\(\\alpha\\)) is the probability threshold for rejecting the null hypothesis, usually set to 0.05(5%). In most cases if the probability of observing the test statistic (p-value) is less than 0.005, the null hypothesis is rejected. Degrees of Freedom(df) are the number of independent values in the data sample that are free to vary when calculating a statistic. In most cases df is calculated as \\[df = n - 1\\] where \\(n\\) represents the sample size(count). In our cases where we have 20 samples therefore \\(df = 20 -1\\) and the degree of freedom will be 19 P-Value indicates the probability of obtaining a test statistic at least as extreme as the observed one, assuming the null hypothesis is true. Below are the steps taken in hypothesis testing; Formulate the null and alternate hypothesis. Assume the null hypothesis is true and compute a test statistic that follows a known distribution. Calculate a test statistic value based on the data sample which follows a known distribution and determine the critical value for chosen distribution at a given significance level. Compare the test statistic to the critical value to assess the plausibility of the null hypothesis. For symettrical distributions(e.g t, normal), critical values are located on; upper-tailed test: positive side of the distribution lower-tailed test: negative side of the distribution two-tailed test: both sides, with equal magnitude Calculate the p-value to find how the probability of obtaining a test statistic more extreme than the observed value. Try it! To get the steps above clear, lets work on an example. This is an hypothetical problem where a regulatory body sets that the acceptable phosphorous concentration in a lake at 1.5mg/L. 30 sample measurements were collected at different locations in the lake. The objective was to determine if the phosphorous levels are significantly higher than the allowed limit. Lets simulate the data. set.seed(123) # for reproducibility phosphorous_levels &lt;- rnorm(n = 30, mean = 1.7, sd = 0.2) Formulate the null hypothesis Null Hypothesis(\\(H_0\\)): \\(\\mu\\) &lt; 1.5 - phosphorous levels are within the acceptable levels. Alternate Hypothesis (\\(H_1\\)): \\(\\mu\\) &gt; 1.5 - phosphorous levels exceed the acceptable limit. This is a right-sided(one-sided) test since we are only interested in detecting an increase in phosphorous levels We use a one-sample ttest since the population data standard deviation is unknown, and the sample size is small(n=30). The formula for t-statistic is calculated as; \\[t = {{\\overline{x} - \\mu_0}\\over{s/\\sqrt{n}}}\\] Where: \\(\\overline{x}\\) is the sample mean. \\(\\mu_0\\) is the population mean under null hypothesis (1.5mg/L) \\(s\\) is the sample standard deviation. \\(n\\) is the sample size. Perform the one-sample t-test # One sample t-test t_test_result &lt;- t.test(phosphorous_levels, mu = 1.5, alternative = &quot;greater&quot;) # Display the results print(t_test_result) ## ## One Sample t-test ## ## data: phosphorous_levels ## t = 5.3201, df = 29, p-value = 5.21e-06 ## alternative hypothesis: true mean is greater than 1.5 ## 95 percent confidence interval: ## 1.629713 Inf ## sample estimates: ## mean of x ## 1.690579 Determine the critical value For the right-tailed test at a 5% significance levels(\\(alpha\\)=0.05), the critical value is the t-value that corresponds to the top 5% of the t-distribution. The qt() function will be used to calculate this; \\[t_{critical} = qt(1-\\alpha, df = n - 1)\\] critical_t &lt;- qt(0.95, df = length(phosphorous_levels) - 1) critical_t ## [1] 1.699127 The t-statistic is greater than the critical t therefore the null hypothesis is rejected. Lets confirm this by also computing the p_value Calculate the null hypothesis p_value &lt;- t_test_result$p.value p_value ## [1] 5.210072e-06 The null hypothesis is rejected as the p_value is less than 0.05. Therefore, we make conclusion that the phosphorous levels in the lake exceed the acceptable levels. 2.2 Parametric tests As described before parametric tests rely on the assumptions that the data follows a known distribution (commonly the normal distribution) and it should be free of major outliers. In case where the test requires the data to follow a normal distribution, if the sample size is small it must follow a normal distribution while for larger sample size (e.g n≥30) the Central Limit Theorem allows the researchers to approximate normality even of the data is not perfectly normal 2.2.1 Parametric single-sample test This type of test is used to determine whether the mean of a single sample is significantly different from a known or hypothesized population mean. It assumes that the sample data comes from a population that follows a normal distribution (or has a large enough sample size to approximate normality). The main objective of this test is to test whether the sample mean differs significantly from a reference or target value, such as an industry standard, a population average, or a theoretical expectation in hypothesis testing These are the steps taken for One-Sample T-test; State the Hypotheses: Null Hypothesis(\\(H_0\\)): The sample mean equals the population mean (\\(\\mu = \\mu_0\\)) Alternate Hypothesis(\\(H_A\\)): The sample mean is different from the population mean(\\(\\mu \\neq \\mu_0\\)) Compute the test statistic The formula for test statistic is \\[t = {{\\overline x - \\mu_0}\\over{s/\\sqrt n}}\\] Where: \\(\\overline x\\): is the sample mean \\(\\mu_0\\): is the hypothesized population mean \\(s\\): is the sample standard deviation \\(n\\): is the sample size Determine the degree of freedom as \\(df = n- 1\\) Compare \\(t\\) to critical value or use p-value 2.2.2 Parametric two-sample test Lets understand what is two-sample test, before specifically diving into the parametric two sample test; Two-sample tests are essential statistical tools used to compare two different populations to determine if there are significant differences between them. These tests are widely applied in ecology to compare environmental factors such as contaminant levels, species diversity, or soil nutrient concentrations across different locations or time periods. Two-sample tests help us compare the means, medians, or other characteristics of two groups. For example, scientists might want to know if the average soil arsenic concentration in an industrial area is higher than that in a nearby natural reserve. To perform these tests, data must be collected from both groups being compared. Here are the assumptions of two-sample tests; Independence of Data Values: Data points should not influence each other. No trends in data: data should not show patterns over time for instance increasing and decreasing trends. If trends exist, adjustments such as pairing or deseasonalization should be considered. In this case, we are focusing on the parametric tests – a parametric two-sample test is carried when the data follows a normal distribution. There are two types of parametric two-sample tests; independent and paired sample tests. Independent Two-Sample Test These tests are used when the two groups being compared have no inherent connection. For example: Comparing soil arsenic levels up-gradient vs. down-gradient of an industrial facility. Measuring PCB concentrations upstream and downstream of a river. In these cases, since the samples are collected from different locations without any relationship, an independent test is appropriate. These are the steps taken when carrying out independent tests; Calculate the test statistic based on the difference between sample means. Compare the test statistic with a critical value from the t-distribution. If the test statistic is extreme or the p-value is less than the significance level (e.g., 0.05), reject the null hypothesis. Paired Two-Sample Tests Paired tests are used when the samples are related or dependent on each other. This often happens when measurements are taken from the same location at different times. Examples include: Before and After Cleanup: Measuring soil contaminant levels at a site before and after remediation. Duplicate Sample Analysis: Sending identical samples to two different laboratories to assess the consistency of results. Before carrying out paired two-sample test it is assumed that the data is paired(each observation in one group corresponds to an observation in the other group), differences between pairs should follow a normal distribution and no extreme outliers should be present. Here are the steps taken when performing paired two-sample test; Calculate the differences between paired observations. Compute the mean and standard deviation of the differences Calculate the t-statistic Compare the test statistic with the critical value or p-value If the test statistic is extreme or the p-value is less than the significance level, reject the null hypothesis. Here is the formula for calculating the t-statistic; \\[{t_0} = {{\\overline D}\\over{S_D/\\sqrt n}}\\] where; \\(\\overline D\\) and \\(S_D\\) is the mean and standard deviation of the differences between the data samples of the two populations \\(n\\) is the sample size The null hypothesis \\(H_0\\) is that the population mean difference between the two populations, $_D $ is zero Imagine an ecologist wants to determine if the cleanup of an industrial site has effectively reduced contaminant levels. By comparing pre- and post-cleanup soil samples at the same locations, they can assess the success of remediation efforts using a paired two-sample test. Similarly, comparing background levels with site concentrations using an independent two-sample test can help identify pollution sources. Try it! ————-Add an example exercise—————– Here are the important considerations when performing parametric two sample test; If the data is not normally distributed, applying a logarithmic transformation can help, but it should be done carefully to avoid misinterpretation. These tests are often conducted using statistical software such as R, Python, or Excel for ease and accuracy. Practical exercises Solution ________________________________________________________________________________ 2.3 Nonparametric test 2.3.1 Nonparametric two-sample paired sign test and paired Wilcoxon Signed Rank Test 2.4 One-Way ANOVA 2.4.1 Parametric One-Way ANOVA 2.4.2 Nonparametric One-Way ANOVA (Kruskal-Wallis Test) 2.5 Hands-on Exercise Solution ________________________________________________________________________________ "],["test-for-autocorrelation-and-temporal-effects.html", "Chapter 3 TEST FOR AUTOCORRELATION AND TEMPORAL EFFECTS 3.1 Test for Autocorrelation Using the Sample Autocorrelation Function", " Chapter 3 TEST FOR AUTOCORRELATION AND TEMPORAL EFFECTS Environment Data is not always stable or stationary over time or space and is frequently subjected to sustained or cyclical change to the affecting factors. These concepts will help us understand the temporal effects better; Temporal Trends: refer to changes in a variable or phenomenon over time. Consider monitoring soil nitrogen levels in agricultural field over several years. There might be temporal rise of nitrogen immediately after application of nitrogen-based fertilizer and return back to almost the average state after time. Although there might be a general increase in soil nitrogen levels, but they won’t match the immediate spikes after fertilizer application. Autocorrelation or serial correlation: is the correlation between data values of the same variable. Positive autocorrelation means that values close in time/space tend to be similar while negative autocorrelation means that they tend to differ. Monotonic trend: this is a trend that is in one direction, either constantly upward or downwards. A good example are the levels of carbon dioxide(CO2) concentrations in the atmosphere, they have been consistently rising due to industrial activities and deforestation. Cyclic or Seasonal trend refer to patterns that repeat at regular intervals such as daily ,hourly or annually. These trends are usually affected by natural cycles, human activities or climatic conditions. For instance, these trends are evident in phosphorous levels in water bodies that might increase during rainy seasons due agricultural runoff and decline during dry seasons. When testing for autocorrelation and temporal effects the data samples are assumed to have values that are independent or uncorrelated, and identically or similarly distributed. The data are said to be autocorrelated or serially correlated when this assumption is violated. These are the tests that are used to check for autocorrelation: Sample Autocorrelation Function(ACF): used for a single time series where data is normally distributed. Rank von Neumann ratio test used when the data samples evidence of nonnormality. The complete block design ANOVA or Friedman test is used for multiple different data samples for instance testing the concentration of ground water contaminant from multiple monitoring wells at a site. 3.1 Test for Autocorrelation Using the Sample Autocorrelation Function Before testing for autocorrelation using the ACF method, the data sample should be; normally distributed. stationary(that is, not trending). free of outliers. containing atleast 10 to 12 observations. … And here is how ACf is computed. Arrange the data in lagged data pairs (\\(x_{i}, x_{i + k}\\)), for \\(i\\) = 1, 2, ….(\\(n-k\\)), where; \\(n\\) is the size of the data samples(number of data values therein). \\(k\\) is the lag. i.e number of sampling events dates separating one data value in the pair from the second value. For instance a lag of 1 shows that the two value pairs were collected in consecutive time intervals(e.g days) while lag shows that the data value pairs were collected every each time interval Calculate the sample Autocorrelation coefficient \\[r_k = {{\\sum^{n-k}_{i=1}(x_i-x^!)(x_{i+k}- x^!)}\\over{\\sum^n_{i=1}(x_i - x^!)^2}}\\] where; \\(n\\) is the data sample size. \\(k\\) is the lag \\(x^!\\) is the data sample mean \\(x_{i}, x_{i + k}\\) are the components of the data pairs formed based on the lag. If \\(k\\) = 1, the sample autocorrelation coefﬁcient, \\(r_1\\) is referred to as the ﬁrst order sample autocorrelation coefﬁcient. If \\(k\\) = 2, \\(r_2\\) is the second order coefﬁcient, and so on. Lets breakdown the possible results of autocorrelation; \\(r_0\\) = 1: Autocorrelation at lag 0 is always one because a value is perfectly correlated with itself. Random Data: if the data is random, most autocorrelation coefficients will be close to zero, and will decrease further as the lag increases. Autocorrelated Data: If there’s autocorrelation, some coefficients(\\(r_k\\)) will be significantly larger than zero, but their strength decreases with higher lags. Trend in data: If there is a trend, coefficients wont diminish, showing persistent correlation. As a summary, the normally distributed data, \\(r_k\\) should be close to zero if there is no correlation. At a 95% confidence level, autocorrelation is insignificant if no \\(r_k\\) exceeds the threshold \\(2\\over{\\sqrt{n}}\\). There result of each autocorrelation are therefore plotted in a correlogram and the confidence limits are show as the horizontal lines. Try it! "],["correlation-covariance-geostatistics.html", "Chapter 4 CORRELATION, COVARIANCE, GEOSTATISTICS 4.1 INTRODUCTION 4.2 CORRELATION AND COVARIANCE 4.3 GEOSTATISTICS", " Chapter 4 CORRELATION, COVARIANCE, GEOSTATISTICS 4.1 INTRODUCTION In this topic, we will focus on Bivariate correlation and multivariate correlation, that is, the correlation between two or multiple variables. Lets define correlation once again! Correlation is the degree if association of variables. Correlation coefficient: is the quantifier that indicates the association of variables whether whether one variable is increasing/decreasing and the other one is increasing/decreasing or if there is no association at all. The correlation coefficient always ranges from -1 to 1 where the closer it gets to +1 means a stronger positive correlation(both variables increases/decreases at a concert) while it gets closer to -1 means a stronger negative correlation where one variable increases and the other variable decreases. A correlation close to zero from any direction(\\(+_{ve}\\) or \\(-_{ve}\\)) shows that there is little to no correlation between variables. Multiple correlation is where a group of variables (usually referred to as the predictor or X variables) to jointly relate to another variable (usually referred to as the response or Y variable), Partial correlation is the correlation between any pair of variables, given the presence of the remaining variables. There are several methods to compute the correlation coefficient; Pearson’s correlation coefficient. - satisfies linearity, normality and other parametric assumptions test. Spearman’s correlation coefficient. - free from linearity, normality etc. conditions. Kendall’s correlation coefficient. - free from linearity, normality etc conditions. 4.2 CORRELATION AND COVARIANCE Remember, Correlation is the degree of association between two variables - and the strength and direction of this relationship is captured by the correlation coeeficient which quantifies how the variables are related. Correlation can be linear or non-linear where the former refers to the proportional changes in variables(e.g doubling of one variables results in doubling the other) while the later refers to the relationship being not proportional(e.g doubling resulting into triple or fourfold change of the other). While correlation does not imply causation, understanding correlations can provide valuable insights. For example, a correlation between soil pH and aluminum levels in groundwater might indicate that lower pH levels increase aluminum dissolution, even though pH itself is not the direct cause of aluminum contamination. Covariance, on the other hand, measures the degree to which two variables change together. Unlike the correlation coefficient, covariance lacks a standardized scale, making it harder to interpret. The correlation coefficient can be derived by dividing the covariance by the product of the standard deviations of the two variables. Application of correlation Correlation forms the foundation for many analyses, including regression, where relationships are used to predict one variable based on another. However, spurious correlations (associations without logical explanation) should be critically evaluated to avoid misleading conclusions. For instance, a correlation between the number of graduating students and basketball game wins is likely coincidental rather than meaningful. In Summary, correlation provides a normalized measure of association, covariance gives an absolute value of joint variability. Together, these metrics are crucial for understanding relationships between variables and guiding further analyses. Lets dive deep into exploring the correlation coefficient mentioned in the introduction. 4.2.1 Pearson’s Correlation Coefficient Pearson’s correlation coefficient, often referred to as Pearson’s r or the product-moment correlation, measures the strength and direction of a linear relationship between two variables. It works best for linear and monotonic relationships, where the association is consistently positive or negative. A high pearson correlation will have the data points when plotted on a scatter plot to align in a straight line and have all points easily predicted/approximated by a straight line(“regression line”). Pearson correlation is computed when the test or data satisfies the parametric assumptions, such that; Be from normally distributed populations. Have no significant outliers. Show consistent variance (homoscedasticity). This type of correlation is sensitive to non-linear data or data containing missing values(e.g non-detects). Under such cases alternatives like Spearman or Kendall coefficients are recommended. The Pearsons correlation coeefficient is: \\[r = {1\\over{n - 1}}{\\sum^n_{i=1}[{{x_i - \\overline{x}}\\over{s_x}}][{{y_i - \\overline{y}}\\over{s_y}}]}\\] Where; \\(x_i\\): the independent variable data value \\(\\overline x\\): the mean of independent data values \\(y_i\\): the dependent variable data value \\(\\overline y\\): the mean of the dependent data values \\(s_x\\): The standard deviation of the independent variable \\(s_y\\): the standard deviation of the dependent variable ALternatively, it can be calculated as; \\[r = {{\\sum(x_i - \\overline x)(y_i- \\overline y)}\\over{\\sqrt{\\sum{(x_i - \\overline x)^2(y_i - \\overline y)^2}}}}\\] Where; \\(x_i\\) and \\(y_i\\): the data values for the two variables \\(\\overline x\\) and \\(\\overline y\\): the mean values of the variables. If r’s magnitude is; 0.95 and above, this indicates a strong correlation. between 0.75 and below 0.95 , indicates a moderate correlation. below 0.5, therefore there is a weak correlation. Significance Testing for Pearson’s Correlation Coefficient When working with real-world ecological data like the release of acidity of agricultural farms near zero grazing structures, significance testing helps determine whether the observed correlation between two variables in a sample reflects a true correlation in the population or is simply due to chance. Here is a step by step process; Forumalate the null hypothesis Null hypothesis (\\(H_0\\)): There is no correlation between the two variables in the population (r = 0) Alternate Hypothesis(\\(H_a\\)): There is a strong correlation Compute the test statistic Test statistic (\\(t_r\\)) can be computed by; \\[t_r = {{r\\sqrt{n-2}}\\over{\\sqrt{1-r^2}}}\\] Where; \\(r\\): Sample correlation coefficient \\(n\\): Number of paired data points This formula adjusts \\(r\\) for the sample size and expresses how extreme the observed correlation is. Compute the degree of freedom The degree of freedom is calculated by; \\[df = n -2 \\] Determine the critical value, pvalue and make your decision —————————-**remember to put this into practice————————————————– 4.2.2 Spearman’s and Kendall’s Correlaton Coeefficients In ecological studies, researchers often analyze relationships between variables that may not follow normal distributions or linear trends. Nonparametric correlation methods like Spearman’s Rho (\\(\\rho\\)) and Kendall’s Tau (\\(\\tau\\)) are particularly useful in such scenarios. These methods allow for the exploration of monotonic relationships—either linear or nonlinear—without relying on strict parametric assumptions. 4.2.2.1 Spearman’s Rho(\\(\\rho\\)) Spearman’s Rho is commonly used in ecology to measure how two variables, such as species abundance and habitat quality, vary together in a monotonic fashion. Key Features and Calculations Data Ranking: Rank the values of one variable (e.g., habitat quality) from smallest to largest. Repeat the ranking process for the second variable (e.g., species abundance). Rank Alignment: Rearrange the ranks of the second variable to match the order of the first variable’s values. These ranks replace the original data values in the formula used for Pearson’s correlation coefficient. This approach ensures that outliers, which can significantly distort parametric methods, have minimal impact. The test of significance for Spearman’s Rho follows the same process as that for Pearson’s coefficient, using the ranked data instead of raw values. While Spearman’s method is robust and flexible, it does not utilize the full magnitude of the data values, making it less precise than Pearson’s method when the assumptions for Pearson’s coefficient are satisfied 4.2.2.2 Kendall’s Tau (\\(\\tau\\)) Kendall’s Tau offers another rank-based approach to measure monotonic relationships and is particularly relevant for ecological data with small sample sizes or irregular distributions. Key Features and Calculations Here is how Kendall’s Tau is calculated; Sorting and Matching: Sort one variable (e.g., elevation) in ascending order while maintaining the alignment of its corresponding variable. Pairwise Comparisons: Compare successive pairs of data to identify whether the relationship is concordant (positive), discordant (negative), or tied. Concordant pairs are assigned a \\(a + 1\\), discordant pairs a \\(a - 1\\), and tied pairs are ignored. Coefficient Calculation: Compute \\(\\tau\\) as the difference between concordant and discordant pairs divided by the total number of possible pairs. Significance Testing For small data sets (e.g, \\(n &lt; 10\\)), compare \\(S\\) (i.e concordant and discordant pairs) to critical values. For larger data sets use a z-statistic with adjustments for ties to assess significance. Kendall’s \\(\\tau\\) could evaluate the relationship between water quality indicators(e.g dissolved oxygen) and fish diversity in a rover system. Advantages of Nonparametric Methods in Ecology Robust to Outliers: Nonparametric methods are well-suited for ecological data, which often include extreme values (e.g., rare species occurrences). No Distribution Assumptions: These methods are ideal for analyzing relationships in data sets that do not conform to normal distributions. Handles Ties: Midrank adjustments make these methods practical for ecological studies, where tied observations are common (e.g., identical temperature readings across sites). How does these non parametric tests compare with the Pearson Correlation in Ecology? Are best when the relationship between variables is monotonic but not linear (e.g., species richness and disturbance level). Pearson’s Correlation assumes normality and linearity, which may not hold for ecological data, such as seasonal variations in bird populations. —- Remember to add content on Covariance —————- 4.3 GEOSTATISTICS Geostatistics is a specialized branch of statistics focused on analyzing and estimating spatially correlated data. It is widely used in fields such as ecology, geology, and environmental science to predict values at unsampled locations based on the spatial relationships observed in sampled data. A geostatistical analysis typically unfolds in two main steps: variogram modeling and kriging. Together, these steps help estimate unknown values and assess the precision of those estimates, enabling informed decision-making in spatial studies. Lets introduce each of them will break down later with live examples 4.3.1 Variogram Modelling The first step in geostatistical analysis involves developing a model to describe the spatial relationships between sampled and unsampled locations. This model is known as a variogram or semivariogram. Here are the some of key features of the Variogram: The variogram quantifies how measurements at nearby locations are more similar (or correlated) than those farther apart.. It incorporates assumptions about the spatial structure of the data, considering known site characteristics and potential spatial trends. A varigram can be used n a study of soil nutrient distribution across a forest, the variogram might reveal that nutrient concentrations are strongly correlated within a radius of 50 meters but weaken beyond that distance. ———————Expand on this———————-(content from the book) 4.3.2 Kriging Once the variogram is established, the second step—kriging—is performed to estimate unknown measurements at unsampled locations. This is how kriging works: Measurements at surrounding sampled locations contribute to the estimate at the unsampled point. Weights are determined based on the variogram model, the location being estimated, and the distance of nearby sampled points. Kriging provides not only an estimate of the unknown value but also an estimate of precision, known as the kriging variance. The precision depends on the variability in the surrounding data. … and this how the accuracy and precision of kriging can be improved; Increasing the number of sampled locations enhances the accuracy and precision of kriging estimates. Kriging outputs are often used to create data contours or isopleths, which provide a visual representation of spatial variation across the study area. Kriging might be used to predict plant species richness across an unsampled landscape, leveraging data from sampled plots to generate a continuous map of biodiversity. —————remember to expand on this—————— "],["linear-regression-analysis.html", "Chapter 5 LINEAR REGRESSION ANALYSIS 5.1 Simple Linear Regression Analysis 5.2 Data transformation versus generalized linear model 5.3 Multiple Linear Regression (MLR) model 5.4 Hands-on Exercise", " Chapter 5 LINEAR REGRESSION ANALYSIS Regression is the finding of a relation between the input(predictor/independent) variable/s and the continuous output(target/dependent) variable. Now Linear Regression is finding the linear relationship between dependent variable and one or more independent features by fitting a linear equation to the observed data. If there is one independent variable therefore the regression in known as simple linear regression while if multiple variables are involved it is known as multiple linear regression. 5.1 Simple Linear Regression Analysis Simple linear regression seeks to explore and model the relationship between two quantitative variables. Here’s what it helps you do: Understand the strength of the relationship: For example, it can reveal how strongly rainfall affects soil erosion. If the relationship is strong, changes in rainfall levels are likely to lead to significant changes in soil erosion. Make predictions: It allows you to estimate the dependent variable’s value for a given value of the independent variable. For instance, if you know how much it rained, you can predict how much soil erosion occurred. The beauty of simple linear regression lies in its simplicity—you’re working with just two variables. One acts as the input, and the other as the output. It’s like finding a straight-line path that best connects the data points and helps explain their relationship. For instance, imagine plotting rainfall (in mm) on the x-axis and soil erosion (in kg) on the y-axis. A regression line (a straight line) is fitted to the data, showing the trend. If the line slopes upwards, it means more rain leads to more soil erosion—a positive relationship. If it slopes downward, it’s a negative relationship. So, simple linear regression isn’t just math—it’s a way to uncover insights and make informed predictions from real-world data! 5.1.1 Empirical model-building-regression analysis Simple linear regression is a parametric test, which means it relies on certain assumptions about your data to ensure accurate and reliable results. Let’s break down these assumptions: Key Assumptions of Simple Linear Regression Homogeneity of variance (Homoscedasticity): The prediction errors (residuals) should remain roughly the same across all values of the independent variable. In simpler terms, the “spread” of data points around the regression line should stay consistent, not get wider or narrower. Independence of observations: Each data point in your dataset should be collected independently. For example, there shouldn’t be hidden relationships or patterns among your observations, like duplicate measurements or a biased sampling method. Normality: The data (especially the residuals) should follow a normal distribution. This is important for making valid inferences from your model. Linearity: The relationship between the independent and dependent variables should be linear. That means the best-fitting line through your data points should be straight, not curved or clustered into groups. What if these assumptions aren’t met? If your data violate assumptions like homoscedasticity or normality, don’t worry—you still have options! For instance, you can use a nonparametric test like the Spearman rank test, which doesn’t rely on strict parametric assumptions. By ensuring your data meets these conditions, you can confidently use simple linear regression to model and predict relationships between variables! The formula for simple linear regression looks like this: \\[y = \\alpha + \\beta X + \\epsilon\\] where; \\(y\\): the predicted value of the dependent variable. \\(X\\): the independent variable believed to be influencing the value of \\(y\\) \\(\\alpha\\): the intercept, or the starting point. It tells you the predicted value of \\(y\\) when \\(X\\) (the independent variable) is 0. \\(\\beta\\): the regression coeefficient, which measures the expected change in \\(y\\) for each one unit increase in \\(X\\). This is the slope/gradient of the line. \\(\\epsilon\\): the error term, representing how much actual data points deviate from the predicted line. This is how linear regression works: Linear regression fits a line of best fit through your data points. It finds the value of \\(\\beta\\) (the slope) that minimizes the total error (the difference between the observed and predicted values of \\(y\\). In simple terms, the regression line represents the most accurate straight-line relationship between your independent variable (\\(X\\)) and your dependent variable (\\(y\\)). Try it! Lets make this practical, you will be required to download the plant height data from here. Remember: The goal in linear regression is obtain the best estimates for the model coefficients(\\(\\alpha\\) and \\(\\beta\\)). We will break down the process in steps; Load the data set df &lt;- read.csv(&quot;data/Plant_height.csv&quot;) The goal of this analysis is to find the linear association between the plant height and the temperature, particularly if plant height is dependent on the temperature. Define the independent and dependent variables: temperature(temp) is the independent variable while plant height(loght) is the dependent variable lm(loght ~ temp, data = df) ## ## Call: ## lm(formula = loght ~ temp, data = df) ## ## Coefficients: ## (Intercept) temp ## -0.22566 0.04241 5.1.2 Interpreting the regression results To get a detailed breakdown of the regression results, such as the coefficient values, \\(R^2\\), test statistics, p-values, and confidence intervals, you need to save the output of the lm function to an object. Then, use the summary() function on that object to extract and review the analysis details. Note: loght is log(plant height). model &lt;- lm(loght ~ temp, data = df) summary(model) ## ## Call: ## lm(formula = loght ~ temp, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.97903 -0.42804 -0.00918 0.43200 1.79893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.225665 0.103776 -2.175 0.031 * ## temp 0.042414 0.005593 7.583 1.87e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6848 on 176 degrees of freedom ## Multiple R-squared: 0.2463, Adjusted R-squared: 0.242 ## F-statistic: 57.5 on 1 and 176 DF, p-value: 1.868e-12 The regression output provides the slope and intercept of the regression line. Based on this analysis, the regression equation for predicting the (log-transformed) plant height as a function of temperature is: \\[loght = −0.22566+0.04241*temp + \\epsilon\\] and this is the breakdown of the analysis; Coefficients The intercept (-0.22566) represents the predicted value of log(plant height) when the temperature is zero. The slope (0.04241) indicates that for every one-unit increase in temperature, the log(plant height) increases by approximately 0.04241. Significance of Coefficients The t-statistics and p-values test whether the coefficients are significantly different from zero. Here, the p-value for temperature (&lt; 0.005) suggests a statistically significant relationship between temperature and plant height. The intercept’s p-value (0.031) is less important since we typically focus on the slope’s significance. Overall Model Significance The F-statistic (57.5) and its associated p-value (&lt;0.001) confirm that the overall model is statistically significant. With a single predictor, the p-value for the F-statistic is identical to the p-value for the slope coefficient. 5.1.3 R-squared and Adjusted R-Squared R-squared, or the coefficient of determination, measures the proportion of the variance in the dependent variable (\\(y\\)) that is explained by the independent variable(s) (\\(x\\)) in a regression model. It is a goodness-of-fit statistic. On the other hand, Adjusted R-squared modifies R-squared to account for the number of predictors in the model, providing a more accurate measure of model performance, especially for multiple regression. Like our model above shows that: The \\(R^2\\) value (0.2463) shows that approximately 24.6% of the variation in plant height is explained by temperature. The adjusted \\(R^2\\) (0.242) slightly adjusts for the model’s complexity and is more relevant when comparing models 5.2 Data transformation versus generalized linear model 5.3 Multiple Linear Regression (MLR) model Unlike simple linear regression, multiple linear regression describes the linear relationship between two or more independent variables with one target(dependent) variable. The objective of multiple linear regression is to;- Find the strength of the relationship between two or more independent variables with the target variables. Find the value of the target variable at a certain value of the independent variable. When working on a multiple linear regression it is assumed that; the variance is homogeneous such that the prediction error does not change significantly across the predictor(independent) variables. It is also assumed, observations were independent and there was no hidden relationships among the variables when collecting the data. Additionally, the collected data follows a normal distribution and the independent variables have a linear relationship(linearity) with the dependent variable, therefore, the line of best fit through the data points is straight and not curved. Multiple linear regression is modeled by;- where;- \\(y\\) is the predicted value of the target variable. \\(\\beta_0\\) is the y-intercept. Value of y when all other parameters are zero. \\(\\beta_1X_1\\): \\(\\beta_1\\) is the regression coefficient of the first independent variable while \\(X_1\\) is the independent variable value. \\(\\cdots\\) do the same for however the number of independent variables are present. \\(\\beta_nX_n\\): the regression coefficient of the last independent variable. \\(\\epsilon\\) is the model error(variation not explained by the independent variables) The Multiple linear regression model calculates three things to find the best fit line for each independent variable;- The regression coefficient \\(\\beta_iX_i\\) that will minimize the overall error rate(model error). The associated p-value. If the relationship between the independent variable is statistically significant. The t-statistic of the model. T-statistic is the ratio of the difference in a number’s estimated value from its assumed value to its standard error. ——add an example —————————— ——Add Assumptions of Multiple Linear Regression———————- Practical exercise Using the airquality data set, fit a multiple linear regression model whereby the Solar radiation(Solar.R), Ozone and Wind are the independent variables while the Temperature (Temp) is the dependent variable. Interpret and analyze the results 5.4 Hands-on Exercise "],["conventional-and-probabilistic-risk-assessment.html", "Chapter 6 CONVENTIONAL AND PROBABILISTIC RISK ASSESSMENT 6.1 Introduction 6.2 Conventional or Point Risk Estimation 6.3 Probabilistic Risk Assessment Using Monte Carlo Simulation 6.4 Practical Exercise", " Chapter 6 CONVENTIONAL AND PROBABILISTIC RISK ASSESSMENT 6.1 Introduction Exposure to chemical is currently inevitable to the current society. Scientists and environmentalists aim not for zero exposure but minimize the release of the chemical contaminants to the environment and limit their potential adverse health or ecological effects. Maximum Contaminant Levels(MCLs) - is the concentration of a chemical contaminant that the environmental protection authorities believe would not cause substantial adverse health effects to the public. Assumptions based on exposure characteristics are made before deciding on a effective MCL. The input factors are then combined with the estimated toxicity or potency of the chemical, to back-calculate an allowable concentration or MCL for the chemical, which is believed would not cause substantial adverse health effects. These are some of the assumptions; The daily rate of water consumption. An individual body weight. Duration of residency at one location Expected human lifespan. These assumptions are the basic of back-calculations that is used to establish the allowable contaminant concentrations. Also, forward risk assessment can be performed to quantify the actual health risks posed by exposure to environmental contaminants. There are two methods used to estimate the potential risk of a contaminant concentration; Conventional or point risk estimation. Probabilistic risk assessment. 6.2 Conventional or Point Risk Estimation Point risk estimation uses a single variable to calculate the potential risk of a chemical contaminant for instance an adult individual body weight of 80 kilos, 1 to 2 liters of water as the daily ingestion rate for an individual, human lifespan of 70 years etc. Based on the above variable values the potential risk of cancer from a contaminant may be 10 in a million. The exposure of contaminants to humans have main three pathways; ingestion : consumption of medium containing the contaminant such as water or soil. dermal contact : absorption of the contaminant through skin contact. inhalation : breathing air containing the contaminant. The exposure might be either; carcinogenic(causing cancer), noncarncinogenic (causing other illness that is not cancer such skin rashes, suffocation, irritation) or both. Here, the potential health risk is estimated by first computing an average daily intake of the contaminant, and then integrating it with the contaminant health factor to quantify the risk. According to USEPA, 1989, the basic equation for computing daily intake or dose the ingestion or oral pathway is as follows; \\[I = {{C * IR * EF * ED}\\over{BW * AT}}\\] Where; \\(I\\): the chronic average daily intake in milligrams per kilogram of bodyweight of the contaminant per day. \\(C\\): the chemical concentration(eg. mg/kg for soil or mg/l for water) \\(IR\\): the ingestion rate(e.g 50mg/day for soil or 2liters/day for water). \\(EF\\): the exposure frequency(days per year that the exposure occurs) \\(ED\\): the exposure duration in years. \\(BW\\): body weight in kilograms. \\(AT\\): averaging time in days(which is equal to ED365 for noncarcinogens and 70365 for carcinogens, where 70 years is the assumed average human lifespan and there are 365 days in the year) Also, according to USEPA, 2004, this is the basic dermally absorbed intake or dose through the soil dermal contact pathway; \\[I = {{Cs * 10^{-6}SA*AF*EV*ABS*EF*ED}\\over{BW*AT}}\\] and this is the equation to calculate the daily absorbed intake or dose through water dermal contact pathway; \\[I = {{Cw * 10^{-3}SA*PC*ET*EF*ED}\\over{BW*AT}}\\] Where: \\(Cs\\) : is the soil concentration of the contaminant, that is, EPC, usually reported in mg/kg but the conversion factor of \\(10^{-6}\\) is applied to convert the units to kg/kg. \\(Cw\\) : is the water concentration of the contaminant, that is, EPC, usually reported in mg/l but the conversion factor of \\(10^{-3}\\) is applied to convert the units to mg/cm3 to maintain consistency of units. \\(SA\\): the exposed skin surface area (cm2). \\(AF\\): s the soil-to-skin adherence factor in milligrams per square centimeters per soil contact event (i.e.,mg/cm2 event) \\(EV\\): is the number of contact events per day, usually assumed as one event per day (i.e., 1/day). \\(PC\\): the dermal permeability constant (cm/h), which estimates the rate of transport of the contaminant across the skin into the body. \\(ABS\\): the dermal absorption fraction for the contaminant (unitless) Try it! ———Add Examples——————- One of the examples is the inhalation risk assessment (example 21.1) 6.3 Probabilistic Risk Assessment Using Monte Carlo Simulation While the method discussed above, point risk estimation, produces a single estimate of the health risk, probabilistic risk assessment produces probabilities or likelihoods of specified risks. This is due to variability and uncertanities regarding contaminant concentrations and other assumptions like daily water consumption, individual bodyweight, genetics and exposure to the contaminant. This leads to result in overestimation or underestimation of the actual exposure risk for some members. Probability risk assessment produces a full probability distribution of the risks indicating the probability or likelihood for each specified risk by using exposure characteristics and contaminant concentrations. Similarly, probability distributions are speciﬁed for other exposure characteristics such as water or soil ingestion rates, inhalation or breathing rates, and skin surface area, if the distributions are known. Monte Carlo simulation is then used to produce hundreds or thousands of various combinations of values from each of the probability distributions speciﬁed for the exposure characteristics and contaminant concentrations, and a point risk computed for each combination of inputs, thereby generating a probability distribution of risks Try it! ———Add Examples——————- One of the examples is the inhalation risk assessment (example 21.2). Will search examples from online 6.4 Practical Exercise Solution ________________________________________________________________________________ "],["datasets.html", "Chapter 7 Datasets", " Chapter 7 Datasets forest_health - https://www.kaggle.com/code/furkan09/forest-health-ecological-diversity-analysis/input africa soil property - https://www.kaggle.com/competitions/afsis-soil-properties/data "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
