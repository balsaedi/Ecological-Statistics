# LINEAR REGRESSION ANALYSIS

**Regression** is the finding of a relation between the input(predictor/independent) variable/s and the continuous output(target/dependent) variable. 

**Linear Regression** is finding the linear relationship between dependent variable and one or more independent features by fitting a linear equation to the observed data. IF there is one independent variable therefore the regression in known as **simple linear regression** while if multiple variables are involved it is known as **multiple linear regression**

## Simple Linear Regression Analysis 

Simple linear regression seeks to explore and model the relationship between two quantitative variables. Here's what it helps you do:

- Understand the strength of the relationship:
For example, it can reveal how strongly rainfall affects soil erosion. If the relationship is strong, changes in rainfall levels are likely to lead to significant changes in soil erosion.

- Make predictions:
It allows you to estimate the dependent variable's value for a given value of the independent variable. For instance, if you know how much it rained, you can predict how much soil erosion occurred.

The beauty of simple linear regression lies in its simplicity—you’re working with just two variables. One acts as the input, and the other as the output. It’s like finding a straight-line path that best connects the data points and helps explain their relationship.

For instance, imagine plotting rainfall (in mm) on the x-axis and soil erosion (in kg) on the y-axis. A regression line (a straight line) is fitted to the data, showing the trend. If the line slopes upwards, it means more rain leads to more soil erosion—a positive relationship. If it slopes downward, it's a negative relationship.

So, simple linear regression isn't just math—it's a way to uncover insights and make informed predictions from real-world data!

### Empirical model-building-regression analysis 

Simple linear regression is a **parametric test**, which means it relies on certain assumptions about your data to ensure accurate and reliable results. Let’s break down these assumptions:

**Key Assumptions of Simple Linear Regression**

1. *Homogeneity of variance (Homoscedasticity)*:
The prediction errors (residuals) should remain roughly the same across all values of the independent variable. In simpler terms, the "spread" of data points around the regression line should stay consistent, not get wider or narrower.

2. *Independence of observations*:
Each data point in your dataset should be collected independently. For example, there shouldn’t be hidden relationships or patterns among your observations, like duplicate measurements or a biased sampling method.

3. *Normality*:
The data (especially the residuals) should follow a normal distribution. This is important for making valid inferences from your model.

4. *Linearity*:
The relationship between the independent and dependent variables should be linear. That means the best-fitting line through your data points should be straight, not curved or clustered into groups.

**What if these assumptions aren’t met?**

If your data violate assumptions like homoscedasticity or normality, don’t worry—you still have options! For instance, you can use a nonparametric test like the Spearman rank test, which doesn’t rely on strict parametric assumptions.

By ensuring your data meets these conditions, you can confidently use simple linear regression to model and predict relationships between variables!

### Interpreting the regression results
### R-squared and Adjusted R-Squared
## Data transformation versus generalized linear model


## Multiple Linear Regression (MLR) model

### Assumptions of Multiple Linear Regression


## <span style="color: green;">**Practical Exercise**</span> 