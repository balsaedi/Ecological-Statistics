# LINEAR REGRESSION ANALYSIS

**Regression** is the finding of a relation between the input(predictor/independent) variable/s and the continuous output(target/dependent) variable. 

Now **Linear Regression** is finding the linear relationship between dependent variable and one or more independent features by fitting a linear equation to the observed data. If there is one independent variable therefore the regression in known as **simple linear regression** while if multiple variables are involved it is known as **multiple linear regression.**

## Simple Linear Regression Analysis 

Simple linear regression seeks to explore and model the relationship between two quantitative variables. Here's what it helps you do:

- *Understand the strength of the relationship*:

For example, it can reveal how strongly rainfall affects soil erosion. If the relationship is strong, changes in rainfall levels are likely to lead to significant changes in soil erosion.

- *Make predictions*:

It allows you to estimate the dependent variable's value for a given value of the independent variable. For instance, if you know how much it rained, you can predict how much soil erosion occurred.

The beauty of simple linear regression lies in its simplicity—you’re working with just two variables. One acts as the input, and the other as the output. It’s like finding a straight-line path that best connects the data points and helps explain their relationship.

For instance, imagine plotting rainfall (in mm) on the x-axis and soil erosion (in kg) on the y-axis. A regression line (a straight line) is fitted to the data, showing the trend. If the line slopes upwards, it means more rain leads to more soil erosion—a positive relationship. If it slopes downward, it's a negative relationship.

So, simple linear regression isn't just math—it's a way to uncover insights and make informed predictions from real-world data!

### Empirical model-building-regression analysis 

Simple linear regression is a **parametric test**, which means it relies on certain assumptions about your data to ensure accurate and reliable results. Let’s break down these assumptions:

**Key Assumptions of Simple Linear Regression**

1. *Homogeneity of variance (Homoscedasticity)*:

The prediction errors (residuals) should remain roughly the same across all values of the independent variable. In simpler terms, the "spread" of data points around the regression line should stay consistent, not get wider or narrower.

2. *Independence of observations*:

Each data point in your dataset should be collected independently. For example, there shouldn’t be hidden relationships or patterns among your observations, like duplicate measurements or a biased sampling method.

3. *Normality*:

The data (especially the residuals) should follow a normal distribution. This is important for making valid inferences from your model.

4. *Linearity*:

The relationship between the independent and dependent variables should be linear. That means the best-fitting line through your data points should be straight, not curved or clustered into groups.

**What if these assumptions aren’t met?**

If your data violate assumptions like homoscedasticity or normality, don’t worry—you still have options! For instance, you can use a nonparametric test like the *Spearman rank test*, which doesn’t rely on strict parametric assumptions.

By ensuring your data meets these conditions, you can confidently use simple linear regression to model and predict relationships between variables!

The formula for simple linear regression looks like this: $$y = \alpha + \beta X + \epsilon$$

where; 

- $y$: the predicted value of the dependent variable.
- $X$: the independent variable believed to be influencing the value of $y$
- $\alpha$: the intercept, or the starting point. It tells you the predicted value of $y$ when $X$ (the independent variable) is 0. 
- $\beta$: the regression coeefficient, which measures the expected change in $y$ for each one unit increase in $X$. This is the slope/gradient of the line. 
- $\epsilon$: the error term, representing how much actual data points deviate from the predicted line. 

*This is how linear regression works:*

Linear regression fits a **line of best fit** through your data points. It finds the value of $\beta$ (**the slope**) that minimizes the total error (the difference between the observed and predicted values of $y$.

In simple terms, the regression line represents the most accurate straight-line relationship between your independent variable ($X$) and your dependent variable ($y$).


<span style="color: orange;">**Try it!**</span> 

Lets make this practical, you will be required to download the plant height data from [here](https://environmentalcomputing.net/datasets/Plant_height.csv). 

**Remember:** The goal in linear regression is obtain the best estimates for the model coefficients($\alpha$ and $\beta$). 

We will break down the process in steps; 

- Load the data set 
```{r}
df <- read.csv("data/Plant_height.csv")
```

- The goal of this analysis is to find the linear association between the plant height and the temperature, particularly if plant height is dependent on the temperature. 

- *Define the independent and dependent variables:* temperature(`temp`) is the independent variable while plant height(`loght`) is the dependent variable
```{r}
lm(loght ~ temp, data = df)
```

### Interpreting the regression results

To get a detailed breakdown of the regression results, such as the coefficient values, $R^2$, test statistics, p-values, and confidence intervals, you need to save the output of the `lm` function to an object. Then, use the `summary()` function on that object to extract and review the analysis details.

Note: `loght` is log(plant height).
```{r}
model <- lm(loght ~ temp, data = df)
summary(model)
```

The regression output provides the slope and intercept of the regression line. Based on this analysis, the regression equation for predicting the (log-transformed) plant height as a function of temperature is: 

$$loght = −0.22566+0.04241*temp + \epsilon$$

and this is the breakdown of the analysis;

- *Coefficients*

The intercept (-0.22566) represents the predicted value of log(plant height) when the temperature is zero.

The slope (0.04241) indicates that for every one-unit increase in temperature, the log(plant height) increases by approximately 0.04241.

- *Significance of Coefficients*

The t-statistics and p-values test whether the coefficients are significantly different from zero.

Here, the p-value for temperature (< 0.005) suggests a statistically significant relationship between temperature and plant height.

The intercept's p-value (0.031) is less important since we typically focus on the slope's significance.

- *Overall Model Significance*

The F-statistic (57.5) and its associated p-value (<0.001) confirm that the overall model is statistically significant.

With a single predictor, the p-value for the F-statistic is identical to the p-value for the slope coefficient.

### R-squared and Adjusted R-Squared

**R-squared**, or the coefficient of determination, measures the proportion of the variance in the dependent variable ($y$) that is explained by the independent variable(s) ($x$) in a regression model. It is a goodness-of-fit statistic.

On the other hand, **Adjusted R-squared** modifies R-squared to account for the number of predictors in the model, providing a more accurate measure of model performance, especially for multiple regression.

Like our model above shows that: 

- The $R^2$ value (0.2463) shows that approximately 24.6% of the variation in plant height is explained by temperature.
- The adjusted $R^2$ (0.242) slightly adjusts for the model's complexity and is more relevant when comparing models



## Data transformation versus generalized linear model


## Multiple Linear Regression (MLR) model
Unlike simple linear regression, multiple linear regression describes the linear relationship between two or more independent variables with one target(dependent) variable. The objective of multiple linear regression is to;-

  i. Find the strength of the relationship between two or more independent variables with the target variables. 
  ii. Find the value of the target variable at a certain value of the independent variable. 

When working on a multiple linear regression it is assumed that; the **variance is homogeneous** such that the prediction error does not change significantly across the predictor(independent) variables. It is also assumed, **observations were independent** and there was no hidden relationships among the variables when collecting the data. Additionally, the collected data follows **a normal distribution** and the independent variables have **a linear relationship(linearity)** with the dependent variable, therefore, the line of best fit through the data points is straight and not curved. 

Multiple linear regression is modeled by;-

![](images/multiple_linear_regression.png)

where;-

  i. $y$ is the predicted value of the target variable. 
  ii. $\beta_0$ is the y-intercept. Value of y when all other parameters are zero. 
  iii. $\beta_1X_1$: $\beta_1$ is the regression coefficient of the first independent variable while $X_1$ is the independent variable value. 
$\cdots$ do the same for however the number of independent variables are present. 
  iv. $\beta_nX_n$: the regression coefficient of the last independent variable. 
  v. $\epsilon$ is the model error(variation not explained by the independent variables)

The Multiple linear regression model calculates three things to find the best fit line for each independent variable;-

i. The regression coefficient $\beta_iX_i$ that will minimize the overall error rate(model error). 
ii. The associated p-value. If the relationship between the independent variable is statistically significant. 
iii. The t-statistic of the model. T-statistic is the ratio of the difference in a number's estimated value from its assumed value to its standard error. 

------add an example ------------------------------


------Add Assumptions of Multiple Linear Regression----------------------

<span style="color: green;">**Practical exercise**</span>

Using the `airquality` data set, fit a multiple linear regression model whereby the Solar radiation(`Solar.R`), `Ozone` and `Wind` are the independent variables while the Temperature (`Temp`) is the dependent variable. Interpret and analyze the results


## <span style="color: green;">**Hands-on Exercise**</span> 